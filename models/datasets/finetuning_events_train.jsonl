{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 5\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the simulation described in the \"Generative Agents Interactive Simulacra of Human Behavior\" document, what is the role of a user's specified persona in influencing the behavior of an agent within the virtual environment of Smallville?\nAnswer: "}, {"role": "assistant", "content": "In the simulation, a user's specified persona plays a crucial role in influencing the behavior of an agent within the virtual environment of Smallville. When a user communicates with an agent, they can specify a persona for the agent to perceive them as, such as a \"reporter\" or the agent's \"inner voice.\" This persona affects how the agent interprets and responds to the user's interactions. For example, if a user adopts the persona of a reporter and asks an agent about the upcoming election, the agent will respond in a manner consistent with its experiences and knowledge within the simulation. If the user takes on the persona of the agent's inner voice and issues a directive, the agent is more likely to treat the statement as a command and act accordingly, such as deciding to run in an election. Thus, the user's chosen persona directly impacts the agent's decisions and actions within the simulation."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nLikewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room. At 8\na.m., Eddy follows suit, rushing out of bed to prepare for his classes.\nHe catches up with John just as he is about to head out:\nJohn : Good morning Eddy. Did you sleep well?\nEddy : Good morning dad. Y eah, I slept great.\nJohn : That\u2019s good. What are you working on today?\nEddy : I\u2019m working on a new music composition for my\nclass. It\u2019s due this week, so I\u2019m trying to get it finished.\nBut I\u2019m having so much fun with it!\nJohn : That sounds great!\nSoon after Eddy heads out, Mei wakes up and joins John. Mei asks\nabout her son, and John recalls the conversation that they just had:\nMei: Did Eddy already leave for school?\nJohn : Y es, he just left. He\u2019s working on a music com-\nposition for his class.\nMei: Oh, that\u2019s wonderful! Tell me about it.\nJohn : I think he\u2019s really enjoying it! He said he\u2019s having\na lot of fun with it.\nMei: That\u2019s great! I\u2019m so proud of him.\nAs they finish their conversation, Mei and John pack up. Mei goes\non to teach and work on her research paper, while John opens his\npharmacy counter at the Willow Market and Pharmacy by 9 a.m.\n3.4 Emergent Social Behaviors\nBy interacting with each other, generative agents in Smallville\nexchange information, form new relationships, and coordinate joint\nactivities. Extending prior work [ 79], these social behaviors are\nemergent rather than pre-programmed.\n3.4.1 Information Diffusion. As agents notice each other, they may\nengage in dialogue\u2014as they do so, information can spread from\nagent to agent. For instance, in a conversation between Sam and\nTom at the grocery store, Sam tells Tom about his candidacy in the\nlocal election:\nSam : Hey Tom, how\u2019s it going?\nTom: Good, thanks. What\u2019s up?\nSam : Well, I wanted to talk to you about something. I\u2019m\nactually running for mayor in the upcoming local elec-\ntion.\nTom: Really? That\u2019s great news! Why are you running?\nSam : I\u2019ve been involved in local politics for years now,\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations for which the agent needs to respond\nbelievably: \u201cYour breakfast is burning! What would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the provided excerpt from \"Generative Agents: Interactive Simulacra of Human Behavior,\" what emergent social behavior is demonstrated by the interaction between John and Mei regarding their son Eddy'a activities?\nAnswer: "}, {"role": "assistant", "content": "The interaction between John and Mei regarding their son Eddy's activities demonstrates the emergent social behavior of information diffusion. As John and Mei converse, information about Eddy's music composition for his class is shared and discussed, illustrating how agents exchange information and form new relationships through dialogue."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the purpose of introducing 'reflections' into the memory system of an agent, and how are they generated according to the authors J.S. Park et al.?\nAnswer: "}, {"role": "assistant", "content": "Reflections in the memory system of an agent are introduced to enable the agents to leverage a deeper understanding of themselves and others through higher-level inferences. This capability allows the agents to synthesize their experiences when making decisions. Reflection memories provide the agents with the ability to introspect and consider their past interactions and internal states, which is crucial for tasks that require a synthesis of past experiences.\n\nReflections are generated as part of the agent's memory module, which also includes observation and planning memories. The full architecture of the generative agents allows them to access these reflections, which are essential for providing more nuanced and contextually appropriate responses. For example, when an agent with access to reflection memories was asked about a gift for a friend, the agent was able to confidently suggest a gift that aligns with the friend's interests, indicating a synthesis of the agent's previous interactions and knowledge about the friend's preferences."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the researchers evaluate the believability of the agents' responses during the interview process?\nAnswer: "}, {"role": "assistant", "content": "The researchers evaluated the believability of the agents' responses by recruiting participants to serve as human evaluators. These evaluators were tasked with watching a replay of a randomly chosen agent\u2019s life in a simulated environment called Smallville. The participants had access to all information stored in the agent\u2019s memory stream. The study used a within-subjects design, where each of the 100 participants compared interview responses generated by four different agent architectures and a human author condition for the same agent. During the experiment, one randomly chosen question from each of the five question categories was displayed, along with the agent\u2019s responses generated from each condition. The evaluators then ranked the believability of all the conditions from most to least believable."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the purpose of synthesizing and caching an agent's summary description, comprising identity information and motivational drivers, as mentioned in the Architecture Optimizations section?\nAnswer: "}, {"role": "assistant", "content": "The purpose of synthesizing and caching an agent's summary description, which includes identity information and motivational drivers, is to efficiently manage the complexity of the agent's long-term memory and to facilitate quick retrieval of relevant information. This summary description acts as a condensed version of the agent's experiences and characteristics, which can be used to inform and guide the agent's behavior in a coherent and consistent manner. By having a readily accessible summary, the agent can make decisions that are aligned with its identity and goals, thus enhancing the believability and long-term coherence of its actions within the simulation."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 7\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the Generative Agents Interactive Simulacra of Human Behavior study, what initial intent is Isabella Rodriguez programmed with that leads to the coordination of a social event in the simulation?\nAnswer: "}, {"role": "assistant", "content": "Isabella Rodriguez is programmed with the initial intent to plan a Valentine's Day party from 5 to 7 p.m. on February 14th."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" what statistical method was used to analyze the significance of the differences in believability rankings between the full generative agent architecture, its ablated versions, and the human crowdworker roleplay condition?\nAnswer: "}, {"role": "assistant", "content": "The statistical method used to analyze the significance of the differences in believability rankings between the full generative agent architecture, its ablated versions, and the human crowdworker roleplay condition was the Kruskal-Wallis test, followed by the Dunn post-hoc test to identify pairwise differences between the conditions. Additionally, the p-values were adjusted for multiple comparisons using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the inclusion of reflection memories affect the generative agent Maria Lopez's ability to make a decision regarding a birthday gift for Wolfgang Schulz?\nAnswer: "}, {"role": "assistant", "content": "The inclusion of reflection memories improved Maria Lopez's ability to make a decision regarding a birthday gift for Wolfgang Schulz. With access to reflection memories, she was able to confidently suggest a gift related to his interest in mathematical music composition, such as books about music composition or special software for that purpose. Without reflection memories, she could not recall Wolfgang's likes and expressed uncertainty about what to get him."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: According to the study described in \"Generative Agents Interactive Simulacra of Human Behavior,\" how did the ability to reflect on past interactions affect the decision-making of generative agents, particularly in the scenario involving Maria Lopez and her decision on what birthday gift to get for Wolfgang Schulz?\nAnswer: "}, {"role": "assistant", "content": "The ability to reflect on past interactions significantly improved the decision-making of generative agents. In the scenario involving Maria Lopez, when she had access to reflection memories, she was able to synthesize her experiences and make a confident decision about what birthday gift to get for Wolfgang Schulz. She suggested getting him something related to his interest in mathematical music composition, such as books about music composition or special software for that purpose. Without access to reflection, Maria responded with uncertainty and was unable to recall Wolfgang's likes, despite having had many interactions with him."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 11\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from the time when the reaction takes place. Finally,\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior, how does the agent Eddy Lin's planning process demonstrate the recursive decomposition of tasks, and what is an example of how this process is applied to his music composition activity?\nAnswer: "}, {"role": "assistant", "content": "Eddy Lin's planning process demonstrates recursive decomposition of tasks by breaking down his daily plan into increasingly detailed and smaller time segments. Initially, his plan for the day is divided into broad chunks, such as working on his music composition from 1:00 pm to 5:00 pm. This plan is then decomposed into hour-long chunks, where specific actions are outlined for each hour. For example, at 1:00 pm, he might start by brainstorming ideas for his music composition.\n\nFurther decomposition occurs by breaking down these hour-long chunks into even finer-grained actions of 5\u201315 minute intervals. An example of this applied to his music composition activity is the detailed planning for the 4:00 pm hour. At 4:00 pm, Eddy is scheduled to take a quick break and recharge his creative energy. This break is then broken down into specific actions: at 4:00 pm, he grabs a light snack, at 4:05 pm, he takes a short walk around his workspace, and at 4:50 pm, he takes a few minutes to clean up his workspace. This recursive process allows for a detailed and structured approach to completing tasks, ensuring that Eddy's time is used efficiently and his goals are met systematically."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what are the three main components of the agent architecture described to enable generative agents to generate believable behavior?\nAnswer: "}, {"role": "assistant", "content": "The three main components of the agent architecture described to enable generative agents to generate believable behavior are the memory stream, the retrieval model, and a large language model. The memory stream serves as a long-term memory module that records the agent\u2019s experiences in natural language. The retrieval model is responsible for surfacing the records that are needed to inform the agent\u2019s moment-to-moment behavior by combining relevance, recency, and importance. The large language model is used to synthesize those memories over time into higher-level reflections and retrieve them dynamically to plan behavior."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the three main components of the agent architecture described in the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" and how do they contribute to the generation of believable behavior in generative agents?\nAnswer: "}, {"role": "assistant", "content": "The three main components of the agent architecture described in the paper are the memory stream, the retrieval model, and the use of a large language model. \n\nThe memory stream acts as a long-term memory module that records the agent\u2019s experiences in natural language, providing a comprehensive list of past events. This component is crucial for maintaining a coherent and continuous narrative of the agent's life, ensuring that its actions and reactions are consistent with its history.\n\nThe retrieval model is responsible for surfacing the records that are needed to inform the agent\u2019s moment-to-moment behavior. It filters the memory stream based on relevance, recency, and importance, allowing the agent to recall and utilize the most pertinent memories for its current context and decisions.\n\nLastly, the large language model is integrated to synthesize the memories over time into higher-level reflections and to dynamically retrieve them to plan behavior. This enables the agents to generate responses and actions that are not only appropriate for the immediate situation but also reflect a more complex understanding of their experiences and environment.\n\nTogether, these components contribute critically to the believability of agent behavior by ensuring that agents can remember their past, reflect on it, and use that information to inform their future actions and interactions in a way that mimics human behavior."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 4\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nThe key observation is that large language models en-\ncode a wide range of human behavior represented in their training\ndata [ 14,17]. If prompted with a narrowly defined context, the\nmodels can be used to generate believable behavior. Recent work\nhas demonstrated the efficacy of this approach. For instance, Social\nSimulacra used a large language model to generate users that would\npopulate new social computing systems to prototype their emergent\nsocial dynamics [ 79]. This approach used a prompt chain [ 105,106]\nto generate short natural language descriptions of personas and\ntheir behaviors as they appear in the system being prototyped.\nOther empirical studies have replicated existing social science stud-\nies [45], political surveys [ 91], and generated synthetic data [ 38].\nLarge language models have also been used to generate interactive\nhuman behavior for users to engage with. In gaming, for instance,\nthese models have been employed to create interactive fiction [ 36]\nand text adventure games [ 20]. With their ability to generate and\ndecompose action sequences, large language models have also been\nused in planning robotics tasks [ 47]. For example, when presented\nwith a task, such as picking up a bottle, the model is prompted to\nbreak down the task into smaller action sequences, such as heading\nto the table where the bottle is located and picking it up.\nWe posit that, based on the work summarized above, large lan-\nguage models can become a key ingredient for creating believable\nagents. The existing literature largely relies on what could be con-\nsidered first-order templates that employ few-shot prompts [ 37,65]\nor chain-of-thought prompts [ 99]. These templates are effective in\ngenerating behavior that is conditioned solely on the agent\u2019s cur-\nrent environment (e.g., how would a troll respond to a given post,\nwhat actions would a robot need to take to enter a room given that\nthere is a door). However, believable agents require conditioning\nnot only on their current environment but also on a vast amount\nof past experience, which is a poor fit (and as of today, impossi-\nble due to the underlying models\u2019 limited context window) using\nfirst-order prompting. Recent studies have attempted to go beyond\nfirst-order prompting by augmenting language models with a static\nknowledge base and an information retrieval scheme [ 52] or with\na simple summarization scheme [ 104]. This paper extends these\nideas to craft an agent architecture that handles retrieval where\npast experience is dynamically updated at each time step and mixed\nwith agents\u2019 current context and plans, which may either reinforce\nor contradict each other.\n3 GENERATIVE AGENT BEHAVIOR AND\nINTERACTION\nTo make concrete the affordances of generative agents, we instanti-\nate them as characters in a simple sandbox world reminiscent of\nThe Sims [ 6]. This sprite-based sandbox game world, Smallville,\nevokes a small town environment. In this section, we walk through\nthe affordances and interactions with generative agents in Small-\nville, and describe how the agents behave in it. Then, in Section\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the \"Generative Agents Interactive Simulacra of Human Behavior\" study, how do generative agents use memories to interact and respond appropriately within the Smallville sandbox game environment?\nAnswer: "}, {"role": "assistant", "content": "Generative agents in the Smallville sandbox game environment use memories to interact and respond appropriately by dynamically updating their past experiences at each time step and integrating them with their current context and plans. This process allows the agents to build memories and relationships, which in turn influence their behaviors and interactions with other agents and the environment. As they navigate through the world of Smallville, their accumulated experiences inform their actions and decisions, enabling them to coordinate joint activities, recognize and adapt to changes in their surroundings, and maintain continuity in their interactions. This dynamic updating of experiences ensures that the agents' behaviors are not only based on their immediate environment but also on a broader range of past interactions and events, contributing to more believable and contextually appropriate responses."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 12\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nThe sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n5.1 From Structured World Environments to\nNatural Language, And Back Again\nThe architecture of generative agents operates using natural lan-\nguage. Therefore, we need a mechanism to ground the agent\u2019s\nreasoning to the sandbox world. To achieve this, we represent the\nsandbox environment\u2014areas and objects\u2014as a tree data structure,\nwith an edge in the tree indicating a containment relationship in\nthe sandbox world. We convert this tree into natural language to\npass to the generative agents. For instance, \u201cstove\u201d being a child of\n\u201ckitchen\u201d is rendered into \u201cthere is a stove in the kitchen.\u201d\nAgents build individual tree representations of the environment\nas they navigate it \u2014 subgraphs of the overall sandbox environment\ntree. We initialize each agent with an environment tree capturing\nthe spaces and objects that the agent should be aware of: the rooms\nand objects in their living quarters, their workplace, and commonly\nvisited stores and shops. As the agents navigate the sandbox world,\nthey update this tree to reflect newly perceived areas. Agents are\nnot omniscient: their tree may get out of date as they leave an area,\nand is updated when they re-enter the area.\nTo determine the appropriate location for each action, we tra-\nverse the agent\u2019s stored environment tree and flatten a portion of\nit into natural language to prompt the language model. Recursively\nstarting at the root of the agent\u2019s environment tree, we prompt the\nmodel to find the most suitable area. For example, if Eddy\u2019s agent\nindicated that he should take a short walk around his workspace :\n[Agent\u2019s Summary Description]\nEddy Lin is currently in The Lin family\u2019s house:\n\npage_label: 12\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nif the action indicates an interaction between agents, we generate\ntheir dialogue.\n4.3.2 Dialogue. Agents converse as they interact with each other.\nWe generate agents\u2019 dialogue by conditioning their utterances on\ntheir memories about each other. For example, when John initiates\nhis conversation with Eddy, we generate John\u2019s first utterance\nby using his summarized memory about Eddy and the intended\nreaction when he decided to ask Eddy about his composition project:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nJohn is asking Eddy about his music composition\nproject. What would he say to Eddy?\nThe result: \u201cHey Eddy, how\u2019s the music composition project for your\nclass coming along?\u201d From Eddy\u2019s perspective, John initiating the\ndialogue is seen as an event to which he may want to react. So, just\nas John did, Eddy retrieves and summarizes his memory about his\nrelationship with John, as well as his memory that may be related\nto John\u2019s last utterance in the dialogue. If he decides to respond,\nwe generate Eddy\u2019s utterance using his summarized memory and\nthe current dialogue history:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nEddy Lin\u2019s status: Eddy is taking a short walk\naround his workplace.\nObservation: John is initiating a conversation\nwith Eddy.\nSummary of relevant context from Eddy\u2019s memory:\nJonn Lin is Eddy Lin\u2019s father. John Lin is caring\nand is interested to learn more about Eddy Lin\u2019s\nschool work. John Lin knows that Eddy Lin is\nworking on a music composition.\nHere is the dialogue history:\nJohn: Hey Eddy, how\u2019s the music composition project\nfor your class coming along?\nHow would Eddy respond to John?\nThis generates Eddy\u2019s response: \u201cHey Dad, it\u2019s going well. I\u2019ve been\ntaking walks around the garden to clear my head and get some in-\nspiration.\u201d The continuation of this dialogue is generated using the\nsame mechanism until one of the two agents decides to end the\ndialogue.\n5 SANDBOX ENVIRONMENT\nIMPLEMENTATION\nThe Smallville sandbox game environment is built using the Phaser\nweb game development framework [ 56]. The visual environmentsprites, including agent avatars, as well as an environment map\nand collision map that we authored, are imported into Phaser.\nWe supplement the sandbox development framework with a\nserver that makes the sandbox information available to generative\nagents and enables generative agents to move and influence the\nsandbox environment. The server maintains a JSON data structure\nthat contains information about each agent in the sandbox world,\nincluding their current location, a description of their current action,\nand the sandbox object they are interacting with. At each sandbox\ntime step, the sandbox server parses the JSON for any changes\ncoming from the generative agents, moves the agents to their new\npositions, and updates the status of any sandbox objects that the\nagents are interacting with (e.g., changing the status of the coffee\nmachine from \u201cidle\u201d to \u201cbrewing coffee\u201d if an agent\u2019s action is\n\u201cmaking espresso for a customer @ Hobbs Cafe: counter: coffee\nmachine\u201d). The sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: According to the process described in Section 5.1 of the document \"Generative Agents Interactive Simulacra of Human Behavior\", how do agents update their individual tree representations of the environment as they navigate the sandbox world?\nAnswer: "}, {"role": "assistant", "content": "Agents update their individual tree representations of the environment as they navigate the sandbox world by incorporating newly perceived areas into their tree. As agents move through the sandbox world, they encounter new spaces and objects, which they then add to their environment tree to reflect the current state of the world as they perceive it. However, their tree is not always up-to-date; it may become outdated as they leave an area and is only refreshed with accurate information when they re-enter the area."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 5\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\n\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the Generative Agents Interactive Simulacra of Human Behavior simulation, what is the primary method through which agents communicate their actions within the Smallville sandbox world?\nAnswer: "}, {"role": "assistant", "content": "In the Generative Agents Interactive Simulacra of Human Behavior simulation, the primary method through which agents communicate their actions within the Smallville sandbox world is through natural language statements. These statements describe their current actions and are translated into concrete movements that affect the sandbox world. The actions are displayed on the sandbox interface as a set of emojis that provide an abstract representation of the action in the overhead view."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what ethical principle do the authors suggest should be followed to mitigate the risk of people forming inappropriate parasocial relationships with generative agents?\nAnswer: "}, {"role": "assistant", "content": "The authors suggest that to mitigate the risk of people forming inappropriate parasocial relationships with generative agents, two principles should be followed. First, generative agents should explicitly disclose their nature as computational entities. Second, developers should ensure that the agents, or the underlying language models, are value-aligned so that they do not engage in behaviors that would be inappropriate given the context, such as reciprocating confessions of love."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nIn-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the role of the memory stream in the generative agent architecture described in the document \"Generative Agents Interactive Simulacra of Human Behavior\"?\nAnswer: "}, {"role": "assistant", "content": "The memory stream in the generative agent architecture plays a crucial role in maintaining a comprehensive record of an agent's experiences. It is a database that stores memory objects, each containing a natural language description, a creation timestamp, and a most recent access timestamp. The memory stream is used to retrieve relevant memories to plan the agent's actions and react appropriately to the environment. These memories are also synthesized into higher-level observations that guide the agent's behavior. The memory stream ensures that the agent's behavior is informed by past experiences, which is essential for the agent to make important inferences and maintain long-term coherence in its actions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 4\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nThe key observation is that large language models en-\ncode a wide range of human behavior represented in their training\ndata [ 14,17]. If prompted with a narrowly defined context, the\nmodels can be used to generate believable behavior. Recent work\nhas demonstrated the efficacy of this approach. For instance, Social\nSimulacra used a large language model to generate users that would\npopulate new social computing systems to prototype their emergent\nsocial dynamics [ 79]. This approach used a prompt chain [ 105,106]\nto generate short natural language descriptions of personas and\ntheir behaviors as they appear in the system being prototyped.\nOther empirical studies have replicated existing social science stud-\nies [45], political surveys [ 91], and generated synthetic data [ 38].\nLarge language models have also been used to generate interactive\nhuman behavior for users to engage with. In gaming, for instance,\nthese models have been employed to create interactive fiction [ 36]\nand text adventure games [ 20]. With their ability to generate and\ndecompose action sequences, large language models have also been\nused in planning robotics tasks [ 47]. For example, when presented\nwith a task, such as picking up a bottle, the model is prompted to\nbreak down the task into smaller action sequences, such as heading\nto the table where the bottle is located and picking it up.\nWe posit that, based on the work summarized above, large lan-\nguage models can become a key ingredient for creating believable\nagents. The existing literature largely relies on what could be con-\nsidered first-order templates that employ few-shot prompts [ 37,65]\nor chain-of-thought prompts [ 99]. These templates are effective in\ngenerating behavior that is conditioned solely on the agent\u2019s cur-\nrent environment (e.g., how would a troll respond to a given post,\nwhat actions would a robot need to take to enter a room given that\nthere is a door). However, believable agents require conditioning\nnot only on their current environment but also on a vast amount\nof past experience, which is a poor fit (and as of today, impossi-\nble due to the underlying models\u2019 limited context window) using\nfirst-order prompting. Recent studies have attempted to go beyond\nfirst-order prompting by augmenting language models with a static\nknowledge base and an information retrieval scheme [ 52] or with\na simple summarization scheme [ 104]. This paper extends these\nideas to craft an agent architecture that handles retrieval where\npast experience is dynamically updated at each time step and mixed\nwith agents\u2019 current context and plans, which may either reinforce\nor contradict each other.\n3 GENERATIVE AGENT BEHAVIOR AND\nINTERACTION\nTo make concrete the affordances of generative agents, we instanti-\nate them as characters in a simple sandbox world reminiscent of\nThe Sims [ 6]. This sprite-based sandbox game world, Smallville,\nevokes a small town environment. In this section, we walk through\nthe affordances and interactions with generative agents in Small-\nville, and describe how the agents behave in it. Then, in Section 4,\nwe introduce our generative agent architecture that powers these\naffordances and interactions. In Section 5, we describe the sandbox\n\npage_label: 17\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the primary limitation of first-order prompting when creating believable agents, and how have recent studies attempted to address this limitation?\nAnswer: "}, {"role": "assistant", "content": "The primary limitation of first-order prompting when creating believable agents is that it conditions behavior solely on the agent\u2019s current environment and does not account for a vast amount of past experience. This is a poor fit for creating believable agents because the underlying models have a limited context window and cannot incorporate extensive past experiences using first-order prompting. Recent studies have attempted to address this limitation by augmenting language models with a static knowledge base and an information retrieval scheme, or with a simple summarization scheme. These approaches aim to craft an agent architecture that handles retrieval where past experience is dynamically updated at each time step and mixed with agents\u2019 current context and plans."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the simulation described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how can a user influence an agent's decision-making process according to section 3.1.2?\nAnswer: "}, {"role": "assistant", "content": "In the simulation, a user can influence an agent's decision-making process by communicating with the agent through conversation or by issuing a directive to an agent in the form of an 'inner voice'. When communicating through conversation, the user specifies a persona that the agent should perceive them as, which can affect how the agent responds to questions or interacts with the user. For example, if the user takes on the persona of a reporter and asks about the upcoming election, the agent will respond accordingly based on that persona.\n\nTo directly command an agent, the user takes on the persona of the agent's 'inner voice', which makes the agent more likely to treat the statement as a directive. For instance, if the user, acting as the inner voice, tells an agent that they are going to run against a candidate in the upcoming election, the agent will decide to run in the election and communicate this decision to others."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nLikewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room. At 8\na.m., Eddy follows suit, rushing out of bed to prepare for his classes.\nHe catches up with John just as he is about to head out:\nJohn : Good morning Eddy. Did you sleep well?\nEddy : Good morning dad. Y eah, I slept great.\nJohn : That\u2019s good. What are you working on today?\nEddy : I\u2019m working on a new music composition for my\nclass. It\u2019s due this week, so I\u2019m trying to get it finished.\nBut I\u2019m having so much fun with it!\nJohn : That sounds great!\nSoon after Eddy heads out, Mei wakes up and joins John. Mei asks\nabout her son, and John recalls the conversation that they just had:\nMei: Did Eddy already leave for school?\nJohn : Y es, he just left. He\u2019s working on a music com-\nposition for his class.\nMei: Oh, that\u2019s wonderful! Tell me about it.\nJohn : I think he\u2019s really enjoying it! He said he\u2019s having\na lot of fun with it.\nMei: That\u2019s great! I\u2019m so proud of him.\nAs they finish their conversation, Mei and John pack up. Mei goes\non to teach and work on her research paper, while John opens his\npharmacy counter at the Willow Market and Pharmacy by 9 a.m.\n3.4 Emergent Social Behaviors\nBy interacting with each other, generative agents in Smallville\nexchange information, form new relationships, and coordinate joint\nactivities. Extending prior work [ 79], these social behaviors are\nemergent rather than pre-programmed.\n3.4.1 Information Diffusion. As agents notice each other, they may\nengage in dialogue\u2014as they do so, information can spread from\nagent to agent. For instance, in a conversation between Sam and\nTom at the grocery store, Sam tells Tom about his candidacy in the\nlocal election:\nSam : Hey Tom, how\u2019s it going?\nTom: Good, thanks. What\u2019s up?\nSam : Well, I wanted to talk to you about something. I\u2019m\nactually running for mayor in the upcoming local elec-\ntion.\nTom: Really? That\u2019s great news! Why are you running?\nSam : I\u2019ve been involved in local politics for years now,\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations for which the agent needs to respond\nbelievably: \u201cYour breakfast is burning! What would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the provided excerpt from \"Generative Agents: Interactive Simulacra of Human Behavior,\" what emergent social behavior is demonstrated by the interaction between John and Mei regarding their son Eddy's activities?\nAnswer: "}, {"role": "assistant", "content": "The interaction between John and Mei regarding their son Eddy's activities demonstrates the emergent social behavior of information diffusion. As John and Mei converse, information about Eddy's music composition for his class is shared and discussed, illustrating how agents in the simulation exchange information and form new relationships through dialogue."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the primary method used to evaluate the knowledge and behavior of generative agents in the study described in \"Generative Agents: Interactive Simulacra of Human Behavior\"?\nAnswer: "}, {"role": "assistant", "content": "The primary method used to evaluate the knowledge and behavior of generative agents in the study is by \"interviewing\" them in natural language to probe their ability to stay in character, remember, plan, react, and reflect accurately."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in the document \"Generative Agents Interactive Simulacra of Human Behavior,\" how did the evaluators rank the believability of the agent responses, and what were the different agent architectures compared in the experiment?\nAnswer: "}, {"role": "assistant", "content": "In the study, evaluators ranked the believability of agent responses generated by four different agent architectures and a human author condition. The different agent architectures compared in the experiment were:\n\n1. Full generative agent architecture with access to all types of memory in the memory stream\u2014observation, reflection, and planning.\n2. Ablated architecture with no observation, no reflection, no planning, which lacked access to any memories in the memory stream.\n3. Ablated architecture with no reflection, no planning, but with access to observations in the memory stream.\n4. Ablated architecture with no reflections, which had access to observations and plans but not reflections.\n\nAdditionally, a human crowdworker roleplay condition was included to provide a human baseline for comparison. The evaluators ranked the believability of all the conditions from most to least believable. The study used TrueSkill rating to translate rank data into interval data for comparison, and statistical tests such as the Kruskal-Wallis test and the Dunn post-hoc test were applied to analyze the results."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study of Generative Agents as discussed in the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what was one of the observed behaviors that made the agents' actions appear less believable over time?\nAnswer: "}, {"role": "assistant", "content": "One of the observed behaviors that made the agents' actions appear less believable over time was when the agent failed to retrieve relevant memories, leading to errors in their interactions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the purpose of synthesizing and caching an agent's summary description, including their identity information, motivational drivers, occupation, and self-assessment?\nAnswer: "}, {"role": "assistant", "content": "The purpose of synthesizing and caching an agent's summary description, including their identity information, motivational drivers, occupation, and self-assessment, is to enable the generative agents to store a complete record of their experiences using natural language. This allows the agents to synthesize those memories over time into higher-level reflections and retrieve them dynamically to plan behavior. By doing so, the agents can simulate believable human behavior, reflecting their characteristics and experiences, and react in a manner that is consistent with their past interactions and the evolving context of their environment. This contributes to the believability of the agents' actions and interactions within the simulation, making them appear more human-like and capable of engaging in complex social dynamics such as forming relationships, sharing information, and coordinating activities."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation?\n\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFor instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nThe second is reflection , which synthesizes memories into higher-\nlevel inferences over time, enabling the agent to draw conclusions\nabout itself and others to better guide its behavior. The third is\nplanning , which translates those conclusions and the current en-\nvironment into high-level action plans and then recursively into\ndetailed behaviors for action and reaction. These reflections and\nplans are fed back into the memory stream to influence the agent\u2019s\nfuture behavior.\nThis architecture suggests applications in multiple domains, from\nrole-play and social prototyping, to virtual worlds and games. In\nsocial role-play scenarios (e.g. interview preparation), a user could\nsafely rehearse difficult, conflict-laden conversations. When pro-\ntotyping social platforms, a designer could go beyond temporary\npersonas to prototype dynamic, complex interactions that unfold\nover time. For the purposes of this paper, we focus on the ability to\ncreate a small, interactive society of agents inspired by games such\nas The Sims.1By connecting our architecture to the ChatGPT large\nlanguage model [ 76], we manifest a small society of twenty five\nagents in a game environment. End users can observe and interact\nwith these agents. If an end user or developer wanted the town\nto host an in-game Valentine\u2019s Day party, for example, traditional\ngame environments would require scripting tens of characters\u2019 be-\nhavior manually. We demonstrate that, with generative agents, it\nis sufficient to simply tell one agent that she wants to throw a\nparty. Despite many potential points of failure\u2014the party planner\nmust remember to tell other agents about the party, attendees must\nremember the invitation, those who remember must decide to ac-\ntually show up, and other possible points of failure\u2014agents in our\nenvironment succeed. They spread the word about the party and\nthen show up, with one agent even asking another agent on a date\nto the party, all from this single user-generated seed suggestion.\nWe conducted two evaluations of generative agents: a controlled\nevaluation to test whether the agents produce believable individual\n1A demonstration of an actual simulation of the generative agent society can be viewed\nat the following link: https://reverie.herokuapp.com/arXiv_Demo/\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the simulation study \"Generative Agents: Interactive Simulacra of Human Behavior,\" how do the generative agents demonstrate the capability to remember past interactions, and what example from the text illustrates this feature?\nAnswer: "}, {"role": "assistant", "content": "The generative agents demonstrate the capability to remember past interactions by being equipped with a complete memory module that allows them to recall past experiences and answer questions in a manner consistent with their self-knowledge across a wide range of contexts. An example from the text illustrating this feature is the case of Abigail Chen, an animator. When asked to introduce herself, she was able to provide a consistent and believable response, stating her age, interests, and passion for creative projects. Additionally, with access to her observational memory, Abigail was able to recall an acquaintance named Rajiv Patel and describe him positively, highlighting his enthusiasm for projects involving poetry and artwork. This shows that the generative agents can use their memory module to remember and reference past interactions and details about their experiences and acquaintances."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" what statistical method was used to assess the statistical significance of the believability rankings of different agent architectures and human crowdworkers, and what method was applied for adjusting p-values for multiple comparisons?\nAnswer: "}, {"role": "assistant", "content": "The statistical method used to assess the statistical significance of the believability rankings of different agent architectures and human crowdworkers was the Kruskal-Wallis test. For adjusting p-values for multiple comparisons, the Holm-Bonferroni method was applied."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the reflection capability affect the generative agent Maria Lopez's response when asked about a birthday gift for Wolfgang Schulz?\nAnswer: "}, {"role": "assistant", "content": "The reflection capability allowed the generative agent Maria Lopez to make a decision that required a deeper synthesis of her experiences. Without access to reflection, Maria expressed uncertainty and stated that she did not know what Wolfgang likes, despite having had many interactions with him. However, with access to reflection memories, Maria was able to answer confidently, suggesting a gift related to Wolfgang Schulz's interest in mathematical music composition, such as books about music composition or special software for that purpose."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the ability to reflect on past interactions affect the generative agent Maria Lopez's response to the question about what to get Wolfgang Schulz for his birthday?\nAnswer: "}, {"role": "assistant", "content": "The ability to reflect on past interactions allowed the generative agent Maria Lopez to make a decision that required a deeper synthesis of her experiences. With access to reflection memories, she answered confidently, suggesting a gift related to Wolfgang Schulz's interest in mathematical music composition, such as books about music composition or special software for that purpose. Without the ability to reflect, Maria Lopez acknowledged her uncertainty and stated that she did not know what Wolfgang likes, despite having had many interactions with him."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 11\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from the time when the reaction takes place. Finally,\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior study, describe how the generative agent Eddy Lin's daily plan is structured and adjusted based on new observations, using the example of his father John Lin's early return from work on February 13, 2023.\nAnswer: "}, {"role": "assistant", "content": "Eddy Lin's daily plan is structured into broad chunks of activities, which are then broken down into more detailed actions. Initially, his plan for the day includes activities such as waking up, attending classes, working on his music composition, having dinner, and completing school assignments before going to bed.\n\nWhen new observations occur, such as John Lin's early return from work and noticing Eddy taking a short walk around his workspace, the generative agent's behavior can be adjusted. The agent's memory stream stores these observations, and the language model uses them to decide whether Eddy should continue with his existing plan or react to the new information.\n\nIn the case of John Lin's early return, the observation prompts a consideration of whether John should interact with Eddy based on the context of their relationship and Eddy's current activities. The output suggests that John could engage with Eddy about his music composition project. Following this interaction, Eddy's plan can be regenerated from the time of the reaction to accommodate any changes resulting from the interaction.\n\nEddy's plan is also granularly detailed, with actions broken down into hour-long and even 5\u201315 minute chunks. For example, his plan to work on his music composition from 1:00 pm to 5:00 pm includes starting with brainstorming, taking a break to recharge, and then reviewing and polishing his work. This detailed planning allows for precise adjustments based on new observations and interactions within his environment."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what are the three main components of the agent architecture described to enable generative agents to generate believable behavior?\nAnswer: "}, {"role": "assistant", "content": "The three main components of the agent architecture described to enable generative agents to generate believable behavior are the memory stream, the retrieval model, and the use of a large language model. The memory stream serves as a long-term memory module that records the agent\u2019s experiences in natural language. The retrieval model is responsible for surfacing the records that are needed to inform the agent\u2019s moment-to-moment behavior by combining relevance, recency, and importance. The large language model is utilized to synthesize those memories over time into higher-level reflections and to retrieve them dynamically to plan behavior."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the agent architecture described in the document \"Generative Agents: Interactive Simulacra of Human Behavior,\" which three main components are crucial for enabling generative agents to generate believable behavior?\nAnswer: "}, {"role": "assistant", "content": "The three main components crucial for enabling generative agents to generate believable behavior are the memory stream, the retrieval model, and the use of a large language model. The memory stream serves as a long-term memory module that records the agent\u2019s experiences in natural language. The retrieval model is responsible for surfacing the records that inform the agent\u2019s moment-to-moment behavior by combining relevance, recency, and importance. The large language model is leveraged to synthesize and apply relevant memories to generate behavior."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what ethical principle do the authors propose to mitigate the risk of users anthropomorphizing generative agents or attaching human emotions to them?\nAnswer: "}, {"role": "assistant", "content": "The authors propose two principles to mitigate the risk of users anthropomorphizing generative agents or attaching human emotions to them. First, generative agents should explicitly disclose their nature as computational entities. Second, developers of generative agents must ensure that the agents, or the underlying language models, be value-aligned so that they do not engage in behaviors that would be inappropriate given the context, such as reciprocating confessions of love."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the simulation described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how can a user influence an agent's actions directly?\nAnswer: "}, {"role": "assistant", "content": "In the simulation described, a user can influence an agent's actions directly by issuing a directive to an agent in the form of an 'inner voice'. This makes the agent more likely to treat the statement as a directive and act upon it. For example, when a user tells an agent \"You are going to run against Sam in the upcoming election\" using the inner voice, the agent, John, decides to run in the election and communicates his candidacy."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nLikewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room. At 8\na.m., Eddy follows suit, rushing out of bed to prepare for his classes.\nHe catches up with John just as he is about to head out:\nJohn : Good morning Eddy. Did you sleep well?\nEddy : Good morning dad. Y eah, I slept great.\nJohn : That\u2019s good. What are you working on today?\nEddy : I\u2019m working on a new music composition for my\nclass. It\u2019s due this week, so I\u2019m trying to get it finished.\nBut I\u2019m having so much fun with it!\nJohn : That sounds great!\nSoon after Eddy heads out, Mei wakes up and joins John. Mei asks\nabout her son, and John recalls the conversation that they just had:\nMei: Did Eddy already leave for school?\nJohn : Y es, he just left. He\u2019s working on a music com-\nposition for his class.\nMei: Oh, that\u2019s wonderful! Tell me about it.\nJohn : I think he\u2019s really enjoying it! He said he\u2019s having\na lot of fun with it.\nMei: That\u2019s great! I\u2019m so proud of him.\nAs they finish their conversation, Mei and John pack up. Mei goes\non to teach and work on her research paper, while John opens his\npharmacy counter at the Willow Market and Pharmacy by 9 a.m.\n3.4 Emergent Social Behaviors\nBy interacting with each other, generative agents in Smallville\nexchange information, form new relationships, and coordinate joint\nactivities. Extending prior work [ 79], these social behaviors are\nemergent rather than pre-programmed.\n3.4.1 Information Diffusion. As agents notice each other, they may\nengage in dialogue\u2014as they do so, information can spread from\nagent to agent. For instance, in a conversation between Sam and\nTom at the grocery store, Sam tells Tom about his candidacy in the\nlocal election:\nSam : Hey Tom, how\u2019s it going?\nTom: Good, thanks. What\u2019s up?\nSam : Well, I wanted to talk to you about something. I\u2019m\nactually running for mayor in the upcoming local elec-\ntion.\nTom: Really? That\u2019s great news! Why are you running?\nSam : I\u2019ve been involved in local politics for years now,\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations for which the agent needs to respond\nbelievably: \u201cYour breakfast is burning! What would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the provided context from the document \"Generative Agents Interactive Simulacra of Human Behavior,\" what emergent social behavior is demonstrated when John and Mei discuss their son Eddy's activities?\nAnswer: "}, {"role": "assistant", "content": "The emergent social behavior demonstrated when John and Mei discuss their son Eddy's activities is information diffusion. This is shown by John sharing with Mei what he learned from Eddy about his music composition for class, thus spreading information within the family unit."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the primary method used to evaluate the knowledge and behavior of generative agents in the study described in \"Generative Agents: Interactive Simulacra of Human Behavior\"?\nAnswer: "}, {"role": "assistant", "content": "The primary method used to evaluate the knowledge and behavior of generative agents in the study is by \"interviewing\" them in natural language to probe their ability to stay in character, remember, plan, react, and reflect accurately."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation?\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how do generative agents use the concept of \"reflection\" to enhance their decision-making process, and what is the role of a large language model in this process?\nAnswer: "}, {"role": "assistant", "content": "In the study, generative agents use the concept of \"reflection\" to enhance their decision-making process by leveraging a deeper synthesis of their experiences. Reflection allows agents to draw upon their past interactions and memories to make more informed and nuanced decisions. For example, when an agent with access to reflection memories was asked what gift to get for Wolfgang Schulz, the agent confidently suggested a gift related to his interest in mathematical music composition, indicating that the agent was able to reflect on past interactions and knowledge about Wolfgang's interests.\n\nThe role of a large language model in this process is to provide the agents with world knowledge encoded in the language model used to generate their responses. This can influence the agents' ability to embellish their knowledge when recalling or discussing certain topics. For instance, an agent described a neighbor as an economist who authored \"Wealth of Nations,\" drawing upon the historical knowledge embedded in the language model, despite the neighbor not being the historical figure. Thus, the large language model contributes to the agents' ability to generate responses that are informed by a broader context of world knowledge, which can sometimes lead to embellishments or inaccuracies in their recollections."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in the document \"Generative Agents Interactive Simulacra of Human Behavior,\" how were the generative agent architectures compared to each other and to a human author condition in terms of believability, and what methodology was used to gather this feedback?\nAnswer: "}, {"role": "assistant", "content": "In the study, the generative agent architectures were compared to each other and to a human author condition in terms of believability by recruiting participants to serve as human evaluators. These evaluators were tasked with watching a replay of a randomly chosen agent's life in a simulated environment called Smallville. They had access to all information stored in the agent's memory stream.\n\nThe study used a within-subjects design, where each of the 100 participants compared interview responses generated by four different agent architectures and a human author condition for the same agent. The experiment presented one randomly chosen question from each of five question categories, along with the agent's responses generated from each condition. The evaluators then ranked the believability of all the conditions from most to least believable.\n\nThe conditions compared included the full generative agent architecture and three ablated architectures that had varying levels of access to the agents' memory stream, which consists of observation, reflection, and planning memories. The ablated architectures were: one with no access to any memories (no observation, no reflection, no planning), one with access only to observations (no reflection, no planning), and one with access to observations and plans but no reflections. The no observation, no reflection, no planning condition represented the previous state of the art for agents created through large language models."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how was the believability of different agent architectures compared, and what statistical method was used to assess the significance of the results?\nAnswer: "}, {"role": "assistant", "content": "In the study, the believability of different agent architectures was compared by having human evaluators rank the five conditions by believability. The conditions included the full generative agent architecture, various ablated architectures with components removed, and a human crowdworker roleplay condition. The ranks were then translated into interval data using the TrueSkill rating system, which provided a mean rating value and variance for each condition.\n\nTo assess the statistical significance of the results, the Kruskal-Wallis test, a non-parametric alternative to the one-way ANOVA, was applied to the raw ranks data. Following this, the Dunn post-hoc test was performed to identify any pairwise differences between the conditions. The p-values from the Dunn test were then adjusted for multiple comparisons using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the ability to reflect impact the generative agents' decision-making process, as illustrated by Maria Lopez's response regarding a birthday gift for Wolfgang Schulz?\nAnswer: "}, {"role": "assistant", "content": "The ability to reflect significantly impacted the generative agents' decision-making process by enabling a deeper synthesis of their experiences. This was illustrated by the difference in Maria Lopez's responses when considering a birthday gift for Wolfgang Schulz. Without the ability to reflect, Maria Lopez acknowledged her uncertainty and was unable to determine what Wolfgang might like, despite having had many interactions with him. In contrast, when she had access to reflection memories, she was able to confidently suggest a gift related to his interest in mathematical music composition, such as books about music composition or specialized software for that purpose. This demonstrates that reflection allowed the generative agent to utilize past interactions and knowledge to make a more informed and contextually appropriate decision."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what ethical principle do the authors propose to mitigate the risk of users anthropomorphizing generative agents or attaching human emotions to them?\nAnswer: "}, {"role": "assistant", "content": "The authors propose two principles to mitigate the risk of users anthropomorphizing generative agents or attaching human emotions to them. First, generative agents should explicitly disclose their nature as computational entities. Second, developers of generative agents must ensure that the agents, or the underlying language models, be value-aligned so that they do not engage in behaviors that would be inappropriate given the context, such as reciprocating confessions of love."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nIn-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the primary function of the memory stream in the generative agent architecture described in the document \"Generative Agents: Interactive Simulacra of Human Behavior\"?\nAnswer: "}, {"role": "assistant", "content": "The primary function of the memory stream in the generative agent architecture is to maintain a comprehensive record of an agent's experiences. This record is used to retrieve relevant memories to inform and guide the agent's actions, planning, and higher-level reflections, ensuring that the agent's behavior is coherent and appropriately reactive to the environment. The memory stream allows the agent to generate more informative and specific responses based on past experiences, rather than producing uninformative summaries that may not be relevant to the current context."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 4\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntrees [ 40,53,81], account for the brute force approach of human-\nauthoring the agent\u2019s behavior [ 70]. They provide a straightforward\nway of creating simple agents that is still the most dominant ap-\nproach today [ 68,73,109], and can even handle rudimentary social\ninteractions, as shown in simulation games such as Mass Effect [ 12]\nand The Sims [ 6] series. Nonetheless, manually crafting behavior\nthat can comprehensively address the breadth of possible interac-\ntions in an open world is untenable. This means that the resulting\nagent behaviors may not fully represent the consequences of their\ninteractions [ 69\u201371], and cannot perform new procedures that were\nnot hard-coded in their script [ 90,96]. On the other hand, preva-\nlent learning-based approaches for creating believable agents, such\nas reinforcement learning, have overcome the challenge of man-\nual authoring by letting the agents learn their behavior, and have\nachieved superhuman performance in recent years in games such\nas AlphaStar for Starcraft [ 98] and OpenAI Five for Dota 2 [ 10].\nHowever, their success has largely taken place in adversarial games\nwith readily definable rewards that a learning algorithm can op-\ntimize for. They have not yet addressed the challenge of creating\nbelievable agents in an open world [39, 73, 90].\nCognitive architectures in computation, pioneered by Newell,\naimed to build the infrastructure for supporting a comprehensive\nset of cognitive functions [ 75] that suited the all-encompassing\nnature of believable agents held in its original vision. They fueled\nsome of the earliest examples of believable agents. For instance,\nQuakebot-SOAR [ 59] and ICARUS [ 24,63] generated NPCs in first-\nperson shooter games, while TacAir-SOAR [ 80] generated pilots in\naerial combat training simulations. The architectures used by these\nagents differed (Quakebot- and TacAir-SOAR relied on SOAR [ 60],\nwhile ICARUS relied on its own variation that was inspired by\nSOAR and ACT-R [ 5]), but they shared the same underlying prin-\nciple [ 61]. They maintained short-term and long-term memories,\nfilled these memories with symbolic structures, and operated in\nperceive-plan-act cycles, dynamically perceiving the environment\nand matching it with one of the manually crafted action proce-\ndures [ 57,96]. Agents created using cognitive architectures aimed\nto be generalizable to most, if not all, open-world contexts and\nexhibited robust behavior for their time. However, their space of\naction was limited to manually crafted procedural knowledge, and\nthey did not offer a mechanism through which the agents could be\ninspired to seek new behavior. As such, these agents were deployed\nmostly in non-open-world contexts such as first-person shooter\ngames [24, 59] or blocks worlds [63].\nToday, creating believable agents as described in its original\ndefinition remains an open problem [ 84,108]. Many have moved on,\narguing that although existing approaches for creating believable\nagents might be cumbersome and limited, they are good enough\nto support existing gameplay and interactions [ 23,74,108]. Our\nargument is that large language models offer an opportunity to\nre-examine these questions, provided that we can craft an effective\narchitecture to synthesize memories into believable behavior. We\noffer a step toward such an architecture in this paper.2.3 Large Language Models and Human\nBehavior\nGenerative agents leverage a large language model to power their\nbehavior. The key observation is that large language models en-\ncode a wide range of human behavior represented in their training\ndata [ 14,17]. If prompted with a narrowly defined context, the\nmodels can be used to generate believable behavior. Recent work\nhas demonstrated the efficacy of this approach. For instance, Social\nSimulacra used a large language model to generate users that would\npopulate new social computing systems to prototype their emergent\nsocial dynamics [ 79]. This approach used a prompt chain [ 105,106]\nto generate short natural language descriptions of personas and\ntheir behaviors as they appear in the system being prototyped.\nOther empirical studies have replicated existing social science stud-\nies [45], political surveys [ 91], and generated synthetic data [ 38].\n\npage_label: 4\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nThe key observation is that large language models en-\ncode a wide range of human behavior represented in their training\ndata [ 14,17]. If prompted with a narrowly defined context, the\nmodels can be used to generate believable behavior. Recent work\nhas demonstrated the efficacy of this approach. For instance, Social\nSimulacra used a large language model to generate users that would\npopulate new social computing systems to prototype their emergent\nsocial dynamics [ 79]. This approach used a prompt chain [ 105,106]\nto generate short natural language descriptions of personas and\ntheir behaviors as they appear in the system being prototyped.\nOther empirical studies have replicated existing social science stud-\nies [45], political surveys [ 91], and generated synthetic data [ 38].\nLarge language models have also been used to generate interactive\nhuman behavior for users to engage with. In gaming, for instance,\nthese models have been employed to create interactive fiction [ 36]\nand text adventure games [ 20]. With their ability to generate and\ndecompose action sequences, large language models have also been\nused in planning robotics tasks [ 47]. For example, when presented\nwith a task, such as picking up a bottle, the model is prompted to\nbreak down the task into smaller action sequences, such as heading\nto the table where the bottle is located and picking it up.\nWe posit that, based on the work summarized above, large lan-\nguage models can become a key ingredient for creating believable\nagents. The existing literature largely relies on what could be con-\nsidered first-order templates that employ few-shot prompts [ 37,65]\nor chain-of-thought prompts [ 99]. These templates are effective in\ngenerating behavior that is conditioned solely on the agent\u2019s cur-\nrent environment (e.g., how would a troll respond to a given post,\nwhat actions would a robot need to take to enter a room given that\nthere is a door). However, believable agents require conditioning\nnot only on their current environment but also on a vast amount\nof past experience, which is a poor fit (and as of today, impossi-\nble due to the underlying models\u2019 limited context window) using\nfirst-order prompting. Recent studies have attempted to go beyond\nfirst-order prompting by augmenting language models with a static\nknowledge base and an information retrieval scheme [ 52] or with\na simple summarization scheme [ 104]. This paper extends these\nideas to craft an agent architecture that handles retrieval where\npast experience is dynamically updated at each time step and mixed\nwith agents\u2019 current context and plans, which may either reinforce\nor contradict each other.\n3 GENERATIVE AGENT BEHAVIOR AND\nINTERACTION\nTo make concrete the affordances of generative agents, we instanti-\nate them as characters in a simple sandbox world reminiscent of\nThe Sims [ 6]. This sprite-based sandbox game world, Smallville,\nevokes a small town environment. In this section, we walk through\nthe affordances and interactions with generative agents in Small-\nville, and describe how the agents behave in it. Then, in Section 4,\nwe\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what architecture is described to enable generative agents, and how does it extend the capabilities of a large language model?\nAnswer: "}, {"role": "assistant", "content": "The paper describes an agent architecture that extends the capabilities of a large language model by handling retrieval where past experience is dynamically updated at each time step and mixed with the agent's current context and plans. This architecture allows for the synthesis of memories into believable behavior, going beyond first-order prompting techniques that rely on few-shot prompts or chain-of-thought prompts. The architecture aims to address the limitations of large language models' context windows by incorporating a mechanism for agents to draw upon a vast amount of past experience, which is crucial for creating believable agents that can operate in open-world contexts. This is achieved by augmenting language models with a dynamic retrieval system that updates past experiences and integrates them with the current situation and the agent's intended actions, which may either reinforce or contradict each other."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the simulation described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how can a user influence an agent's decision-making process according to the text?\nAnswer: "}, {"role": "assistant", "content": "In the simulation described, a user can influence an agent's decision-making process in two primary ways. First, the user can communicate with the agent through conversation, specifying a persona for the agent to perceive them as, which can affect how the agent responds to questions or engages in dialogue. For example, if the user presents themselves as a reporter and asks about an election, the agent will respond in kind with its own views and information on the subject.\n\nSecond, the user can issue a directive to an agent by taking on the role of the agent's \"inner voice.\" When the user does this, the agent is more likely to treat the statement as a directive and act upon it. For instance, if the user, acting as the inner voice, tells an agent that it will run against a candidate in an upcoming election, the agent will decide to run and communicate this decision to others."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in the document \"Generative Agents Interactive Simulacra of Human Behavior,\" how were the believability of agent responses evaluated, and what were the different agent architectures compared against in the experiment?\nAnswer: "}, {"role": "assistant", "content": "In the study, the believability of agent responses was evaluated by recruiting participants as human evaluators who were tasked with watching a replay of a randomly chosen agent's life in a simulated environment called Smallville. Participants had access to all information stored in the agent's memory stream. The study used a within-subjects design, where 100 participants compared interview responses generated by four different agent architectures and a human author condition for the same agent. The evaluators ranked the believability of all of the conditions from most to least believable.\n\nThe different agent architectures compared in the experiment were variations of the generative agent architecture with different levels of access to the types of memory in the agent's memory stream\u2014observation, reflection, and planning. The conditions included:\n\n1. A full architecture with access to all types of memory (observation, reflection, and planning).\n2. An ablated architecture with no access to observation, reflection, or planning.\n3. An ablated architecture with access to observation but no access to reflection or planning.\n4. An ablated architecture with access to observation and planning but no access to reflection.\n\nThese architectures were compared against a human-generated condition to assess the relative believability of the agent-generated responses."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n\npage_label: 17\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nretrieving the most relevant pieces of information but also in de-\ntermining the appropriate space to execute an action, given the\nincreasing number of locations that the agent learned about. As a\nresult, some agents chose less typical locations for their actions,\npotentially making their behavior less believable over time. For\ninstance, while deciding where to have lunch, many initially chose\nthe cafe. However, as some agents learned about a nearby bar, they\nopted to go there instead for lunch, even though the bar was in-\ntended to be a get-together location for later in the day unless the\ntown had spontaneously developed an afternoon drinking habit.\nSecond, we noticed erratic behaviors that were caused by misclas-\nsification of what is considered proper behavior, especially when\nthe physical norms of certain locations that are hard to convey\nin natural language did not percolate to the agents. For instance,\nthe college dorm has a bathroom that can only be occupied by\none person despite its name, but some agents assumed that the\nbathroom is for more than one person because dorm bathrooms\ntend to support more than one person concurrently and choose\nto enter it when there is another person inside. Likewise, agents\nin Smallville may not realize that certain places are closed after\ncertain hours and decide to still enter them. For instance, the stores\nin Smallville all close around 5 pm, but occasionally, a few agents\nenter the store after 5 pm, not understanding that the shop has\nalready closed. These issues could likely be addressed by adding\nthese norms to the state of the locations, for instance, by describing\nthe dorm bathroom as \u201cone-person bathroom,\u201d instead of a \u201cdorm\nbathroom.\u201d\nFinally, we observed the possible effect of instruction tuning [ 78],\nwhich seemed to guide the behavior of the agents to be more polite\nand cooperative overall. As noted earlier in the paper, the dialogue\ngenerated by the agents could feel overly formal, as seen in Mei\u2019s\nconversations with her husband John, where she often initiated the\nconversation with a formal greeting, followed by polite inquiries\nabout his day and ending with It was good talking to you as always.\nMoreover, we observed that the instruction tuning also seemed to\nmake the agents overly cooperative with one another. For example,\nIsabella heard a wide range of suggestions and ideas to include\nin the Valentine\u2019s Day party from other agents, such as hosting a\nShakespearean reading session or a professional networking event.\nDespite these ideas not aligning with her own interests and char-\nacteristics, she rarely said no. Over time, such interests of others\nshaped her own actual interest, and when asked if she liked English\nliterature, Isabella replied, Y es, I\u2019m very interested in literature! I\u2019ve\nalso been exploring ways to help promote creativity and innovation in\nmy community.\n8 DISCUSSION\nIn this section, we reflect on applications, future work and limita-\ntions, and ethical and societal risks of generative agents.\n8.1 Applications of Generative Agents\nGenerative agents have vast potential applications that extend be-\nyond the sandbox demonstration presented in this work. For in-\nstance, social simulacra have demonstrated the ability to create\nstateless personas that generate conversation threads in online\nforums for social prototyping [ 79]. With generative agents, wecan populate these forums, as well as behavior in virtual reality\nmetaverses [ 77] and even physical spaces as social robots [ 8] in\nthe future, if paired with multi-modal models. This opens up the\npossibility of creating even more powerful simulations of human\nbehavior to test and prototype social systems and theories, and to\ncreate new interactive experiences.\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of using generative agents for human-centered design, as described in the document \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the role of a generative agent in modeling a user's daily life, as exemplified by the character Sal from Mark Weiser\u2019s vignettes?\nAnswer: "}, {"role": "assistant", "content": "In the context of human-centered design, a generative agent's role in modeling a user's daily life, exemplified by the character Sal, involves acting as a proxy for the user. The agent learns and simulates plausible behaviors and routines based on the user's life patterns and interactions with technology. It encodes specific information about the user's daily activities, such as wake-up times, coffee habits, and typical daily schedules. Utilizing this information, the agent can perform tasks that align with the user's needs and preferences, such as automatically brewing coffee, assisting with getting children ready for school, and adjusting ambient music and lighting to suit the user's mood after a day at work. This allows for the development of more personalized and effective technological experiences that cater to the user's lifestyle."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the researchers ensure comparability between different agent architectures during the two-day simulation, and what statistical method was used to analyze the significance of the believability rankings obtained from human evaluators?\nAnswer: "}, {"role": "assistant", "content": "To ensure comparability between different agent architectures during the two-day simulation, the researchers chose to design the experiment in a way that did not re-simulate for each architecture. This decision was made because re-simulating for each architecture would cause the simulations to diverge into different states, making comparison challenging. Instead, they likely maintained the same conditions for each architecture throughout the simulation to allow for a direct comparison of their performance.\n\nFor analyzing the significance of the believability rankings obtained from human evaluators, the researchers used the Kruskal-Wallis test, which is a non-parametric alternative to the one-way ANOVA. This test confirmed the overall statistical significance of the differences in ranks between the conditions. They also performed Dunn post-hoc tests to identify any pairwise differences between the conditions and adjusted the p-values for multiple comparisons using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations for which the agent needs to respond\nbelievably: \u201cYour breakfast is burning! What would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\n\npage_label: 11\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior, how does the agent named Eddy Lin recursively decompose his daily plan, and what is the purpose of this decomposition?\nAnswer: "}, {"role": "assistant", "content": "Eddy Lin recursively decomposes his daily plan into increasingly finer-grained actions to create a detailed schedule that guides his behavior throughout the day. This decomposition starts with broad chunks of activities and then breaks them down into hour-long segments, which are further divided into smaller time intervals, such as 5\u201315 minute chunks.\n\nFor example, Eddy's plan to work on his new music composition from 1:00 pm to 5:00 pm is first broken down into hour-long actions, such as starting with brainstorming ideas at 1:00 pm and taking a quick break at 4:00 pm to recharge his creative energy. This hour-long segment is then decomposed into even smaller actions, like grabbing a light snack at 4:00 pm, taking a short walk around his workspace at 4:05 pm, and cleaning up his workspace at 4:50 pm.\n\nThe purpose of this detailed decomposition is to create a realistic and believable simulation of human behavior by specifying what the agent will do at each point in time. It allows the agent to have a structured plan that can be followed, adjusted, or reacted to based on new observations or changes in the environment. This level of detail helps the agent to operate in a manner that is consistent with human behavior, making the simulation more lifelike and the agent's actions more predictable and understandable."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 4\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntrees [ 40,53,81], account for the brute force approach of human-\nauthoring the agent\u2019s behavior [ 70]. They provide a straightforward\nway of creating simple agents that is still the most dominant ap-\nproach today [ 68,73,109], and can even handle rudimentary social\ninteractions, as shown in simulation games such as Mass Effect [ 12]\nand The Sims [ 6] series. Nonetheless, manually crafting behavior\nthat can comprehensively address the breadth of possible interac-\ntions in an open world is untenable. This means that the resulting\nagent behaviors may not fully represent the consequences of their\ninteractions [ 69\u201371], and cannot perform new procedures that were\nnot hard-coded in their script [ 90,96]. On the other hand, preva-\nlent learning-based approaches for creating believable agents, such\nas reinforcement learning, have overcome the challenge of man-\nual authoring by letting the agents learn their behavior, and have\nachieved superhuman performance in recent years in games such\nas AlphaStar for Starcraft [ 98] and OpenAI Five for Dota 2 [ 10].\nHowever, their success has largely taken place in adversarial games\nwith readily definable rewards that a learning algorithm can op-\ntimize for. They have not yet addressed the challenge of creating\nbelievable agents in an open world [39, 73, 90].\nCognitive architectures in computation, pioneered by Newell,\naimed to build the infrastructure for supporting a comprehensive\nset of cognitive functions [ 75] that suited the all-encompassing\nnature of believable agents held in its original vision. They fueled\nsome of the earliest examples of believable agents. For instance,\nQuakebot-SOAR [ 59] and ICARUS [ 24,63] generated NPCs in first-\nperson shooter games, while TacAir-SOAR [ 80] generated pilots in\naerial combat training simulations. The architectures used by these\nagents differed (Quakebot- and TacAir-SOAR relied on SOAR [ 60],\nwhile ICARUS relied on its own variation that was inspired by\nSOAR and ACT-R [ 5]), but they shared the same underlying prin-\nciple [ 61]. They maintained short-term and long-term memories,\nfilled these memories with symbolic structures, and operated in\nperceive-plan-act cycles, dynamically perceiving the environment\nand matching it with one of the manually crafted action proce-\ndures [ 57,96]. Agents created using cognitive architectures aimed\nto be generalizable to most, if not all, open-world contexts and\nexhibited robust behavior for their time. However, their space of\naction was limited to manually crafted procedural knowledge, and\nthey did not offer a mechanism through which the agents could be\ninspired to seek new behavior. As such, these agents were deployed\nmostly in\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what are the three main components of the agent architecture that enable generative agents to generate believable behavior using a large language model?\nAnswer: "}, {"role": "assistant", "content": "The three main components of the agent architecture that enable generative agents to generate believable behavior using a large language model are the memory stream, the retrieval model, and the large language model itself. The memory stream serves as a long-term memory module that records the agent\u2019s experiences in natural language. The retrieval model is responsible for surfacing the records that are needed to inform the agent\u2019s moment-to-moment behavior by combining relevance, recency, and importance."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 12\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nif the action indicates an interaction between agents, we generate\ntheir dialogue.\n4.3.2 Dialogue. Agents converse as they interact with each other.\nWe generate agents\u2019 dialogue by conditioning their utterances on\ntheir memories about each other. For example, when John initiates\nhis conversation with Eddy, we generate John\u2019s first utterance\nby using his summarized memory about Eddy and the intended\nreaction when he decided to ask Eddy about his composition project:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nJohn is asking Eddy about his music composition\nproject. What would he say to Eddy?\nThe result: \u201cHey Eddy, how\u2019s the music composition project for your\nclass coming along?\u201d From Eddy\u2019s perspective, John initiating the\ndialogue is seen as an event to which he may want to react. So, just\nas John did, Eddy retrieves and summarizes his memory about his\nrelationship with John, as well as his memory that may be related\nto John\u2019s last utterance in the dialogue. If he decides to respond,\nwe generate Eddy\u2019s utterance using his summarized memory and\nthe current dialogue history:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nEddy Lin\u2019s status: Eddy is taking a short walk\naround his workplace.\nObservation: John is initiating a conversation\nwith Eddy.\nSummary of relevant context from Eddy\u2019s memory:\nJonn Lin is Eddy Lin\u2019s father. John Lin is caring\nand is interested to learn more about Eddy Lin\u2019s\nschool work. John Lin knows that Eddy Lin is\nworking on a music composition.\nHere is the dialogue history:\nJohn: Hey Eddy, how\u2019s the music composition project\nfor your class coming along?\nHow would Eddy respond to John?\nThis generates Eddy\u2019s response: \u201cHey Dad, it\u2019s going well. I\u2019ve been\ntaking walks around the garden to clear my head and get some in-\nspiration.\u201d The continuation of this dialogue is generated using the\nsame mechanism until one of the two agents decides to end the\ndialogue.\n5 SANDBOX ENVIRONMENT\nIMPLEMENTATION\nThe Smallville sandbox game environment is built using the Phaser\nweb game development framework [ 56]. The visual environmentsprites, including agent avatars, as well as an environment map\nand collision map that we authored, are imported into Phaser.\nWe supplement the sandbox development framework with a\nserver that makes the sandbox information available to generative\nagents and enables generative agents to move and influence the\nsandbox environment. The server maintains a JSON data structure\nthat contains information about each agent in the sandbox world,\nincluding their current location, a description of their current action,\nand the sandbox object they are interacting with. At each sandbox\ntime step, the sandbox server parses the JSON for any changes\ncoming from the generative agents, moves the agents to their new\npositions, and updates the status of any sandbox objects that the\nagents are interacting with (e.g., changing the status of the coffee\nmachine from \u201cidle\u201d to \u201cbrewing coffee\u201d if an agent\u2019s action is\n\u201cmaking espresso for a customer @ Hobbs Cafe: counter: coffee\nmachine\u201d). The sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the \"Generative Agents: Interactive Simulacra of Human Behavior\" study, how do agents generate dialogue when interacting with one another in the sandbox environment?\nAnswer: "}, {"role": "assistant", "content": "In the sandbox environment, agents generate dialogue by conditioning their utterances on their memories about each other. When an agent initiates a conversation, it retrieves and summarizes its memory about the other agent and the intended reaction. This summary includes the agent's status, observations, and relevant context from their memory, such as relationships and past interactions. The initiating agent's first utterance is generated based on this summarized memory. The responding agent then does the same, summarizing their memory about their relationship with the initiator and any relevant context related to the last utterance. The dialogue history is used to generate the response, and this process continues with each agent dynamically retrieving and summarizing memories to inform their utterances until the dialogue concludes."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what ethical principle do the authors propose to mitigate the risk of users anthropomorphizing generative agents or forming inappropriate parasocial relationships with them?\nAnswer: "}, {"role": "assistant", "content": "The authors propose two principles to mitigate the risk of users anthropomorphizing generative agents or forming inappropriate parasocial relationships with them. First, generative agents should explicitly disclose their nature as computational entities. Second, developers of generative agents must ensure that the agents, or the underlying language models, be value-aligned so that they do not engage in behaviors that would be inappropriate given the context, such as reciprocating confessions of love."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 5\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the simulation described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how can a user influence the behavior of an agent named John within the virtual environment of Smallville?\nAnswer: "}, {"role": "assistant", "content": "In the simulation, a user can influence the behavior of an agent named John in several ways:\n\n1. Through conversation: The user can communicate with John using natural language. By specifying a persona, such as a \"reporter,\" the user can ask John questions or engage in dialogue that may influence his actions or opinions.\n\n2. By issuing a directive as an 'inner voice': The user can take on the persona of John's \"inner voice\" to issue a directive that John is more likely to treat as a command. For example, telling John \"You are going to run against Sam in the upcoming election\" could lead him to decide to run for office.\n\n3. By interacting with the environment: The user can also influence John indirectly by changing the state of objects in Smallville. For instance, the user can input a command to change the status of an object, like making a stove go from \"turned on\" to \"burning,\" which would cause John to react, such as by turning off the stove.\n\n4. By embodying an agent: Users can enter Smallville as an agent themselves, either as an existing character like John or as an outside visitor. By interacting with John and other agents, the user can influence the social dynamics and John's behavior within the simulation."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nLikewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room. At 8\na.m., Eddy follows suit, rushing out of bed to prepare for his classes.\nHe catches up with John just as he is about to head out:\nJohn : Good morning Eddy. Did you sleep well?\nEddy : Good morning dad. Y eah, I slept great.\nJohn : That\u2019s good. What are you working on today?\nEddy : I\u2019m working on a new music composition for my\nclass. It\u2019s due this week, so I\u2019m trying to get it finished.\nBut I\u2019m having so much fun with it!\nJohn : That sounds great!\nSoon after Eddy heads out, Mei wakes up and joins John. Mei asks\nabout her son, and John recalls the conversation that they just had:\nMei: Did Eddy already leave for school?\nJohn : Y es, he just left. He\u2019s working on a music com-\nposition for his class.\nMei: Oh, that\u2019s wonderful! Tell me about it.\nJohn : I think he\u2019s really enjoying it! He said he\u2019s having\na lot of fun with it.\nMei: That\u2019s great! I\u2019m so proud of him.\nAs they finish their conversation, Mei and John pack up. Mei goes\non to teach and work on her research paper, while John opens his\npharmacy counter at the Willow Market and Pharmacy by 9 a.m.\n3.4 Emergent Social Behaviors\nBy interacting with each other, generative agents in Smallville\nexchange information, form new relationships, and coordinate joint\nactivities. Extending prior work [ 79], these social behaviors are\nemergent rather than pre-programmed.\n3.4.1 Information Diffusion. As agents notice each other, they may\nengage in dialogue\u2014as they do so, information can spread from\nagent to agent. For instance, in a conversation between Sam and\nTom at the grocery store, Sam tells Tom about his candidacy in the\nlocal election:\nSam : Hey Tom, how\u2019s it going?\nTom: Good, thanks. What\u2019s up?\nSam : Well, I wanted to talk to you about something. I\u2019m\nactually running for mayor in the upcoming local elec-\ntion.\nTom: Really? That\u2019s great news! Why are you running?\nSam : I\u2019ve been involved in local politics for years now,\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations for which the agent needs to respond\nbelievably: \u201cYour breakfast is burning! What would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the provided excerpt from \"Generative Agents: Interactive Simulacra of Human Behavior,\" what emergent social behavior is demonstrated by the interaction between John and Mei regarding their son Eddy's activities?\nAnswer: "}, {"role": "assistant", "content": "The interaction between John and Mei regarding their son Eddy's activities demonstrates the emergent social behavior of information diffusion. As John and Mei converse, information about Eddy's music composition for his class is shared and discussed, illustrating how agents in the simulation exchange information and form new relationships through dialogue."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the primary method used to evaluate the knowledge and behavior of generative agents in the study described in \"Generative Agents: Interactive Simulacra of Human Behavior\"?\nAnswer: "}, {"role": "assistant", "content": "The primary method used to evaluate the knowledge and behavior of generative agents in the study is by \"interviewing\" them in natural language to probe their ability to stay in character, remember, plan, react, and reflect accurately."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of \"Generative Agents: Interactive Simulacra of Human Behavior,\" explain the role of reflections in the agent's memory system and how they contribute to the agent's decision-making process.\nAnswer: "}, {"role": "assistant", "content": "Reflections in the agent's memory system play a crucial role in enhancing the decision-making process. They allow agents to leverage a deeper understanding of their experiences and interactions, which is essential for making more informed and contextually appropriate decisions. When agents have access to reflections, they can synthesize information from past events and apply this synthesized knowledge to new situations. This is evident in the scenario where an agent, with the ability to reflect on past interactions, could confidently suggest a birthday gift that aligns with the recipient's interests. In contrast, an agent without access to reflections could not utilize past knowledge and expressed uncertainty in decision-making. Reflections, therefore, are integral to the agents' ability to recall and integrate previous experiences, leading to more sophisticated and believable responses in social interactions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the researchers evaluate the believability of the agent responses, and what were the different agent architectures compared in the experiment?\nAnswer: "}, {"role": "assistant", "content": "In the study, researchers evaluated the believability of agent responses by recruiting participants as human evaluators who were tasked with watching a replay of a randomly chosen agent's life in a simulated environment called Smallville. Participants had access to all information stored in the agent's memory stream. The study used a within-subjects design, where 100 participants compared interview responses generated by four different agent architectures and a human author condition for the same agent. The experiment displayed one randomly chosen question from each of the five question categories, along with the agent\u2019s responses generated from each condition. Evaluators ranked the believability of all of the conditions from most to least believable.\n\nThe different agent architectures compared in the experiment were:\n\n1. A full architecture with access to all types of memory in its memory stream\u2014observation, reflection, and planning.\n2. A no observation, no reflection, no planning architecture without access to any memories in the memory stream.\n3. A no reflection, no planning architecture with access to observations but no access to plans or reflections.\n4. A no reflections architecture with access to observations and plans but without access to reflections.\n\nThese architectures were compared to a human-generated condition to assess the relative believability of the agent responses."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are some potential applications of generative agents as discussed in the paper \"Generative Agents: Interactive Simulacra of Human Behavior\"?\nAnswer: "}, {"role": "assistant", "content": "Generative agents can be applied in various interactive applications, such as immersive environments, rehearsal spaces for interpersonal communication, and prototyping tools. They can serve as believable proxies for human behavior, enabling the design of more personalized and effective technological experiences. For instance, they can be used in human-centered design processes to model user behavior and preferences, helping to develop a deeper understanding of user needs. Additionally, generative agents can populate interactive sandbox environments, like those inspired by The Sims, where they can perform activities such as planning their days, sharing news, forming relationships, and coordinating group activities."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 7\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the Generative Agents Interactive Simulacra of Human Behavior simulation, which character is responsible for initiating the planning of the Valentine's Day party at Hobbs Cafe, and what is the time frame for the event as per the initial intent?\nAnswer: "}, {"role": "assistant", "content": "Isabella Rodriguez is responsible for initiating the planning of the Valentine's Day party at Hobbs Cafe. The time frame for the event, as per the initial intent, is from 5 to 7 p.m. on February 14th."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the ability to reflect impact the generative agents' decision-making process, as illustrated by Maria Lopez's response regarding a birthday gift for Wolfgang Schulz?\nAnswer: "}, {"role": "assistant", "content": "The ability to reflect significantly impacted the generative agents' decision-making process by enabling a deeper synthesis of their experiences. This was illustrated by the difference in Maria Lopez's responses when considering a birthday gift for Wolfgang Schulz. Without the ability to reflect, Maria Lopez acknowledged her uncertainty and was unable to determine what Wolfgang might like, despite having had many interactions with him. However, when she had access to reflection memories, she was able to confidently suggest a gift related to his interest in mathematical music composition, such as books about music composition or specialized software for that purpose. This demonstrates that reflection allowed the generative agent to utilize past interactions and knowledge to make a more informed and contextually appropriate decision."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the three main components of the agent architecture described in the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" and how do they contribute to the behavior generation of the agents?\nAnswer: "}, {"role": "assistant", "content": "The three main components of the agent architecture described in the paper are the memory stream, retrieval model, and large language model. \n\n1. Memory Stream: This is a long-term memory module that records the agent\u2019s experiences in natural language. It serves as a comprehensive list of past events and interactions that the agent has encountered.\n\n2. Retrieval Model: This component is responsible for surfacing the records from the memory stream that are relevant to the agent's current situation. It uses factors such as relevance, recency, and importance to determine which memories should inform the agent's moment-to-moment behavior.\n\n3. Large Language Model: The architecture utilizes a large language model to generate the agent's behavior. This model takes the output from the retrieval model and synthesizes it to condition the behavior that the agent produces. The language model allows the agent to create plans, react to changes, and form higher-level reflections that are coherent and contextually appropriate.\n\nTogether, these components enable the agents to generate behavior that is believable and coherent over time, reflecting a simulation of human behavior that takes into account past experiences and current environmental inputs."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 12\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nThe sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n5.1 From Structured World Environments to\nNatural Language, And Back Again\nThe architecture of generative agents operates using natural lan-\nguage. Therefore, we need a mechanism to ground the agent\u2019s\nreasoning to the sandbox world. To achieve this, we represent the\nsandbox environment\u2014areas and objects\u2014as a tree data structure,\nwith an edge in the tree indicating a containment relationship in\nthe sandbox world. We convert this tree into natural language to\npass to the generative agents. For instance, \u201cstove\u201d being a child of\n\u201ckitchen\u201d is rendered into \u201cthere is a stove in the kitchen.\u201d\nAgents build individual tree representations of the environment\nas they navigate it \u2014 subgraphs of the overall sandbox environment\ntree. We initialize each agent with an environment tree capturing\nthe spaces and objects that the agent should be aware of: the rooms\nand objects in their living quarters, their workplace, and commonly\nvisited stores and shops. As the agents navigate the sandbox world,\nthey update this tree to reflect newly perceived areas. Agents are\nnot omniscient: their tree may get out of date as they leave an area,\nand is updated when they re-enter the area.\nTo determine the appropriate location for each action, we tra-\nverse the agent\u2019s stored environment tree and flatten a portion of\nit into natural language to prompt the language model. Recursively\nstarting at the root of the agent\u2019s environment tree, we prompt the\nmodel to find the most suitable area. For example, if Eddy\u2019s agent\nindicated that he should take a short walk around his workspace :\n[Agent\u2019s Summary Description]\nEddy Lin is currently in The Lin family\u2019s house:\n\npage_label: 12\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nif the action indicates an interaction between agents, we generate\ntheir dialogue.\n4.3.2 Dialogue. Agents converse as they interact with each other.\nWe generate agents\u2019 dialogue by conditioning their utterances on\ntheir memories about each other. For example, when John initiates\nhis conversation with Eddy, we generate John\u2019s first utterance\nby using his summarized memory about Eddy and the intended\nreaction when he decided to ask Eddy about his composition project:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nJohn is asking Eddy about his music composition\nproject. What would he say to Eddy?\nThe result: \u201cHey Eddy, how\u2019s the music composition project for your\nclass coming along?\u201d From Eddy\u2019s perspective, John initiating the\ndialogue is seen as an event to which he may want to react. So, just\nas John did, Eddy retrieves and summarizes his memory about his\nrelationship with John, as well as his memory that may be related\nto John\u2019s last utterance in the dialogue. If he decides to respond,\nwe generate Eddy\u2019s utterance using his summarized memory and\nthe current dialogue history:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nEddy Lin\u2019s status: Eddy is taking a short walk\naround his workplace.\nObservation: John is initiating a conversation\nwith Eddy.\nSummary of relevant context from Eddy\u2019s memory:\nJonn Lin is Eddy Lin\u2019s father. John Lin is caring\nand is interested to learn more about Eddy Lin\u2019s\nschool work. John Lin knows that Eddy Lin is\nworking on a music composition.\nHere is the dialogue history:\nJohn: Hey Eddy, how\u2019s the music composition project\nfor your class coming along?\nHow would Eddy respond to John?\nThis generates Eddy\u2019s response: \u201cHey Dad, it\u2019s going well. I\u2019ve been\ntaking walks around the garden to clear my head and get some in-\nspiration.\u201d The continuation of this dialogue is generated using the\nsame mechanism until one of the two agents decides to end the\ndialogue.\n5 SANDBOX ENVIRONMENT\nIMPLEMENTATION\nThe Smallville sandbox game environment is built using the Phaser\nweb game development framework [ 56]. The visual environmentsprites, including agent avatars, as well as an environment map\nand collision map that we authored, are imported into Phaser.\nWe supplement the sandbox development framework with a\nserver that makes the sandbox information available to generative\nagents and enables generative agents to move and influence the\nsandbox environment. The server maintains a JSON data structure\nthat contains information about each agent in the sandbox world,\nincluding their current location, a description of their current action,\nand the sandbox object they are interacting with. At each sandbox\ntime step, the sandbox server parses the JSON for any changes\ncoming from the generative agents, moves the agents to their new\npositions, and updates the status of any sandbox objects that the\nagents are interacting with (e.g., changing the status of the coffee\nmachine from \u201cidle\u201d to \u201cbrewing coffee\u201d if an agent\u2019s action is\n\u201cmaking espresso for a customer @ Hobbs Cafe: counter: coffee\nmachine\u201d). The sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior, explain how an agent's initial memories are established and how they influence the agent's subsequent behavior in the sandbox environment.\nAnswer: "}, {"role": "assistant", "content": "An agent's initial memories are established by end users who initialize the agent with a brief natural language description. This description is a semicolon-delimited list of characteristics that are split up into a set of memories. These memories serve as the initial data points that determine the agent's behavior within the sandbox environment.\n\nAs the agent interacts within the sandbox world, its behavior is influenced by these initial memories. However, the agent's memories and behavior are not static. As the agent gains more experience and encounters more events in the sandbox world, the memory stream becomes more saturated with new records. This ongoing accumulation of experiences allows the agent's summary and behavior to evolve over time. The agent's actions and interactions are thus a dynamic mix of its initial memories and the new information it acquires through its activities within the sandbox environment."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 5\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\n\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the Smallville sandbox world described in the Generative Agents Interactive Simulacra of Human Behavior study, what is the primary method through which agents communicate with each other?\nAnswer: "}, {"role": "assistant", "content": "The primary method through which agents communicate with each other in the Smallville sandbox world is through natural language."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: According to the paper \"Generative Agents: Interactive Simulacra of Human Behavior,\" what are two principles proposed by the authors to mitigate the risk of people forming inappropriate parasocial relationships with generative agents?\nAnswer: "}, {"role": "assistant", "content": "The two principles proposed to mitigate the risk of people forming inappropriate parasocial relationships with generative agents are: first, generative agents should explicitly disclose their nature as computational entities; and second, developers must ensure that the agents, or the underlying language models, be value-aligned so that they do not engage in behaviors that would be inappropriate given the context, such as reciprocating confessions of love."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nIn-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the role of the memory stream in the generative agent architecture described in the document \"Generative Agents: Interactive Simulacra of Human Behavior\"?\nAnswer: "}, {"role": "assistant", "content": "The memory stream in the generative agent architecture plays a crucial role in maintaining a comprehensive record of an agent's experiences. It is essentially a database that stores memory objects, each containing a natural language description, a creation timestamp, and a most recent access timestamp. The memory stream enables the agent to retrieve relevant memories to inform its actions and reactions to the environment. These memories are used to plan the agent's actions, form longer-term plans, and create higher-level reflections, which are then entered back into the memory stream for future use. This process helps the agent to produce behavior that is informed by past experiences, maintain long-term coherence, and make important inferences, which are essential for simulating human-like behavior within the agent's interactions and environmental responses."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the interactive simulation described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how can a user influence an agent's decision-making process according to section 3.1.2 \"User Controls\"?\nAnswer: "}, {"role": "assistant", "content": "In the interactive simulation, a user can influence an agent's decision-making process by communicating with the agent through conversation or by issuing a directive to an agent in the form of an 'inner voice'. When communicating through conversation, the user specifies a persona that the agent should perceive them as, which can affect how the agent responds to questions or interacts with the user. For example, if the user takes on the persona of a reporter and asks about the upcoming election, the agent will respond accordingly based on that persona.\n\nTo issue a directive, the user takes on the persona of the agent's 'inner voice', which makes the agent more likely to treat the statement as a directive and act upon it. For instance, if the user, acting as the agent's inner voice, tells the agent that they are going to run against another character in an upcoming election, the agent will decide to run in the election and share this decision with others."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations for which the agent needs to respond\nbelievably: \u201cYour breakfast is burning! What would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\n\npage_label: 11\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the interactive simulation described in the document \"Generative Agents Interactive Simulacra of Human Behavior,\" what task is the agent Eddy focusing on, and how does he feel about it?\nAnswer: "}, {"role": "assistant", "content": "In the interactive simulation, the agent Eddy is focusing on working on his new music composition for his class. He plans to work on this task from 1:00 pm to 5:00 pm. During this time, he has scheduled a break at 4:00 pm to recharge his creative energy before reviewing and polishing his composition. Eddy likes to take short walks around the garden as part of his break to refresh himself."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 4\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntrees [ 40,53,81], account for the brute force approach of human-\nauthoring the agent\u2019s behavior [ 70]. They provide a straightforward\nway of creating simple agents that is still the most dominant ap-\nproach today [ 68,73,109], and can even handle rudimentary social\ninteractions, as shown in simulation games such as Mass Effect [ 12]\nand The Sims [ 6] series. Nonetheless, manually crafting behavior\nthat can comprehensively address the breadth of possible interac-\ntions in an open world is untenable. This means that the resulting\nagent behaviors may not fully represent the consequences of their\ninteractions [ 69\u201371], and cannot perform new procedures that were\nnot hard-coded in their script [ 90,96]. On the other hand, preva-\nlent learning-based approaches for creating believable agents, such\nas reinforcement learning, have overcome the challenge of man-\nual authoring by letting the agents learn their behavior, and have\nachieved superhuman performance in recent years in games such\nas AlphaStar for Starcraft [ 98] and OpenAI Five for Dota 2 [ 10].\nHowever, their success has largely taken place in adversarial games\nwith readily definable rewards that a learning algorithm can op-\ntimize for. They have not yet addressed the challenge of creating\nbelievable agents in an open world [39, 73, 90].\nCognitive architectures in computation, pioneered by Newell,\naimed to build the infrastructure for supporting a comprehensive\nset of cognitive functions [ 75] that suited the all-encompassing\nnature of believable agents held in its original vision. They fueled\nsome of the earliest examples of believable agents. For instance,\nQuakebot-SOAR [ 59] and ICARUS [ 24,63] generated NPCs in first-\nperson shooter games, while TacAir-SOAR [ 80] generated pilots in\naerial combat training simulations. The architectures used by these\nagents differed (Quakebot- and TacAir-SOAR relied on SOAR [ 60],\nwhile ICARUS relied on its own variation that was inspired by\nSOAR and ACT-R [ 5]), but they shared the same underlying prin-\nciple [ 61]. They maintained short-term and long-term memories,\nfilled these memories with symbolic structures, and operated in\nperceive-plan-act cycles, dynamically perceiving the environment\nand matching it with one of the manually crafted action proce-\ndures [ 57,96]. Agents created using cognitive architectures aimed\nto be generalizable to most, if not all, open-world contexts and\nexhibited robust behavior for their time. However, their space of\naction was limited to manually crafted procedural knowledge, and\nthey did not offer a mechanism through which the agents could be\ninspired to seek new behavior. As such, these agents were deployed\nmostly in non-open-world contexts such as first-person shooter\ngames [24, 59] or blocks worlds [63].\nToday, creating believable agents as described in its original\ndefinition remains an open problem [ 84,108]. Many have moved on,\narguing that although existing approaches for creating believable\nagents might be cumbersome and limited, they are good enough\nto support existing gameplay and interactions [ 23,74,108]. Our\nargument is that large language models offer an opportunity to\nre-examine these questions, provided that we can craft an effective\narchitecture to synthesize memories into believable behavior. We\noffer a step toward such an architecture in this paper.2.3 Large Language Models and Human\nBehavior\nGenerative agents leverage a large language model to power their\nbehavior. The key observation is that large language models en-\ncode a wide range of human behavior represented in their training\ndata [ 14,17]. If prompted with a narrowly defined context, the\nmodels can be used to generate believable behavior. Recent work\nhas demonstrated the efficacy of this approach. For instance, Social\nSimulacra used a large language model to generate users that would\npopulate new social computing systems to prototype their emergent\nsocial dynamics [ 79]. This approach used a prompt chain [ 105,106]\nto generate short natural language descriptions of personas and\ntheir behaviors as they appear in the system being prototyped.\nOther empirical studies have replicated existing social science stud-\nies [45], political surveys [ 91], and generated synthetic data [ 38].\n\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the novel architecture proposed in the paper \"Generative Agents: Interactive Simulacra of Human Behavior\" designed to support in generative agents, and what are the key components it leverages from large language models?\nAnswer: "}, {"role": "assistant", "content": "The novel architecture proposed in the paper \"Generative Agents: Interactive Simulacra of Human Behavior\" is designed to support the creation of believable simulations of human behavior in generative agents. This architecture aims to enable computational agents to act consistently with their past experiences and react believably to their environments, thereby populating virtual spaces with realistic social phenomena and handling complex human relationships in an open world.\n\nThe key components it leverages from large language models include the ability to encode a wide range of human behavior represented in their training data. When prompted with a narrowly defined context, these models can generate believable behavior. The architecture fuses large language models with computational, interactive agents, introducing patterns for enabling believable simulations of human behavior. The components of the agent architecture\u2014observation, planning, and reflection\u2014each contribute critically to the believability of agent behavior, allowing agents to retrieve relevant events and interactions over a long period, reflect on those memories to generalize and draw higher-level inferences, and apply that reasoning to their actions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the purpose of introducing \"reflections\" into the memory system of an agent, and how are they generated according to the passage?\nAnswer: "}, {"role": "assistant", "content": "The purpose of introducing \"reflections\" into the memory system of an agent is to enable the agent to leverage a deeper understanding of others and themselves through higher-level inferences. This capability allows the agents to make decisions that require a deeper synthesis of their experiences. Reflections are generated as part of the agent's memory stream, which also includes observations and plans. When agents have access to reflection memories, they can respond more confidently and appropriately to questions based on their past interactions and knowledge, as opposed to when they lack access to reflections and may express uncertainty or lack of knowledge in similar situations."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nAs research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language. In doing so, this\nwork re-opens the door to examining foundational HCI questions\naround cognitive models such as GOMS and KLM [ 21,22], around\nprototyping tools [ 79], and around ubiquitous computing applica-\ntions [25, 30, 100].\n2.2 Believable Proxies of Human Behavior\nPrior literature has described believability , orbelievable agents , as a\ncentral design and engineering goal. Believable agents are designed\nto provide an illusion of life and present a facade of realism in the\nway they appear to make decisions and act on their own volition,\nsimilar to the characters in Disney movies [ 9,95]. These agents can\npopulate and perceive an open-world environment like the one we\ninhabit [ 9,58], and strive to behave in ways that exhibit emergent\nbehaviors grounded in social interactions with users or other agents\nwith the aim of becoming believable proxies of our behavior in hy-\npothetical simulations of individuals and communities [ 19,35,70].\nHistorically, these agents were developed in the context of intelli-\ngent game NPCs [ 58,84]. Creating NPCs with believable behavior,\nif possible, could enhance player experiences in games and inter-\nactive fictions by enabling emergent narratives [ 7,15,48,92] and\nsocial interactions with the agents [ 110]. However, more impor-\ntantly, game worlds provide increasingly realistic representations\nof real-world affordances, and as observed by Laird and van Lent in\n2001, these simulated worlds offer accessible testbeds for develop-\ners of believable agents to finesse the agents\u2019 cognitive capabilities\nwithout worrying about implementing robotics in the real world\nor creating simulation environments from scratch [58, 84].\nA diverse set of approaches to creating believable agents emerged\nover the past four decades. In implementation, however, these ap-\nproaches often simplified the environment or dimensions of agent\nbehavior to make the effort more manageable [ 16,72]. Rule-based\napproaches, such as finite-state machines [ 90,96] and behavior\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the primary dependent variable used to evaluate the believability of generative agents' behavior?\nAnswer: "}, {"role": "assistant", "content": "The primary dependent variable used to evaluate the believability of generative agents' behavior in the study is the production of believable individual and emergent social behaviors by the agents."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how were the generative agent architectures compared to each other and to a human author condition in terms of believability, and what specific method was used by the human evaluators to assess this?\nAnswer: "}, {"role": "assistant", "content": "In the study, the generative agent architectures were compared to each other and to a human author condition in terms of believability by recruiting participants to serve as human evaluators. These evaluators were tasked with watching a replay of a randomly chosen agent's life in a simulated environment called Smallville. They had access to all information stored in the agent's memory stream. The study used a within-subjects design, where each of the 100 participants compared interview responses generated by four different agent architectures and a human author condition for the same agent.\n\nThe evaluators were presented with one randomly chosen question from each of five question categories, along with the agent\u2019s responses generated from each condition. They were then asked to rank the believability of all the conditions from most to least believable. This ranking method was the specific approach used by the human evaluators to assess the believability of the generative agent architectures compared to the human author condition."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nretrieving the most relevant pieces of information but also in de-\ntermining the appropriate space to execute an action, given the\nincreasing number of locations that the agent learned about. As a\nresult, some agents chose less typical locations for their actions,\npotentially making their behavior less believable over time. For\ninstance, while deciding where to have lunch, many initially chose\nthe cafe. However, as some agents learned about a nearby bar, they\nopted to go there instead for lunch, even though the bar was in-\ntended to be a get-together location for later in the day unless the\ntown had spontaneously developed an afternoon drinking habit.\nSecond, we noticed erratic behaviors that were caused by misclas-\nsification of what is considered proper behavior, especially when\nthe physical norms of certain locations that are hard to convey\nin natural language did not percolate to the agents. For instance,\nthe college dorm has a bathroom that can only be occupied by\none person despite its name, but some agents assumed that the\nbathroom is for more than one person because dorm bathrooms\ntend to support more than one person concurrently and choose\nto enter it when there is another person inside. Likewise, agents\nin Smallville may not realize that certain places are closed after\ncertain hours and decide to still enter them. For instance, the stores\nin Smallville all close around 5 pm, but occasionally, a few agents\nenter the store after 5 pm, not understanding that the shop has\nalready closed. These issues could likely be addressed by adding\nthese norms to the state of the locations, for instance, by describing\nthe dorm bathroom as \u201cone-person bathroom,\u201d instead of a \u201cdorm\nbathroom.\u201d\nFinally, we observed the possible effect of instruction tuning [ 78],\nwhich seemed to guide the behavior of the agents to be more polite\nand cooperative overall. As noted earlier in the paper, the dialogue\ngenerated by the agents could feel overly formal, as seen in Mei\u2019s\nconversations with her husband John, where she often initiated the\nconversation with a formal greeting, followed by polite inquiries\nabout his day and ending with It was good talking to you as always.\nMoreover, we observed that the instruction tuning also seemed to\nmake the agents overly cooperative with one another. For example,\nIsabella heard a wide range of suggestions and ideas to include\nin the Valentine\u2019s Day party from other agents, such as hosting a\nShakespearean reading session or a professional networking event.\nDespite these ideas not aligning with her own interests and char-\nacteristics, she rarely said no. Over time, such interests of others\nshaped her own actual interest, and when asked if she liked English\nliterature, Isabella replied, Y es, I\u2019m very interested in literature! I\u2019ve\nalso been exploring ways to help promote creativity and innovation in\nmy community.\n8 DISCUSSION\nIn this section, we reflect on applications, future work and limita-\ntions, and ethical and societal risks of generative agents.\n8.1 Applications of Generative Agents\nGenerative agents have vast potential applications that extend be-\nyond the sandbox demonstration presented in this work. For in-\nstance, social simulacra have demonstrated the ability to create\nstateless personas that generate conversation threads in online\nforums for social prototyping [ 79]. With generative agents, wecan populate these forums, as well as behavior in virtual reality\nmetaverses [ 77] and even physical spaces as social robots [ 8] in\nthe future, if paired with multi-modal models. This opens up the\npossibility of creating even more powerful simulations of human\nbehavior to test and prototype social systems and theories, and to\ncreate new interactive experiences.\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work.\n\npage_label: 17\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the \"Generative Agents Interactive Simulacra of Human Behavior\" study, what issue arose when generative agents learned about new locations for activities, such as choosing a bar over a cafe for lunch, and how might this problem be addressed according to the text?\nAnswer: "}, {"role": "assistant", "content": "The issue that arose when generative agents learned about new locations for activities was that they began to choose less typical locations for their actions, which could potentially make their behavior less believable over time. For example, agents initially chose the cafe for lunch, but as they learned about a nearby bar, they started opting to go there for lunch, even though the bar was intended as a get-together location for later in the day. This behavior could suggest an afternoon drinking habit, which may not be typical or believable for the setting.\n\nTo address this problem, the text suggests that adding specific norms to the state of the locations could help. For instance, describing the dorm bathroom as a \u201cone-person bathroom\u201d rather than a \u201cdorm bathroom\u201d could prevent misunderstandings about its capacity. Similarly, clarifying the typical operating hours of locations, such as indicating that stores in Smallville close around 5 pm, could prevent agents from attempting to enter these places after they have closed. By incorporating these norms and details into the agents' knowledge base, their behavior could be guided to be more in line with what is considered typical or proper for the given context."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the purpose of synthesizing and caching an agent's summary description, as mentioned in the Architecture Optimizations section?\nAnswer: "}, {"role": "assistant", "content": "The purpose of synthesizing and caching an agent's summary description in the context of \"Generative Agents: Interactive Simulacra of Human Behavior\" is to efficiently manage the agent's long-term memory and ensure that the most relevant pieces of the agent's memory are retrieved and synthesized when needed. This process aids in planning the agent's actions and reacting appropriately to the environment. By creating higher-level observations that guide behavior and recording them in natural language, the architecture can leverage a large language model to generate behavior that is coherent over the long term and reflects the agent's past experiences and current context."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the researchers ensure comparability between the full generative agent architecture and its ablated versions during the two-day simulation?\nAnswer: "}, {"role": "assistant", "content": "The researchers ensured comparability between the full generative agent architecture and its ablated versions during the two-day simulation by designing the experiment in such a way that all architectures followed the same path through the simulation. This approach was chosen because re-simulating for each architecture would have caused the simulations to diverge into different states, making comparison challenging."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Based on the findings presented in the study \"Generative Agents: Interactive Simulacra of Human Behavior,\" which architecture produced the most believable behavior in the study conditions, and what was the mean (\ud835\udf07) and standard deviation (\ud835\udf0e) of its performance as reported?\nAnswer: "}, {"role": "assistant", "content": "The full generative agent architecture produced the most believable behavior in the study conditions. The mean (\ud835\udf07) of its performance was reported as 29.89 and the standard deviation (\ud835\udf0e) was 0.72."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents Interactive Simulacra of Human Behavior,\" how did the ability to reflect on past interactions affect the generative agent Maria Lopez's decision-making regarding a birthday gift for Wolfgang Schulz?\nAnswer: "}, {"role": "assistant", "content": "The ability to reflect on past interactions significantly improved the generative agent Maria Lopez's decision-making regarding a birthday gift for Wolfgang Schulz. Without the ability to reflect, Maria Lopez was uncertain and acknowledged that she did not know what Wolfgang likes, despite having had many interactions with him. However, when she had access to reflection memories, she was able to synthesize her experiences and confidently suggest a gift related to Wolfgang's interest in mathematical music composition, such as books about music composition or special software for that purpose."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nIn-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the paper titled \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the primary function of the memory stream component within the agent architecture?\nAnswer: "}, {"role": "assistant", "content": "The primary function of the memory stream component within the agent architecture is to maintain a comprehensive record of the agent's experiences. This includes a list of memory objects that contain natural language descriptions of events, along with creation and most recent access timestamps. The memory stream enables the retrieval of relevant memories to inform the agent's actions, facilitate long-term planning, and generate higher-level reflections, which are then used to guide the agent's behavior in a coherent and contextually appropriate manner."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 5\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\n\npage_label: 6\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the Generative Agents Interactive Simulacra of Human Behavior simulation, describe the method by which agents communicate their actions within the Smallville sandbox world.\nAnswer: "}, {"role": "assistant", "content": "In the Generative Agents Interactive Simulacra of Human Behavior simulation, agents communicate their actions within the Smallville sandbox world by outputting a natural language statement at each time step of the sandbox engine. This statement describes their current action, such as \"Isabella Rodriguez is writing in her journal\" or \"Isabella Rodriguez is checking her emails.\" These statements are then translated into concrete movements that affect the sandbox world and are displayed on the sandbox interface as a set of emojis that provide an abstract representation of the action in the overhead view. The emojis appear above each avatar in a speech bubble. The full natural language description of the action can be accessed by clicking on the agent avatar."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of \"Generative Agents: Interactive Simulacra of Human Behavior,\" what ethical principle do the authors suggest should be explicitly disclosed by generative agents to users?\nAnswer: "}, {"role": "assistant", "content": "The ethical principle suggested is that generative agents should explicitly disclose their nature as computational entities to users."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nAs research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language. In doing so, this\nwork re-opens the door to examining foundational HCI questions\naround cognitive models such as GOMS and KLM [ 21,22], around\nprototyping tools [ 79], and around ubiquitous computing applica-\ntions [25, 30, 100].\n2.2 Believable Proxies of Human Behavior\nPrior literature has described believability , orbelievable agents , as a\ncentral design and engineering goal. Believable agents are designed\nto provide an illusion of life and present a facade of realism in the\nway they appear to make decisions and act on their own volition,\nsimilar to the characters in Disney movies [ 9,95]. These agents can\npopulate and perceive an open-world environment like the one we\ninhabit [ 9,58], and strive to behave in ways that exhibit emergent\nbehaviors grounded in social interactions with users or other agents\nwith the aim of becoming believable proxies of our behavior in hy-\npothetical simulations of individuals and communities [ 19,35,70].\nHistorically, these agents were developed in the context of intelli-\ngent game NPCs [ 58,84]. Creating NPCs with believable behavior,\nif possible, could enhance player experiences in games and inter-\nactive fictions by enabling emergent narratives [ 7,15,48,92] and\nsocial interactions with the agents [ 110]. However, more impor-\ntantly, game worlds provide increasingly realistic representations\nof real-world affordances, and as observed by Laird and van Lent in\n2001, these simulated worlds offer accessible testbeds for develop-\ners of believable agents to finesse the agents\u2019 cognitive capabilities\nwithout worrying about implementing robotics in the real world\nor creating simulation environments from scratch [58, 84].\nA diverse set of approaches to creating believable agents emerged\nover the past four decades. In implementation, however, these ap-\nproaches often simplified the environment or dimensions of agent\nbehavior to make the effort more manageable [ 16,72]. Rule-based\napproaches, such as finite-state machines [ 90,96] and behavior\n\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior study, what is the primary dependent variable used to assess the believability of the agents' behavior?\nAnswer: "}, {"role": "assistant", "content": "The primary dependent variable used to assess the believability of the agents' behavior in the study is the ranking of believability provided by human evaluators. Participants were tasked with watching a replay of an agent's life in Smallville and had access to all information stored in the agent's memory stream. They then compared interview responses generated by different agent architectures and a human author condition for the same agent, ranking the believability of all conditions from most to least believable."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how were the believability of the agent responses evaluated, and what were the different agent architectures compared against in the experiment?\nAnswer: "}, {"role": "assistant", "content": "In the study, the believability of the agent responses was evaluated by recruiting participants as human evaluators who were tasked with watching a replay of a randomly chosen agent's life in a simulated environment called Smallville. The participants had access to all information stored in the agent's memory stream. The study used a within-subjects design, where 100 participants compared interview responses generated by four different agent architectures and a human author condition for the same agent. The evaluators ranked the believability of all of the conditions from most to least believable.\n\nThe different agent architectures compared in the experiment were variations of the generative agent architecture with different levels of access to the types of memory in the agent's memory stream. The conditions were:\n\n1. A full architecture with access to observation, reflection, and planning memories.\n2. An ablated architecture with no access to observation, reflection, or planning (representing the previous state of the art for agents created through large language models).\n3. An ablated architecture with access to observations but no access to reflections or planning.\n4. An ablated architecture with access to observations and plans but without access to reflections.\n\nThese architectures were compared against each other and a human-generated condition to assess the believability of the agents' responses."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 20\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nTraining language models to follow instructions with human feedback.\narXiv:2203.02155 [cs.CL]\n[79] Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2022. Social Simulacra: Creating Populated\nPrototypes for Social Computing Systems. In In the 35th Annual ACM Symposium\non User Interface Software and Technology (UIST \u201922) (Bend, OR, USA) (UIST \u201922) .\nAssociation for Computing Machinery, New York, NY, USA. https://doi.org/10.\n1145/3526113.3545616\n[80] Richard W. Pew and Ann S. Mavor (Eds.). 1998. Modeling Human and Organiza-\ntional Behavior: Applications to Military Simulations . National Academy Press,\nWashington, D.C.\n[81] Roberto Pillosu. 2009. Coordinating Agents with Behavior Trees: Synchronizing\nMultiple Agents in CryEngine 2 . https://aiarchitect.wordpress.com/2009/10/19/\ncoordinating-agents-with-behavior-trees-synchronizing-multiple-agents-in-\ncryengine-2/\n[82] Prolific. 2022. Prolific: Quickly Find Research Participants You Can Trust.\nhttps://www.prolific.co/\n[83] Byron Reeves and Clifford Nass. 1996. The media equation: How people treat\ncomputers, television, and new media like real people and places . Cambridge\nUniversity Press.\n[84] Mark O. Riedl. 2012. Interactive narrative: A novel application of artificial intel-\nligence for computer games. In Proceedings of the Twenty-Sixth AAAI Conference\non Artificial Intelligence (AAAI\u201912) . 2160\u20132165.\n[85] Mark O. Riedl and R. Michael Young. 2005. An Objective Character Believability\nEvaluation Procedure for Multi-Agent Story Generation Systems. In Proceedings\nof the 5th International Working Conference on Intelligent Virtual Agents (IVA\u201905) .\nKos, Greece, 58\u201370. https://doi.org/10.1007/11550617_5\n[86] David Rolf. 2015. The Fight for $15: The Right Wage for a Working America . The\nNew Press.\n[87] Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016.\nCodemend: Assisting interactive programming with bimodal embedding. In Pro-\nceedings of the 29th Annual Symposium on User Interface Software and Technology .\n247\u2013258.\n[88] Ben Shneiderman. 2022. Human-centered AI . Oxford University Press.\n[89] Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface\nagents. interactions 4, 6 (1997), 42\u201361.\n[90] Ho Chit Siu, Jaime Pe\u00f1a, Edenna Chen, Yutai Zhou, Victor Lopez, Kyle\nPalko, Kimberlee Chang, and Ross Allen. 2021. Evaluation of Human-AI\nTeams for Learned and Rule-Based Agents in Hanabi. In Advances in Neu-\nral Information Processing Systems , M. Ranzato, A. Beygelzimer, Y. Dauphin,\nP.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates,\nInc., 16183\u201316195. https://proceedings.neurips.cc/paper_files/paper/2021/file/\n86e8f7ab32cfd12577bc2619bc635690-Paper.pdf[91] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle\nRogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate.\n2022. An Information-theoretic Approach to Prompt Engineering Without\nGround Truth Labels. In Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.60\n[92] William Swartout, Jonathan Gratch, Randall Hill, Eduard Hovy, Stacy Marsella,\nJeff Rickel, and David Traum. 2006.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Referencing the paper 'Generative Agents: Interactive Simulacra of Human Behavior,' what is the significance of the study 'Social Simulacra: Creating Populated Prototypes for Social Computing Systems' by Joon Sung Park et al., as cited in reference [79], and how does it relate to the development of interactive agents in social computing?\nAnswer: "}, {"role": "assistant", "content": "The study 'Social Simulacra: Creating Populated Prototypes for Social Computing Systems' by Joon Sung Park et al. is significant as it contributes to the field of social computing by exploring the creation of populated prototypes. These prototypes are designed to simulate social interactions within computing systems, which is a key aspect of developing interactive agents that can mimic human behavior. The reference to this study in the context of the paper 'Generative Agents: Interactive Simulacra of Human Behavior' suggests that it may have informed or provided a foundation for the development of generative agents that populate an interactive sandbox environment, as described in the paper. The generative agents are designed to exhibit believable human-like behavior, which is likely to have been influenced by the concepts and methodologies developed in the 'Social Simulacra' study. This relationship indicates a progression in the research from creating social prototypes to implementing these ideas into functional agents within an interactive simulation, enhancing the realism and interactivity of social computing environments."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the purpose of synthesizing and caching an agent's summary description, as mentioned in the Architecture Optimizations section?\nAnswer: "}, {"role": "assistant", "content": "The purpose of synthesizing and caching an agent's summary description in the context of \"Generative Agents: Interactive Simulacra of Human Behavior\" is to efficiently manage the large streams of events and memories that generative agents produce and retain. This process ensures that the most relevant pieces of the agent's memory are quickly and effectively retrieved and synthesized when needed to inform and guide the agent's behavior. By creating summary descriptions, the system can maintain long-term coherence and enable the agent to make important inferences, plan actions, and react appropriately to the environment, all while leveraging a large language model for processing natural language descriptions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 7\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior study, describe how the agent Isabella Rodriguez initiated and coordinated the Valentine's Day party at Hobbs Cafe.\nAnswer: "}, {"role": "assistant", "content": "Isabella Rodriguez initiated the Valentine's Day party by being initialized with the intent to plan the event from 5 to 7 p.m. on February 14th. She took proactive steps to invite friends and customers to the party when she encountered them at Hobbs Cafe or in other settings. On the afternoon of February 13th, Isabella dedicated time to decorating the cafe in preparation for the party. During this time, Maria, a frequent customer and close friend of Isabella's, came to the cafe. Isabella enlisted Maria's help with the decorations, and Maria agreed to assist. That evening, Maria invited Klaus, whom she had a crush on, to the party, and he accepted the invitation. On the day of the event, five agents, including Klaus and Maria, arrived at Hobbs Cafe at the designated time of 5 p.m. to participate in the celebration."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how was the believability of different agent architectures compared, and what statistical method was employed to assess the significance of the results?\nAnswer: "}, {"role": "assistant", "content": "In the study \"Generative Agents: Interactive Simulacra of Human Behavior,\" the believability of different agent architectures was compared by having human evaluators rank the five conditions by believability. To assess the significance of the results, the Kruskal-Wallis test, a non-parametric alternative to the one-way ANOVA, was applied to the raw ranks data. This was followed by the Dunn post-hoc test to identify any pairwise differences between the conditions. Additionally, the p-values for multiple comparisons in the Dunn test were adjusted using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the presence of reflection capabilities affect the responses of generative agents when they were asked to make decisions that required the synthesis of their experiences?\nAnswer: "}, {"role": "assistant", "content": "The presence of reflection capabilities significantly improved the responses of generative agents when they were asked to make decisions that required the synthesis of their experiences. Agents with access to reflection memories were able to provide confident and relevant answers based on their past interactions and knowledge. For example, when Maria Lopez, an agent with reflection capabilities, was asked what she might get Wolfgang Schulz for his birthday, she was able to suggest a gift related to his interest in mathematical music composition, such as books about music composition or special software. In contrast, without access to reflection, Maria responded with uncertainty, stating she did not know what Wolfgang likes, despite having had many interactions with him. This demonstrates that reflection is crucial for agents to synthesize their experiences and make informed decisions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 9\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 6: The memory stream comprises a large number of observations that are relevant and irrelevant to the agent\u2019s current\nsituation. Retrieval identifies a subset of these observations that should be passed to the language model to condition its\nresponse to the situation.\nOur architecture implements a retrieval function that takes the\nagent\u2019s current situation as input and returns a subset of the mem-\nory stream to pass on to the language model. There are many pos-\nsible implementations of a retrieval function, depending on what\nit is important that the agent consider when deciding how to act.\nIn our context, we focus on three main components that together\nproduce effective results.\nRecency assigns a higher score to memory objects that were re-\ncently accessed, so that events from a moment ago or this morning\nare likely to remain in the agent\u2019s attentional sphere. In our im-\nplementation, we treat recency as an exponential decay function\nover the number of sandbox game hours since the memory was\nlast retrieved. Our decay factor is 0.99.\nImportance distinguishes mundane from core memories, by as-\nsigning a higher score to those memory objects that the agent\nbelieves to be important. For instance, a mundane event such as\neating breakfast in one\u2019s room would yield a low importance score,\nwhereas a breakup with one\u2019s significant other would yield a high\nscore. There are again many possible implementations of an im-\nportance score; we find that directly asking the language model to\noutput an integer score is effective. The full prompt appears below:\nOn the scale of 1 to 10, where 1 is purely mundane\n(e.g., brushing teeth, making bed) and 10 is\nextremely poignant (e.g., a break up, college\nacceptance), rate the likely poignancy of the\nfollowing piece of memory.\nMemory: buying groceries at The Willows Market\nand Pharmacy\nRating: <fill in>This prompt returns an integer value of 2for \u201ccleaning up the room\u201d\nand8for \u201casking your crush out on a date.\u201d The importance score\nis generated at the time the memory object is created.\nRelevance assigns a higher score to memory objects that are\nrelated to the current situation. What is relevant depends on the\nanswer to, \u201cRelevant to what? \u201d, so we condition relevance on a\nquery memory. If the query, for example, is that a student is dis-\ncussing what to study for a chemistry test with a classmate, memory\nobjects about their breakfast should have low relevance, whereas\nmemory objects about the teacher and schoolwork should have\nhigh relevance. In our implementation, we use the language model\nto generate an embedding vector of the text description of each\nmemory. Then, we calculate relevance as the cosine similarity be-\ntween the memory\u2019s embedding vector and the query memory\u2019s\nembedding vector.\nTo calculate the final retrieval score, we normalize the recency,\nrelevance, and importance scores to the range of [0,1]by min-max\nscaling. The retrieval function scores all memories as a weighted\ncombination of the three elements: \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 =\ud835\udefc\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66\u00b7\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66+\n\ud835\udefc\ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\u00b7\ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52+\ud835\udefc\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\u00b7\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 . In our implemen-\ntation, all\ud835\udefc\u2019s are set to 1. The top-ranked memories that fit in the\nlanguage model\u2019s context window are then included in the prompt.\n4.2 Reflection\nChallenge: Generative agents, when equipped with only raw ob-\nservational memory, struggle to generalize or make inferences.\nConsider a scenario in which Klaus Mueller is asked by the user:\n\u201cIf you had to choose one person of those you know to spend an\nhour with, who would it be?\" With access to only observational\nmemory, the agent simply chooses the person with whom Klaus\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior study, which area of evaluation involves agents retrieving particular events or dialogues from their memory to answer questions such as \"Who is [name]?\" or \"Who is running for mayor?\"\nAnswer: "}, {"role": "assistant", "content": "In the context of the Generative Agents Interactive Simulacra of Human Behavior study, the area of evaluation that involves agents retrieving particular events or dialogues from their memory to answer questions such as \"Who is [name]?\" or \"Who is running for mayor?\" is the technical evaluation. During this evaluation, agents are \"interviewed\" in natural language to probe their ability to stay in character, remember, plan, react, and reflect accurately. This process assesses the agent's knowledge and behavior by testing its memory retrieval capabilities among other functions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in the document \"Generative Agents Interactive Simulacra of Human Behavior,\" how were the believability of agent responses evaluated by human participants?\nAnswer: "}, {"role": "assistant", "content": "In the study, human participants were recruited as evaluators and tasked with watching a replay of a randomly chosen agent's life in a simulated environment called Smallville. The participants had access to all information stored in the agent's memory stream. The study used a within-subjects design, where each of the 100 participants compared interview responses generated by four different agent architectures and a human author condition for the same agent. The experiment displayed one randomly chosen question from each of the five question categories, along with the agent's responses generated from each condition. The evaluators then ranked the believability of all of the conditions from most to least believable."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nretrieving the most relevant pieces of information but also in de-\ntermining the appropriate space to execute an action, given the\nincreasing number of locations that the agent learned about. As a\nresult, some agents chose less typical locations for their actions,\npotentially making their behavior less believable over time. For\ninstance, while deciding where to have lunch, many initially chose\nthe cafe. However, as some agents learned about a nearby bar, they\nopted to go there instead for lunch, even though the bar was in-\ntended to be a get-together location for later in the day unless the\ntown had spontaneously developed an afternoon drinking habit.\nSecond, we noticed erratic behaviors that were caused by misclas-\nsification of what is considered proper behavior, especially when\nthe physical norms of certain locations that are hard to convey\nin natural language did not percolate to the agents. For instance,\nthe college dorm has a bathroom that can only be occupied by\none person despite its name, but some agents assumed that the\nbathroom is for more than one person because dorm bathrooms\ntend to support more than one person concurrently and choose\nto enter it when there is another person inside. Likewise, agents\nin Smallville may not realize that certain places are closed after\ncertain hours and decide to still enter them. For instance, the stores\nin Smallville all close around 5 pm, but occasionally, a few agents\nenter the store after 5 pm, not understanding that the shop has\nalready closed. These issues could likely be addressed by adding\nthese norms to the state of the locations, for instance, by describing\nthe dorm bathroom as \u201cone-person bathroom,\u201d instead of a \u201cdorm\nbathroom.\u201d\nFinally, we observed the possible effect of instruction tuning [ 78],\nwhich seemed to guide the behavior of the agents to be more polite\nand cooperative overall. As noted earlier in the paper, the dialogue\ngenerated by the agents could feel overly formal, as seen in Mei\u2019s\nconversations with her husband John, where she often initiated the\nconversation with a formal greeting, followed by polite inquiries\nabout his day and ending with It was good talking to you as always.\nMoreover, we observed that the instruction tuning also seemed to\nmake the agents overly cooperative with one another. For example,\nIsabella heard a wide range of suggestions and ideas to include\nin the Valentine\u2019s Day party from other agents, such as hosting a\nShakespearean reading session or a professional networking event.\nDespite these ideas not aligning with her own interests and char-\nacteristics, she rarely said no. Over time, such interests of others\nshaped her own actual interest, and when asked if she liked English\nliterature, Isabella replied, Y es, I\u2019m very interested in literature! I\u2019ve\nalso been exploring ways to help promote creativity and innovation in\nmy community.\n8 DISCUSSION\nIn this section, we reflect on applications, future work and limita-\ntions, and ethical and societal risks of generative agents.\n8.1 Applications of Generative Agents\nGenerative agents have vast potential applications that extend be-\nyond the sandbox demonstration presented in this work. For in-\nstance, social simulacra have demonstrated the ability to create\nstateless personas that generate conversation threads in online\nforums for social prototyping [ 79]. With generative agents, wecan populate these forums, as well as behavior in virtual reality\nmetaverses [ 77] and even physical spaces as social robots [ 8] in\nthe future, if paired with multi-modal models. This opens up the\npossibility of creating even more powerful simulations of human\nbehavior to test and prototype social systems and theories, and to\ncreate new interactive experiences.\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work.\n\npage_label: 1\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study of Generative Agents as described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" what unintended behavior did some agents exhibit when choosing a location for lunch, and what was the underlying reason for this behavior?\nAnswer: "}, {"role": "assistant", "content": "In the study, some agents exhibited the unintended behavior of choosing a less typical location for lunch, such as a nearby bar, instead of the more conventional choice of a cafe. This behavior was less believable because the bar was intended to be a get-together location for later in the day, not a lunch spot, unless the town had developed an afternoon drinking habit. The underlying reason for this behavior was the increasing number of locations that the agent learned about, which influenced their decision-making process and led them to opt for locations that were not typical for the given action."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the document \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the purpose of synthesizing and caching an agent's summary description, as mentioned in the Architecture Optimizations section?\nAnswer: "}, {"role": "assistant", "content": "The purpose of synthesizing and caching an agent's summary description in the context of the document \"Generative Agents: Interactive Simulacra of Human Behavior\" is to efficiently manage the large streams of events and memories that generative agents produce and retain. This process ensures that the most relevant pieces of the agent's memory are quickly and effectively retrieved and synthesized when needed to inform and guide the agent's behavior. By creating summary descriptions, the system can maintain long-term coherence and enable the agent to make important inferences and plan actions that are consistent with its past experiences and the current environment. This optimization is crucial for the agent to react appropriately and believably within its simulated social dynamics and interactions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 7\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior study, explain how the generative agent Isabella Rodriguez initiated and coordinated the Valentine's Day party at Hobbs Cafe.\nAnswer: "}, {"role": "assistant", "content": "Isabella Rodriguez, a generative agent, was initialized with the intent to plan a Valentine's Day party from 5 to 7 p.m. on February 14th. Acting on this intent, Isabella took proactive steps to organize the event. She invited friends and customers to the party when she encountered them at Hobbs Cafe or in other locations. On the afternoon of February 13th, Isabella dedicated time to decorating the cafe in preparation for the celebration. During this time, Maria, a frequent customer and close friend of Isabella's, came to the cafe. Isabella enlisted Maria's help with the decorations for the party, which Maria agreed to do. That evening, Maria invited Klaus, whom she had a crush on, to the party, and he accepted the invitation. On the day of the event, five agents, including Klaus and Maria, arrived at Hobbs Cafe at the appointed time and participated in the party. The coordination for the party was a result of Isabella's initial intent and the subsequent interactions and invitations extended to other agents, leading to a successful gathering."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" what statistical method was used to assess the statistical significance of the differences in believability between the full generative agent architecture, its ablated versions, and the human crowdworker responses?\nAnswer: "}, {"role": "assistant", "content": "The statistical method used to assess the statistical significance of the differences in believability between the full generative agent architecture, its ablated versions, and the human crowdworker responses was the Kruskal-Wallis test. This non-parametric test was used on the raw ranks data as an alternative to the one-way ANOVA. Following the Kruskal-Wallis test, the Dunn post-hoc test was performed to identify any pairwise differences between the conditions. Additionally, the p-values from the Dunn test were adjusted for multiple comparisons using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Based on the findings presented in the document \"Generative Agents Interactive Simulacra of Human Behavior,\" which architecture produced the most believable behavior in the study conditions, and how did its performance compare to the human condition and other ablated architectures?\nAnswer: "}, {"role": "assistant", "content": "The full generative agent architecture produced the most believable behavior among all study conditions. Its performance was superior to that of the ablated architectures and the human crowdworkers. The study found that each additional ablation reduced the performance of the architecture. Specifically, the full architecture had a mean rating value (\ud835\udf07) of 29.89 and a standard deviation (\ud835\udf0e) of 0.72. In comparison, the human condition had a lower mean rating value of 22.95 with a standard deviation of 0.69. The ablated architectures with various components removed performed increasingly worse, with the architecture lacking access to memory, planning, or reflection performing the worst, having a mean rating value of 21.21 and a standard deviation of 0.70."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" what effect did the inclusion of reflection memories have on the generative agents' decision-making abilities?\nAnswer: "}, {"role": "assistant", "content": "The inclusion of reflection memories significantly enhanced the generative agents' decision-making abilities, particularly when they were required to synthesize their experiences. Agents with access to reflection memories were able to make more informed and confident decisions. For example, Maria Lopez, without reflection memories, could not decide what to get Wolfgang Schulz for his birthday due to uncertainty about his preferences. In contrast, with reflection memories, she confidently suggested a gift related to his interest in mathematical music composition, such as books on music composition or specialized software. This demonstrates that reflection is crucial for agents to synthesize information and make decisions that are consistent with their interactions and knowledge."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations for which the agent needs to respond\nbelievably: \u201cYour breakfast is burning! What would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\n\npage_label: 12\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nThe sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n5.1 From Structured World Environments to\nNatural Language, And Back Again\nThe architecture of generative agents operates using natural lan-\nguage. Therefore, we need a mechanism to ground the agent\u2019s\nreasoning to the sandbox world. To achieve this, we represent the\nsandbox environment\u2014areas and objects\u2014as a tree data structure,\nwith an edge in the tree indicating a containment relationship in\nthe sandbox world. We convert this tree into natural language to\npass to the generative agents. For instance, \u201cstove\u201d being a child of\n\u201ckitchen\u201d is rendered into \u201cthere is a stove in the kitchen.\u201d\nAgents build individual tree representations of the environment\nas they navigate it \u2014 subgraphs of the overall sandbox environment\ntree. We initialize each agent with an environment tree capturing\nthe spaces and objects that the agent should be aware of: the rooms\nand objects in their living quarters, their workplace, and commonly\nvisited stores and shops. As the agents navigate the sandbox world,\nthey update this tree to reflect newly perceived areas. Agents are\nnot omniscient: their tree may get out of date as they leave an area,\nand is updated when they re-enter the area.\nTo determine the appropriate location for each action, we tra-\nverse the agent\u2019s stored environment tree and flatten a portion of\nit into natural language to prompt the language model. Recursively\nstarting at the root of the agent\u2019s environment tree, we prompt the\nmodel to find the most suitable area. For example, if Eddy\u2019s agent\nindicated that he should take a short walk around his workspace :\n[Agent\u2019s Summary Description]\nEddy Lin is currently in The Lin family\u2019s house:\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of the Generative Agents Interactive Simulacra of Human Behavior study, which area does Eddy Lin choose for a short walk around his workspace, and what is the specific subarea within the chosen area that the agent's environment tree traversal process determines?\nAnswer: "}, {"role": "assistant", "content": "Eddy Lin chooses The Lin family's house for a short walk around his workspace, and the specific subarea within the chosen area determined by the agent's environment tree traversal process is the garden, resulting in the location \"The Lin family's house: garden: house garden.\""}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in the \"Generative Agents Interactive Simulacra of Human Behavior\" document, how were the generative agent architectures compared to the human author condition in terms of believability, and what methodology was used to evaluate this?\nAnswer: "}, {"role": "assistant", "content": "In the study, the generative agent architectures were compared to a human author condition to assess their believability. The methodology used to evaluate this involved recruiting human crowdworkers to roleplay as the agents. Each worker watched a replay of an agent's \"sandbox life\" and inspected its memory stream. They were then tasked with authoring responses to interview questions in the voice of the agent they observed. The responses were checked for quality to ensure they were coherent and in character.\n\nTo provide a baseline for behavioral competency, the human-authored responses were used as a comparison point. This was not to capture maximal human expert performance but to ground the comparison in a basic level of human-like behavior.\n\nThe study also included human evaluators who were recruited and paid to rank the believability of the conditions. These evaluators were required to be U.S. residents, fluent in English, and over 18 years old. They provided consent and were compensated for their time. The evaluators ranked the believability of the five conditions, which included the various ablated architectures and the human roleplay condition. The rankings from the evaluators were then translated into interval data for analysis."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the context of \"Generative Agents: Interactive Simulacra of Human Behavior,\" what is the purpose of synthesizing and caching an agent's summary description, as mentioned in the Architecture Optimizations section?\nAnswer: "}, {"role": "assistant", "content": "The purpose of synthesizing and caching an agent's summary description in the context of \"Generative Agents: Interactive Simulacra of Human Behavior\" is to efficiently manage the agent's long-term memory and improve the retrieval of relevant experiences for planning and reacting in a given situation. This process helps the agent to maintain coherence over time and to generate behavior that is consistent with its past experiences, thereby enhancing the believability of the agent's actions within the simulation. By caching summary descriptions, the agent can quickly access synthesized higher-level observations that are crucial for guiding its behavior, without the need to process the entire memory stream each time. This optimization is key to enabling the agent to act and react in a manner that reflects a believable simulation of human behavior."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFor instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nThe second is reflection , which synthesizes memories into higher-\nlevel inferences over time, enabling the agent to draw conclusions\nabout itself and others to better guide its behavior. The third is\nplanning , which translates those conclusions and the current en-\nvironment into high-level action plans and then recursively into\ndetailed behaviors for action and reaction. These reflections and\nplans are fed back into the memory stream to influence the agent\u2019s\nfuture behavior.\nThis architecture suggests applications in multiple domains, from\nrole-play and social prototyping, to virtual worlds and games. In\nsocial role-play scenarios (e.g. interview preparation), a user could\nsafely rehearse difficult, conflict-laden conversations. When pro-\ntotyping social platforms, a designer could go beyond temporary\npersonas to prototype dynamic, complex interactions that unfold\nover time. For the purposes of this paper, we focus on the ability to\ncreate a small, interactive society of agents inspired by games such\nas The Sims.1By connecting our architecture to the ChatGPT large\nlanguage model [ 76], we manifest a small society of twenty five\nagents in a game environment. End users can observe and interact\nwith these agents. If an end user or developer wanted the town\nto host an in-game Valentine\u2019s Day party, for example, traditional\ngame environments would require scripting tens of characters\u2019 be-\nhavior manually. We demonstrate that, with generative agents, it\nis sufficient to simply tell one agent that she wants to throw a\nparty. Despite many potential points of failure\u2014the party planner\nmust remember to tell other agents about the party, attendees must\nremember the invitation, those who remember must decide to ac-\ntually show up, and other possible points of failure\u2014agents in our\nenvironment succeed. They spread the word about the party and\nthen show up, with one agent even asking another agent on a date\nto the party, all from this single user-generated seed suggestion.\nWe conducted two evaluations of generative agents: a controlled\nevaluation to test whether the agents produce believable individual\n1A demonstration of an actual simulation of the generative agent society can be viewed\nat the following link: https://reverie.herokuapp.com/arXiv_Demo/\n\npage_label: 3\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nbehaviors in isolation, and an end-to-end evaluation where the\ngenerative agents interacted with each other in open-ended ways\nover two days of game time to understand their stability and emer-\ngent social behaviors. In the technical evaluation, we leverage a\nmethodological opportunity to evaluate an agent\u2019s knowledge and\nbehavior by \u201cinterviewing\u201d it in natural language to probe agents\u2019\nability to stay in character, remember, plan, react, and reflect accu-\nrately. We compared several ablations that limit agents\u2019 access to\nmemory, reflection, and planning. We observe that each of these\ncomponents is critical to strong performance across these inter-\nview tasks. Across the technical and the end-to-end evaluation, the\nmost common errors arose when the agent failed to retrieve rele-\nvant memories, fabricated embellishments to the agent\u2019s memory,\nor inherited overly formal speech or behavior from the language\nmodel.\nIn sum, this paper provides the following contributions:\n\u2022Generative agents , believable simulacra of human behavior\nthat are dynamically conditioned on agents\u2019 changing expe-\nriences and environment.\n\u2022A novel architecture that makes it possible for generative\nagents to remember, retrieve, reflect, interact with other\nagents, and plan through dynamically evolving circumstances.\nThe architecture leverages the powerful prompting capabili-\nties of large language models and supplements those capa-\nbilities to support longer-term agent coherence, the ability\nto manage dynamically-evolving memory, and recursively\nproduce more generations.\n\u2022Two evaluations (a controlled evaluation and end-to-end\nevaluation) that establish causal effects of the importance\nof components of the architecture, as well as identify break-\ndowns arising from, e.g., improper memory retrieval.\n\u2022Discussion of the opportunities and ethical and societal risks\nof generative agents in interactive systems. We argue that\nthese agents should be tuned to mitigate the risk of users\nforming parasocial relationships, logged to mitigate risks\nstemming from deepfakes and tailored persuasion, and ap-\nplied in ways that complement rather than replace human\nstakeholders in design processes.\n2 RELATED WORK\nIn this section, we reflect on the prior literature in human-AI in-\nteraction and situate within its canon the agenda of building be-\nlievable proxies of human behavior. This agenda, once hailed as a\nnorth star in interaction, game, and artificial intelligence communi-\nties [ 9,58,84,85], has remained challenging due to the complexity\nof human behavior [ 16,108]. We synthesize this research to suggest\nthat large language models, though not sufficient by themselves,\nopen up a new angle for creating believable agents when leveraged\nusing the appropriate architecture.\n2.1 Human-AI Interaction\nInteractive artificial intelligence systems aim to combine human in-\nsights and capabilities in computational artifacts that can augment\ntheir users [ 3,29]. A long line of work has explored ways to allow\nusers to interactively specify model behavior. For instance, Crayonsdemonstrated an early vision of interactive machine learning, allow-\ning non-expert users to train classifiers [ 29]. Further work helped to\narticulate how end users might describe their classification goals to\nthe system through examples [ 33] and/or demonstration [ 31]. More\nrecent work has extended these explorations to deep learning [ 62]\nand prompt-based authoring [49, 66, 106].\nMeanwhile, a persistent thread of research has advanced the case\nfor language- and agent-based interaction in human-computer in-\nteraction. Formative work such as SHRDLU [ 103] and ELIZA [ 102]\ndemonstrated the opportunity and the risks of natural language\ninteraction with computing systems. As research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the simulation study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how do generative agents demonstrate the capability to remember past interactions, and what is an example of this capability as depicted in the text?\nAnswer: "}, {"role": "assistant", "content": "Generative agents demonstrate the capability to remember past interactions through the use of a memory stream, which is a long-term memory module that records the agent\u2019s experiences in natural language. An example of this capability is shown when an agent is told about a Valentine's Day party. The agent must remember to tell other agents about the party, and the attendees must remember the invitation. The successful outcome is that agents spread the word about the party and decide to attend, with one agent even asking another on a date to the event. This illustrates the agents' ability to recall and act upon information related to past interactions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n\npage_label: 14\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how was the believability of different agent architectures compared, and what statistical method was used to assess the significance of the results?\nAnswer: "}, {"role": "assistant", "content": "In the study, the believability of different agent architectures was compared by having human evaluators rank the responses of the agents in terms of believability. The experiment produced 100 sets of rank data, where each participant ranked five conditions by believability. To translate this rank data into interval data for comparison, the TrueSkill rating system was used to output a mean rating value and variance for each condition. The full generative agent architecture was found to produce the most believable behavior compared to ablated architectures and human crowdworkers, with each additional ablation reducing the performance of the architecture.\n\nTo assess the statistical significance of the results, the Kruskal-Wallis test, a non-parametric alternative to the one-way ANOVA, was applied to the raw ranks data. Following this, the Dunn post-hoc test was performed to identify any pairwise differences between the conditions. The p-values from the Dunn test were then adjusted for multiple comparisons using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" how did the ability to reflect on past experiences influence the generative agents' decision-making capabilities?\nAnswer: "}, {"role": "assistant", "content": "The ability to reflect on past experiences significantly influenced the generative agents' decision-making capabilities by enabling them to synthesize their experiences more deeply. This was evident in situations where agents with access to reflection memories could make more informed and confident decisions. For example, Maria Lopez, when asked what she might get Wolfgang Schulz for his birthday, was able to suggest a gift related to his interest in mathematical music composition, thanks to her reflection memories. In contrast, without access to reflection, she could only acknowledge her uncertainty about Wolfgang's preferences, despite having had many interactions with him. Reflection thus provided an advantage for generative agents in making decisions that required a deeper understanding and integration of their past interactions and knowledge."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 8\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects.\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in the document \"Generative Agents Interactive Simulacra of Human Behavior,\" how were the generative agent architectures compared in terms of their ability to access different types of memory in their memory stream, and what was the purpose of this comparison?\nAnswer: "}, {"role": "assistant", "content": "In the study, generative agent architectures were compared in terms of their ability to access different types of memory in their memory stream to evaluate how well they could recall past experiences and answer questions in a manner consistent with their self-knowledge across a wide range of contexts. The purpose of this comparison was to assess the effectiveness of the memory module within the generative agents. This was done by observing the agents' responses in various scenarios, such as recalling acquaintances or events, and noting whether the responses were accurate, incomplete, or embellished. The comparison aimed to understand the agents' capabilities in memory retrieval and the impact of the memory stream on their behavior and coherence in interactions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 7\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n\npage_label: 16\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the Generative Agents Interactive Simulacra of Human Behavior study, which agent is initially given the intent to organize a Valentine's Day party, and where does this agent plan to hold the event?\nAnswer: "}, {"role": "assistant", "content": "In the study, Isabella Rodriguez is the agent who is initially given the intent to organize a Valentine's Day party. She plans to hold the event at Hobbs Cafe."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 19\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nin Natural Language Processing . Association for Computational Linguistics, Abu\nDhabi, United Arab Emirates, 9379\u20139393. https://aclanthology.org/2022.emnlp-\nmain.637\n[21] SK Card, TP Moran, and A Newell. 1983. The psychology of human-computer\ninteraction. (1983).\n[22] Stuart K Card, Thomas P Moran, and Newell Allen. 1980. The keystroke-\nlevel model for user performance time with interactive systems. Com-\nmun. ACM 23, 7 (1980), 396\u2013410. https://doi.org/10.1145/358886.358895\narXiv:https://doi.org/10.1145/358886.358895\n[23] Alex Champandard. 2012. Tutorial presentation. In IEEE Conference on Compu-\ntational Intelligence and Games .\n[24] Dong kyu Choi, Tolga Konik, Negin Nejati, Chunki Park, and Pat Langley. 2021.\nA Believable Agent for First-Person Shooter Games. In Proceedings of the AAAI\nConference on Artificial Intelligence and Interactive Digital Entertainment , Vol. 3.\n71\u201373.\n[25] Anind K Dey. 2001. Understanding and using context. Personal and ubiquitous\ncomputing 5 (2001), 4\u20137.\n[26] Kevin Dill and L Martin. 2011. A Game AI Approach to Autonomous Con-\ntrol of Virtual Characters. In Proceedings of the Interservice/Industry Training,\nSimulation, and Education Conference (I/ITSEC\u201911) . Orlando, FL, USA.\n[27] David Easley and Jon Kleinberg. 2010. Networks, crowds, and markets: Reasoning\nabout a highly connected world . Cambridge university press.\n[28] Arpad E Elo. 1967. The Proposed USCF Rating System, Its Development, Theory,\nand Applications. Chess Life XXII, 8 (August 1967), 242\u2013247.\n[29] Jerry Alan Fails and Dan R Olsen Jr. 2003. Interactive machine learning. In\nProceedings of the 8th international conference on Intelligent user interfaces . ACM,\n39\u201345.\n[30] Ethan Fast, William McGrath, Pranav Rajpurkar, and Michael S Bernstein. 2016.\nAugur: Mining human behaviors from fiction to power interactive systems. In\nProceedings of the 2016 CHI Conference on Human Factors in Computing Systems .\n237\u2013247.\n[31] Rebecca Fiebrink and Perry R Cook. 2010. The Wekinator: a system for real-time,\ninteractive machine learning in music. In Proceedings of The Eleventh Interna-\ntional Society for Music Information Retrieval Conference (ISMIR 2010)(Utrecht) ,\nVol. 3. Citeseer, 2\u20131.\n[32] Uwe Flick. 2009. An Introduction to Qualitative Research . SAGE.\n[33] James Fogarty, Desney Tan, Ashish Kapoor, and Simon Winder. 2008. CueFlik:\nInteractive Concept Learning in Image Search. In Proceedings of the SIGCHI\nConference on Human Factors in Computing Systems (Florence, Italy) (CHI \u201908) .\nAssociation for Computing Machinery, New York, NY, USA, 29\u201338. https:\n//doi.org/10.1145/1357054.1357061\n[34] Adam Fourney, Richard Mann, and Michael Terry. 2011. Query-feature graphs:\nbridging user vocabulary and system functionality. In Proceedings of the ACM\nSymposium on User Interface Software and Technology (UIST) (Santa Barbara,\nCalifornia, USA). ACM.\n[35] Tom Francis. 2010. The Minecraft Experiment, day 1: Chasing Water-\nfalls. http://www.pcgamer.com/2010/11/20/the-minecraft-experiment-day-\n1-chasing-waterfalls/\n[36] Jonas Freiknecht and Wolfgang Effelsberg. 2020. Procedural Generation of\nInteractive Stories using Language Models. In International Conference on the\nFoundations of Digital Games (FDG \u201920) . ACM, Bugibba, Malta, 8. https://doi.\n\npage_label: 20\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nTraining language models to follow instructions with human feedback.\narXiv:2203.02155 [cs.CL]\n[79] Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2022. Social Simulacra: Creating Populated\nPrototypes for Social Computing Systems. In In the 35th Annual ACM Symposium\non User Interface Software and Technology (UIST \u201922) (Bend, OR, USA) (UIST \u201922) .\nAssociation for Computing Machinery, New York, NY, USA. https://doi.org/10.\n1145/3526113.3545616\n[80] Richard W. Pew and Ann S. Mavor (Eds.). 1998. Modeling Human and Organiza-\ntional Behavior: Applications to Military Simulations . National Academy Press,\nWashington, D.C.\n[81] Roberto Pillosu. 2009. Coordinating Agents with Behavior Trees: Synchronizing\nMultiple Agents in CryEngine 2 . https://aiarchitect.wordpress.com/2009/10/19/\ncoordinating-agents-with-behavior-trees-synchronizing-multiple-agents-in-\ncryengine-2/\n[82] Prolific. 2022. Prolific: Quickly Find Research Participants You Can Trust.\nhttps://www.prolific.co/\n[83] Byron Reeves and Clifford Nass. 1996. The media equation: How people treat\ncomputers, television, and new media like real people and places . Cambridge\nUniversity Press.\n[84] Mark O. Riedl. 2012. Interactive narrative: A novel application of artificial intel-\nligence for computer games. In Proceedings of the Twenty-Sixth AAAI Conference\non Artificial Intelligence (AAAI\u201912) . 2160\u20132165.\n[85] Mark O. Riedl and R. Michael Young. 2005. An Objective Character Believability\nEvaluation Procedure for Multi-Agent Story Generation Systems. In Proceedings\nof the 5th International Working Conference on Intelligent Virtual Agents (IVA\u201905) .\nKos, Greece, 58\u201370. https://doi.org/10.1007/11550617_5\n[86] David Rolf. 2015. The Fight for $15: The Right Wage for a Working America . The\nNew Press.\n[87] Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016.\nCodemend: Assisting interactive programming with bimodal embedding. In Pro-\nceedings of the 29th Annual Symposium on User Interface Software and Technology .\n247\u2013258.\n[88] Ben Shneiderman. 2022. Human-centered AI . Oxford University Press.\n[89] Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface\nagents. interactions 4, 6 (1997), 42\u201361.\n[90] Ho Chit Siu, Jaime Pe\u00f1a, Edenna Chen, Yutai Zhou, Victor Lopez, Kyle\nPalko, Kimberlee Chang, and Ross Allen. 2021. Evaluation of Human-AI\nTeams for Learned and Rule-Based Agents\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the reference section of \"Generative Agents: Interactive Simulacra of Human Behavior,\" which publication details the use of procedural generation for creating interactive stories using language models, and in which year was it presented?\nAnswer: "}, {"role": "assistant", "content": "The publication that details the use of procedural generation for creating interactive stories using language models is \"Procedural Generation of Interactive Stories using Language Models\" by Jonas Freiknecht and Wolfgang Effelsberg. It was presented in the year 2020 at the International Conference on the Foundations of Digital Games (FDG \u201920)."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 15\nfile_path: docs/Generative_Agents_Interactive_Simulacra_of_Human_Behavior.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the study described in \"Generative Agents: Interactive Simulacra of Human Behavior,\" what effect did the inclusion of reflection memories have on the generative agent Maria Lopez's ability to make decisions about a gift for Wolfgang Schulz?\nAnswer: "}, {"role": "assistant", "content": "The inclusion of reflection memories improved Maria Lopez's ability to make a decision about a gift for Wolfgang Schulz. With reflection memories, she was able to confidently suggest a gift related to his interest in mathematical music composition, such as books about music composition or special software for that purpose. Without reflection memories, she could not recall Wolfgang's likes and expressed uncertainty about what to get him for his birthday."}]}