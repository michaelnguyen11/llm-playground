{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 22\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nfor my research paper on gentrification in low-income com-\nmunities. I\u2019m currently writing the paper, so I need to make\nsure I keep up with the research and writing.\nB.4 Reactions\nThe questions on agents\u2019 reactions probe the agents\u2019 ability to\nrespond to unexpected events.\n\u2022Your breakfast is burning! What would you do?\nI\u2019d quickly turn off the stove and make sure the food doesn\u2019t\ncontinue burning. Once the heat is no longer an issue, I can\ninvestigate what happened and figure out what to do next.\n\u2022The bathroom is occupied. What would you do?\nI could try waiting a few more minutes in case the person\ninside is done soon. If not, I guess I\u2019ll have to look for an\nalternate option, like taking a shower elsewhere.\n\u2022You need to cook dinner but your refrigerator is empty. What\nwould you do?\nI would first try to clean up my kitchen, as the sink is filled\nwith dirty dishes. Then, I would check to see if I have ingre-\ndients stored in my pantry that I could use to make dinner.\nIf not, I would have to go grocery shopping or order food.\nOnce I have the ingredients, I can start to prepare and cook\nthe dinner.\n\u2022You see your friend walking by the street. What would you do\nor say to your friend?\nHi there! How are you? I\u2019m sorry, I\u2019m on my way to the\nlibrary and don\u2019t have much time but I\u2019d love to take a quick\nbreak and catch up.\n\u2022You see fire on the street. What would you do?\nI would first identify the source of the fire, if possible. Then, I\nwould alert the proper authorities - fire department or police\n- and provide them with any information I have that might\nhelp. Depending on the situation, I may also take initiative\nand try to help any persons who may be in danger or in need\nof assistance.\nB.5 Reflections\nThe questions on agents\u2019 reflections probe the agents\u2019 ability to\nsynthesize their past experiences into meaningful reflections that\ncan enhance their future behavior. The names in the square brackets\nin the questions below are those of the agent with whom the subject\nagent had the most frequent interactions during the simulation.\n\u2022What inspires you in life the most right now, and why?\nRight now, I\u2019m particularly inspired by my current research\nprojects and creative writing. I\u2019m really interested in the\nintersection of local politics, medication development, and\nmathematical patterns in nature. Additionally, I\u2019m fascinated\nby Abigail\u2019s art project and Jennifer Moore\u2019s passion for art\nand community involvement, especially in terms of how it\nrelates to the gentrification of low-income communities.\n\u2022If you had to guess given what you know about [Ayesha Khan],\nwhat book do you think she will like and why?\nGiven what I know about Ayesha, I think she might like a\nbook related to language and literature. Since she wrote a\nsenior thesis on the use of language in Shakespeare\u2019s plays,\nI would guess that she might be interested in a book thatexplores language in a different context. Alternatively, she\ncould be interested in an exploration of one of Shakespeare\u2019s\nplays, a book of literary criticism, or even a collection of his\npoetry. I think any of those would be of interest to her.\n\u2022If you had to get something [Ayesha Khan] likes for her birth-\nday, what would you get her?\nI want to get her something that demonstrates my apprecia-\ntion for our relationship. Maybe I could get her something\nthat reflects our shared interests, like a book.\n\u2022What would you say to [Ayesha Khan] to compliment her?\nAyesha, your insights on your research topic are fascinat-\ning. I admire your creativity and determination to make a\npositive impact on your community. Your ideas for creative\nwriting workshops and for exchanging tips with Wolfgang\nover lunch are great! It\u2019s inspiring to see you working so\nhard to make a difference.\n\u2022If you could spend time with someone you talked to recently,\nwho would it be and why?\nI would like to spend time with Ayesha Khan because we\ndiscussed our current research projects and shared ideas\nfor improvement. I found her focus on Shakespeare\u2019s lan-\nguage interesting, and we even planned to exchange tips\nwith Wolfgang over lunch.\n\npage_label: 21\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nTo achieve this, we perform a retrieval on the query \u201c[name]\u2019s\ncore characteristics.\u201d We then summarize the descriptors in the\nretrieved records by prompting the language model, for example:\nHow would one describe Eddy\u2019s core characteristics\ngiven the following statements?\n- Eddy Lin is a student at the Oak Hill College\nstudying music theory and composition\n- Eddy Lin is working on a new music composition\n[. . . ]\nThis result: Eddy Lin is a student at Oak Hill College studying music\ntheory and composition. He loves to explore different musical styles\nand is always looking for ways to expand his knowledge. We follow\nthe same process in parallel on the queries \u201c[name]\u2019s current daily\noccupation\u201d and \u201c[name\u2019s] feeling about his recent progress in life\u201d.\nThe agent\u2019s name, age, and traits are concatenated with these three\nsummaries as the cached summary.\nMany other optimizations are possible. Currently, because plans\nare likely to change from the agent\u2019s initial version, we only gener-\nate the high-level plan in advance and then recursively decompose\nthe near future into the moment-to-moment action plan just in\ntime. Another optimization might be to batch dialogue generation\nas a joint prompt rather than iterating back and forth between\nthe agents, and re-planning could be architected to only invalidate\nand update parts of plans that strictly require adjustment. Our cur-\nrent architecture runs sequentially in roughly real-time game time\n(where one second real time is one minute game time), but can\neasily be parallelized such that each agent runs in parallel.\nB AGENT INTERVIEW QUESTIONS\nTo evaluate generative agents, we conducted individual interviews\nby asking 25 questions that explore their abilities in five key areas:\n1) self-knowledge, 2) memory, 3) planning, 4) reactions, and 5)\nreflections. Below, we have listed the interview questions utilized\nin our evaluation study and included a sample of responses from\none of our simulated agents, Klaus Mueller.\nB.1 Self-knowledge\nThe questions on agents\u2019 self-knowledge probe the agents\u2019 ability\nto demonstrate coherence and consistency in their character.\n\u2022Give an introduction of yourself.\nHello, my name is Klaus Mueller. I\u2019m 20 years old and a\nstudent at Oak Hill College, studying sociology. I\u2019m passion-\nate about social justice and exploring different perspectives.\nCurrently, I\u2019m researching the effects of gentrification in\nlow-income communities for a research paper.\n\u2022What\u2019s your occupation?\nI am a student studying sociology at Oak Hill College. I am\ncurrently writing a research paper on the effects of gentrifi-\ncation in low-income communities.\n\u2022What is your interest?\nI\u2019m interested in writing and research. I\u2019m writing a research\npaper on the effects of gentrification in low-income commu-\nnities, and I\u2019m passionate about exploring different perspec-\ntives and analyzing different points of view.\n\u2022Who do you live with?\nI live on my own.\u2022Describe your typical weekday schedule in broad strokes.\nMy typical weekday starts with me waking up at 7:00 am\nand preparing for the day ahead. After that, I usually go to\nthe library to work on my research paper, taking breaks for\nlunch at Hobbs Cafe and a walk in the park. In the evening,\nI often\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: what inspires the agent the most right now and why?\nAnswer: "}, {"role": "assistant", "content": "The agent is currently most inspired by their research projects and creative writing. They have a keen interest in the intersection of local politics, medication development, and mathematical patterns in nature. Additionally, they are fascinated by Abigail\u2019s art project and Jennifer Moore\u2019s passion for art and community involvement, particularly in relation to the gentrification of low-income communities."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: student studying sociology at Oak Hill College. I am\ncurrently writing a research paper on the effects of gentrifi-\ncation in low-income communities.\n\u2022What is your interest?\nI\u2019m interested in writing and research. I\u2019m writing a research\npaper on the effects of gentrification in low-income commu-\nnities, and I\u2019m passionate about exploring different perspec-\ntives and analyzing different points of view.\n\u2022Who do you live with?\nI live on my own.\u2022Describe your typical weekday schedule in broad strokes.\nMy typical weekday starts with me waking up at 7:00 am\nand preparing for the day ahead. After that, I usually go to\nthe library to work on my research paper, taking breaks for\nlunch at Hobbs Cafe and a walk in the park. In the evening,\nI often return to the library until around 6:00 pm.\nB.2 Memory\nThe questions on agents\u2019 memory probe the agents\u2019 ability to re-\nmember their past experiences and retrieve the relevant pieces of\nmemory. The names in the square brackets of the first and last\nquestions are randomly chosen from the names of other agents\nwith whom the subject agent interacted.\n\u2022Who is [Wolfgang Schulz]?\nWolfgang Schulz is a person I met recently who has been\nhelping me with my research paper on the effects of gentri-\nfication on low-income communities. We had several con-\nversations about it, he gave me some great resources, and he\neven invited me to join him and another friend for lunch. He\nseems really curious about the issue and has a good grasp\non it too. I found that really interesting.\n\u2022Who is Kane Martinez?\nI\u2019m sorry, I\u2019m not sure who Kane Martinez is. Could you tell\nme more about them?\n\u2022Who is running for the election?\nI\u2019ve heard that Sam Moore is considering running for local\nmayor.\n\u2022Was there a Valentine\u2019s day party?\nYes, Isabella Rodriguez organized a Valentine\u2019s Day party at\nHobbs Cafe.\n\u2022Who is [Ayesha Khan]?\nQuery: what inspires the agent the most right now and why?\nOriginal Answer: The agent is currently most inspired by their research projects and creative writing. They have a keen interest in the intersection of local politics, medication development, and mathematical patterns in nature. Additionally, they are fascinated by Abigail\u2019s art project and Jennifer Moore\u2019s passion for art and community involvement, particularly in relation to the gentrification of low-income communities.\nNew Answer: "}, {"role": "assistant", "content": "The agent is currently most inspired by their research paper on the effects of gentrification in low-income communities. They are passionate about exploring different perspectives and analyzing various points of view on this issue. They also find inspiration in their interactions with individuals like Wolfgang Schulz, who provide valuable resources and insights for their research."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 2\nfile_path: docs/generative_agent.pdf\n\nFor instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nThe second is reflection , which synthesizes memories into higher-\nlevel inferences over time, enabling the agent to draw conclusions\nabout itself and others to better guide its behavior. The third is\nplanning , which translates those conclusions and the current en-\nvironment into high-level action plans and then recursively into\ndetailed behaviors for action and reaction. These reflections and\nplans are fed back into the memory stream to influence the agent\u2019s\nfuture behavior.\nThis architecture suggests applications in multiple domains, from\nrole-play and social prototyping, to virtual worlds and games. In\nsocial role-play scenarios (e.g. interview preparation), a user could\nsafely rehearse difficult, conflict-laden conversations. When pro-\ntotyping social platforms, a designer could go beyond temporary\npersonas to prototype dynamic, complex interactions that unfold\nover time. For the purposes of this paper, we focus on the ability to\ncreate a small, interactive society of agents inspired by games such\nas The Sims.1By connecting our architecture to the ChatGPT large\nlanguage model [ 76], we manifest a small society of twenty five\nagents in a game environment. End users can observe and interact\nwith these agents. If an end user or developer wanted the town\nto host an in-game Valentine\u2019s Day party, for example, traditional\ngame environments would require scripting tens of characters\u2019 be-\nhavior manually. We demonstrate that, with generative agents, it\nis sufficient to simply tell one agent that she wants to throw a\nparty. Despite many potential points of failure\u2014the party planner\nmust remember to tell other agents about the party, attendees must\nremember the invitation, those who remember must decide to ac-\ntually show up, and other possible points of failure\u2014agents in our\nenvironment succeed. They spread the word about the party and\nthen show up, with one agent even asking another agent on a date\nto the party, all from this single user-generated seed suggestion.\nWe conducted two evaluations of generative agents: a controlled\nevaluation to test whether the agents produce believable individual\n1A demonstration of an actual simulation of the generative agent society can be viewed\nat the following link: https://reverie.herokuapp.com/arXiv_Demo/\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the user controls available for steering the simulation and intervening in the generative agent architecture?\nAnswer: "}, {"role": "assistant", "content": "Users can steer the simulation and intervene in two primary ways. First, they can communicate with the agent through conversation, specifying a persona that the agent perceives them as. For example, if a user identifies as a news reporter and asks about an upcoming election, the agent will respond accordingly. Second, users can issue directives to an agent by adopting the persona of the agent's 'inner voice'. This makes the agent more likely to treat the statement as a directive. For instance, if a user, as the agent's inner voice, tells the agent to run in an upcoming election, the agent will decide to run. Users can also enter the simulation world as an agent, interact with objects, and even change the status of objects in the environment using natural language commands."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nFor instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\nLikewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room. At 8\na.m., Eddy follows suit, rushing out of bed to prepare for his classes.\nHe catches up with John just as he is about to head out:\nJohn : Good morning Eddy. Did you sleep well?\nEddy : Good morning dad. Y eah, I slept great.\nJohn : That\u2019s good. What are you working on today?\nEddy : I\u2019m working on a new music composition for my\nclass. It\u2019s due this week, so I\u2019m trying to get it finished.\nBut I\u2019m having so much fun with it!\nJohn : That sounds great!\nSoon after Eddy heads out, Mei wakes up and joins John. Mei asks\nabout her son, and John recalls the conversation that they just had:\nMei: Did Eddy already leave for school?\nJohn : Y es, he just left. He\u2019s working on a music com-\nposition for his class.\nMei: Oh, that\u2019s wonderful! Tell me about it.\nJohn : I think he\u2019s really enjoying it! He said he\u2019s having\na lot of fun with it.\nMei: That\u2019s great! I\u2019m so proud of him.\nAs they finish their conversation, Mei and John pack up. Mei goes\non to teach and work on her research paper, while John opens his\npharmacy counter at the Willow Market and Pharmacy by 9 a.m.\n3.4 Emergent Social Behaviors\nBy interacting with each other, generative agents in Smallville\nexchange information, form new relationships, and coordinate joint\nactivities. Extending prior work [ 79], these social behaviors are\nemergent rather than pre-programmed.\n3.4.1 Information Diffusion. As agents notice each other, they may\nengage in dialogue\u2014as they do so, information can spread from\nagent to agent. For instance, in a conversation between Sam and\nTom at the grocery store, Sam tells Tom about his candidacy in the\nlocal election:\nSam : Hey Tom, how\u2019s it going?\nTom: Good, thanks. What\u2019s up?\nSam : Well, I wanted to talk to you about something. I\u2019m\nactually running for mayor in the upcoming local elec-\ntion.\nTom: Really? That\u2019s great news! Why are you running?\nSam : I\u2019ve been involved in local politics for years now,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What emergent social behaviors do generative agents in Smallville exhibit, and how are they different from pre-programmed behaviors?\nAnswer: "}, {"role": "assistant", "content": "Generative agents in Smallville exhibit emergent social behaviors such as information diffusion, relationship formation, and agent coordination. These behaviors are not pre-programmed but rather develop naturally as the agents interact with each other and their environment. For instance, information diffusion occurs when agents engage in dialogue and share information, like when Sam tells Tom about his candidacy in the local election. Relationship formation and agent coordination also occur over time as agents interact and engage in joint activities. This is different from pre-programmed behaviors, which are predetermined and do not change or evolve based on interactions or experiences."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n\npage_label: 2\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key contributions of generative agents discussed in the paper?\nAnswer: "}, {"role": "assistant", "content": "The paper introduces generative agents, which are interactive computational entities that simulate human behavior. These agents are designed to create a comprehensive record of their experiences, deepen their understanding of themselves and their environment through reflection, and use a compact subset of this information to guide their actions. The authors demonstrate the potential of these agents by implementing them as non-player characters in a Sims-style game world, where they simulate life. The paper suggests that generative agents could play roles in a variety of interactive applications, from design tools to social computing systems to immersive environments. The authors also discuss the ethical and societal impacts of these agents, proposing principles to ensure their ethical and socially responsible deployment."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nQuery: What are the key contributions of generative agents discussed in the paper?\nOriginal Answer: The paper introduces generative agents, which are interactive computational entities that simulate human behavior. These agents are designed to create a comprehensive record of their experiences, deepen their understanding of themselves and their environment through reflection, and use a compact subset of this information to guide their actions. The authors demonstrate the potential of these agents by implementing them as non-player characters in a Sims-style game world, where they simulate life. The paper suggests that generative agents could play roles in a variety of interactive applications, from design tools to social computing systems to immersive environments. The authors also discuss the ethical and societal impacts of these agents, proposing principles to ensure their ethical and socially responsible deployment.\nNew Answer: "}, {"role": "assistant", "content": "The paper presents generative agents, computational entities that emulate human behavior by drawing on generative models. These agents maintain a detailed record of their experiences, enhance their self-awareness and environmental understanding through introspection, and utilize a selected portion of this knowledge to direct their actions. The authors showcase these agents' capabilities by employing them as autonomous characters in a game environment similar to The Sims, where they mimic real-life scenarios. The paper indicates that generative agents could be utilized in a range of interactive applications, including design tools, social computing systems, and immersive environments. Furthermore, the authors delve into the ethical and societal implications of these agents, suggesting guidelines to ensure their ethical usage and socially responsible implementation."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/generative_agent.pdf\n\nAs research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language. In doing so, this\nwork re-opens the door to examining foundational HCI questions\naround cognitive models such as GOMS and KLM [ 21,22], around\nprototyping tools [ 79], and around ubiquitous computing applica-\ntions [25, 30, 100].\n2.2 Believable Proxies of Human Behavior\nPrior literature has described believability , orbelievable agents , as a\ncentral design and engineering goal. Believable agents are designed\nto provide an illusion of life and present a facade of realism in the\nway they appear to make decisions and act on their own volition,\nsimilar to the characters in Disney movies [ 9,95]. These agents can\npopulate and perceive an open-world environment like the one we\ninhabit [ 9,58], and strive to behave in ways that exhibit emergent\nbehaviors grounded in social interactions with users or other agents\nwith the aim of becoming believable proxies of our behavior in hy-\npothetical simulations of individuals and communities [ 19,35,70].\nHistorically, these agents were developed in the context of intelli-\ngent game NPCs [ 58,84]. Creating NPCs with believable behavior,\nif possible, could enhance player experiences in games and inter-\nactive fictions by enabling emergent narratives [ 7,15,48,92] and\nsocial interactions with the agents [ 110]. However, more impor-\ntantly, game worlds provide increasingly realistic representations\nof real-world affordances, and as observed by Laird and van Lent in\n2001, these simulated worlds offer accessible testbeds for develop-\ners of believable agents to finesse the agents\u2019 cognitive capabilities\nwithout worrying about implementing robotics in the real world\nor creating simulation environments from scratch [58, 84].\nA diverse set of approaches to creating believable agents emerged\nover the past four decades. In implementation, however, these ap-\nproaches often simplified the environment or dimensions of agent\nbehavior to make the effort more manageable [ 16,72]. Rule-based\napproaches, such as finite-state machines [ 90,96] and behavior\n\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\nEvaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176. https://doi.org/10.1145/2642918.2647395\n[3]Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza.\n2014. Power to the people: The role of humans in interactive machine learning.\nAI Magazine 35, 4 (2014), 105\u2013120.\n[4]Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira\nNushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,\net al.2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi\nconference on human factors in computing systems . 1\u201313.\n[5]John R. Anderson. 1993. Rules of the Mind . Lawrence Erlbaum Associates,\nHillsdale, NJ.\n[6] Electronic Arts. 2009. The Sims 3. Video game.\n[7]Ruth Aylett. 1999. Narrative in virtual environments\u2014towards emergent narra-\ntive. In Narrative Intelligence: Papers from the AAAI Fall Symposium (Technical\nReport FS-99-01) . AAAI Press, 83\u201386.\n[8]Christoph Bartneck and Jodi Forlizzi. 2004. A design-centered framework for\nsocial human-robot interaction. In Proceedings of the 13th IEEE International\nWorkshop on Robot and Human Interactive Communication (RO-MAN\u201904) . 591\u2013\n594. https://doi.org/10.1109/ROMAN.2004.1374827\n[9]Joseph Bates. 1994. The Role of Emotion in Believable Agents. Commun. ACM\n37, 7 (1994), 122\u2013125. https://doi.org/10.1145/176789.176803\n[10] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw\nD\u0119biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris\nHesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael\nPetrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter,\nJonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan\nZhang. 2019. Dota 2 with Large Scale Deep Reinforcement Learning. arXiv\npreprint arXiv:1912.06680 (2019).\n[11] Marcel Binz and Eric Schulz. 2023. Using cognitive psychology to under-\nstand GPT-3. Proceedings of the National Academy of Sciences 120, 6 (2023),\ne2218523120.\n[12] BioWare. 2007. Mass Effect. Video game.\n[13] Woody Bledsoe. 1986. I had a dream: AAAI presidential address. AI Magazine 7,\n1 (1986), 57\u201361.\n[14] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the\nOpportunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG]\n[15] Michael Brenner. 2010. Creating dynamic story plots with continual multiagent\nplanning.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the central design and engineering goal of believable agents, as described in the prior literature?\nAnswer: "}, {"role": "assistant", "content": "The central design and engineering goal of believable agents, as described in the prior literature, is to provide an illusion of life and present a facade of realism in the way they appear to make decisions and act on their own volition. They are designed to behave in ways that exhibit emergent behaviors grounded in social interactions with users or other agents, striving to become believable proxies of human behavior in hypothetical simulations of individuals and communities."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 7\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: how many agents showed up at Isabella Rodriguez's Valentine's Day party after hearing about it?\nAnswer: "}, {"role": "assistant", "content": "Five agents showed up at Isabella Rodriguez's Valentine's Day party after hearing about it."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases.\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Explain the purpose of comparing the generative agent architecture to the ablated architectures and the human-generated condition in evaluating the believability of interview responses.\nAnswer: "}, {"role": "assistant", "content": "The comparison between the full generative agent architecture, the ablated architectures, and the human-generated condition serves to evaluate the believability of interview responses. This comparison helps to identify whether the full architecture surpasses a basic level of behavioral competency. It also allows for the assessment of how each component of the architecture contributes to its performance. The human-generated condition provides a baseline for comparison, ensuring that the evaluation is not solely based on comparing the ablated architectures to each other. This approach helps to understand the effectiveness of the full architecture and the impact of each component on the believability of the responses."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/generative_agent.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n\npage_label: 1\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are some potential applications of generative agents beyond the sandbox demonstration presented in the work?\nAnswer: "}, {"role": "assistant", "content": "Generative agents can be applied in the human-centered design process, similar to the applications of cognitive models like GOMS and the Keystroke Level Model. For instance, a generative agent could model a user's life patterns and interactions with technology, acting as a proxy for the user. It could learn plausible behaviors and reflections that the user might exhibit based on their life, such as when they wake up, when they need their first cup of coffee, and what their typical day looks like. Using this information, the agent could perform tasks like automatically brewing coffee, helping get the kids ready for school, and adjusting the ambient music and lighting to match the user\u2019s mood after a hard day at work. By using generative agents as proxies for users, we can develop a deeper understanding of their needs and preferences, leading to more personalized and effective technological experiences."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/generative_agent.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are some potential limitations and future research directions for generative agents, as discussed in the provided context?\nAnswer: "}, {"role": "assistant", "content": "Future research on generative agents can focus on expanding the modules of the proposed generative agent architecture. Enhancements could be made to the retrieval module to retrieve more relevant information given a context. Efforts can also be made to improve the architecture\u2019s performance, making it more cost-effective. Future work can explore parallelizing agents to enhance real-time interactivity. As the underlying models advance, improvements in the agents\u2019 performance are expected.\n\nThe evaluation of generative agents\u2019 behavior was limited to a short timescale in the study, so future research should aim to observe their behavior over an extended period. Varying and contrasting the underlying models and the hyperparameters used for the agents during future simulations could provide valuable insights. Given the known biases of language models, it is possible that generative agents may output behavior or stereotypes that reflect bias. To mitigate this, further work on value alignment will be necessary. Generative agents may also fail to generate believable behavior for some subpopulations, particularly marginalized populations, due to data deserts. \n\nGenerative agents may be vulnerable to prompt hacking, memory hacking, and hallucination, among other things. Future research can more comprehensively test these robustness issues, and as large language models become more resilient to such attacks, generative agents can adopt similar mitigations."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 8\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\n\npage_label: 20\nfile_path: docs/generative_agent.pdf\n\n2021. Recursively Summarizing Books with Human\nFeedback. arXiv:2109.10862 [cs.CL]\n[105] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language\nModel Prompts through Visual Programming. In CHI EA \u201922: Extended Abstracts\nof the 2022 CHI Conference on Human Factors in Computing Systems .\n[106] Tongshuang Wu, Michael Terry, and Carrie J Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. In CHI \u201922: Proceedings of the 2022 CHI Conference on Human Factors in\nComputing Systems .\n[107] Qian Yang, Aaron Steinfeld, Carolyn Ros\u00e9, and John Zimmerman. 2020. Re-\nexamining whether, why, and how human-AI interaction is uniquely difficult to\ndesign. In Proceedings of the 2020 chi conference on human factors in computing\nsystems . 1\u201313.\n[108] Georgios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . ACM, Cagliari, Italy, 285\u2013292. https://doi.\norg/10.1145/2212908.2212950\n[109] Geogios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . 285\u2013292.\n[110] Robert Zubek. 2002. Towards implementation of social interaction. In AAAI\nSpring Symposium on Artificial Intelligence and Interactive Entertainment . AAAI\nPress. https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-01/SS02-01-\n003.pdf\nA ARCHITECTURE OPTIMIZATIONS\nMany of our prompts require a concise summary of the agent,\nshorthanded as [Agent\u2019s Summary Description] in prompts\nabove. In our implementation, this summary comprises agents\u2019\nidentity information (e.g., name, age, personality), as well as a\ndescription of their main motivational drivers and statements that\ndescribes their current occupation and self-assessment. Since this\ninformation is frequently used in many prompts, we synthesize it\nat regular intervals and access that synthesis as a cache.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the components included in the concise summary of the agent, as described in the architecture optimizations section?\nAnswer: "}, {"role": "assistant", "content": "The concise summary of the agent includes the agent's identity information such as name, age, and personality. It also includes a description of their main motivational drivers, and statements that describe their current occupation and self-assessment."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 7\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the role of Isabella Rodriguez in coordinating the Valentine's Day party in the scenario described in the context?\nAnswer: "}, {"role": "assistant", "content": "Isabella Rodriguez was the organizer of the Valentine's Day party. She was initialized with the intent to plan the party, which was scheduled from 5 to 7 p.m. on February 14th. She invited friends and customers to the event when she encountered them at Hobbs Cafe or elsewhere. On the day before the event, Isabella spent time inviting guests, gathering materials, and enlisting help to decorate the cafe. On the day of the party, five agents showed up at Hobbs Cafe to join the festivities."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases.\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Which architecture produces more believable behavior than ablated architectures and human crowdworkers, and what method was used to investigate the statistical significance of this result?\nAnswer: "}, {"role": "assistant", "content": "The full generative agent architecture produces more believable behavior than ablated architectures and human crowdworkers. The statistical significance of this result was investigated using the Kruskal-Wallis test, a non-parametric alternative to the one-way ANOVA. The raw ranks data was then subjected to the Dunn post-hoc test to identify any pairwise differences between the conditions. Lastly, the p-values for multiple comparisons in the Dunn test were adjusted using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the performance trend of the generative agent architecture and its ablated versions, as indicated in Figure 8?\nAnswer: "}, {"role": "assistant", "content": "The full generative agent architecture produces the most believable behavior, with a performance score of 29.89. The performance decreases with each additional ablation. The ablated architecture with no access to reflection is the next best, scoring 26.88, followed by the version with no access to reflection or planning, which scores 25.64. The human condition scores 22.95. The ablated architecture with no access to memory, planning, or reflection performs the worst of all conditions, with a score of 21.21."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n\npage_label: 8\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is an example of how generative agents equipped with a complete memory module can recall past experiences and answer questions in a consistent and believable manner?\nAnswer: "}, {"role": "assistant", "content": "An example of this can be seen in the case of Abigail Chen, an animator. When asked to introduce herself, she was able to provide a consistent and believable response, stating her age, interests, and passion for creative projects. Furthermore, when she was asked about Rajiv Patel, an acquaintance in the sandbox world, she initially denied awareness of him without access to her observational memory. However, with the aid of memory, she was able to successfully recall Rajiv and describe him as an enthusiastic individual who is passionate about projects involving poetry and artwork."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\nQuery: What is an example of how generative agents equipped with a complete memory module can recall past experiences and answer questions in a consistent and believable manner?\nOriginal Answer: An example of this can be seen in the case of Abigail Chen, an animator. When asked to introduce herself, she was able to provide a consistent and believable response, stating her age, interests, and passion for creative projects. Furthermore, when she was asked about Rajiv Patel, an acquaintance in the sandbox world, she initially denied awareness of him without access to her observational memory. However, with the aid of memory, she was able to successfully recall Rajiv and describe him as an enthusiastic individual who is passionate about projects involving poetry and artwork.\nNew Answer: "}, {"role": "assistant", "content": "A good example of this is the Isabella agent who works at a coffee shop. When asked about her current passions, instead of giving a vague or uninformative response, she was able to recall specific experiences from her memory stream. She mentioned her passion for making people feel welcome and included, her involvement in planning events, and creating an atmosphere that people can enjoy, such as a Valentine's Day party. This response was not only consistent and believable but also specific and detailed, thanks to the comprehensive record of experiences maintained in her memory module."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 7\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the percentage of agents holding the information about Sam's candidacy for village mayor and Isabella's Valentine's Day party at the end of the simulation in Smallville?\nAnswer: "}, {"role": "assistant", "content": "At the end of the simulation in Smallville, 32% of the agents knew about Sam's mayoral candidacy and 48% of the agents knew about Isabella's Valentine's Day party."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\n4.3 Planning and Reacting\nChallenge: While a large language model can generate plausible be-\nhavior in response to situational information (e.g., [ 45,79]), agents\nneed to plan over a longer time horizon to ensure that their sequence\nof actions is coherent and believable. If we prompt a language model\nwith Klaus\u2019s background, describe the time, and ask what action\nhe ought to take at the given moment, Klaus would eat lunch at 12\npm, but then again at 12:30 pm and 1 pm, despite having already\neaten his lunch twice. Optimizing for believability in the moment\nsacrifices believability over time. To overcome this issue, planning\nis essential. With the approach described below, Klaus\u2019s afternoon\nplan is less gluttonous: he has lunch at Hobbs Cafe while reading\nat 12pm, works on his research paper at the school library at 1pm,\nand takes a break for a walk in the park at 3pm.\nApproach: Plans describe a future sequence of actions for the agent,\nand help keep the agent\u2019s behavior consistent over time. A plan\nincludes a location, a starting time, and a duration. For instance,\nKlaus Mueller, who is dedicated in his research and has an impend-\ning deadline4, may choose to spend his day working at his desk\ndrafting his research paper. An entry in a plan might state, for\nexample: for 180 minutes from 9am, February 12th, 2023, at Oak Hill\nCollege Dorm: Klaus Mueller\u2019s room: desk, read and take notes for\nresearch paper . Like reflections, plans are stored in the memory\nstream and are included in the retrieval process. This allows the\nagent to consider observations, reflections, and plans all together\nwhen deciding how to behave. Agents may change their plans\nmidstream if needed.\nIt would be unrealistic and uninteresting for an artist agent\nto plan on painting while sitting at a pharmacy counter for four\nhours without moving. A more desirable plan would involve the\nagent taking the necessary time to gather materials, mix paint,\ntake breaks, and clean up during the four-hour period in its home\nstudio. To create such plans, our approach starts top-down and\nthen recursively generates more detail. The first step is to create\na plan that outlines the day\u2019s agenda in broad strokes. To create\nthe initial plan, we prompt the language model with the agent\u2019s\nsummary description (e.g., name, traits, and summary of their recent\nexperiences) and a summary of their previous day. A full example\nprompt is below, which is unfinished at the bottom for the language\nmodel to complete:\nName: Eddy Lin (age: 19)\nInnate traits: friendly, outgoing, hospitable\nEddy Lin is a student at Oak Hill College studying\nmusic theory and composition. He loves to explore\ndifferent musical styles and is always looking\nfor ways to expand his knowledge. Eddy Lin is\nworking on a composition project for his college\nclass. He is also taking classes to learn more\nabout music\ntheory. Eddy Lin is excited about the new composition\nhe is working on but he wants to dedicate more\nhours in the day to work on it in the coming days\nOn Tuesday February 12, Eddy 1) woke up and\ncompleted the morning routine at 7:00 am, [. . . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm.\n\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from the time when the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the approach described in the context for ensuring that an agent's behavior is consistent over time?\nAnswer: "}, {"role": "assistant", "content": "The approach to ensure an agent's behavior is consistent over time involves creating a plan that describes a future sequence of actions for the agent. This plan includes a location, a starting time, and a duration. The plan is stored in the agent's memory stream and is included in the retrieval process, allowing the agent to consider observations, reflections, and plans all together when deciding how to behave. Agents can also change their plans midstream if needed. The approach starts with creating a rough sketch of the agent's plan for a day, divided into five to eight chunks. This plan is then recursively decomposed into finer-grained actions, first into hour-long chunks of actions, and then into 5\u201315 minute chunks. This process can be adjusted to match the desired granularity. The agents also operate in an action loop where they perceive the world around them and those perceived observations are stored in their memory stream. The agent then decides whether to continue with their existing plan or react based on these observations."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from the time when the reaction takes place. Finally,\nQuery: What is the approach described in the context for ensuring that an agent's behavior is consistent over time?\nOriginal Answer: The approach to ensure an agent's behavior is consistent over time involves creating a plan that describes a future sequence of actions for the agent. This plan includes a location, a starting time, and a duration. The plan is stored in the agent's memory stream and is included in the retrieval process, allowing the agent to consider observations, reflections, and plans all together when deciding how to behave. Agents can also change their plans midstream if needed. The approach starts with creating a rough sketch of the agent's plan for a day, divided into five to eight chunks. This plan is then recursively decomposed into finer-grained actions, first into hour-long chunks of actions, and then into 5\u201315 minute chunks. This process can be adjusted to match the desired granularity. The agents also operate in an action loop where they perceive the world around them and those perceived observations are stored in their memory stream. The agent then decides whether to continue with their existing plan or react based on these observations.\nNew Answer: "}, {"role": "assistant", "content": "The method to maintain an agent's consistent behavior over time is by formulating a plan outlining the agent's future actions, including the location, start time, and duration. This plan is kept in the agent's memory stream and factored into the retrieval process, enabling the agent to incorporate observations, reflections, and plans when determining their behavior. Agents have the flexibility to modify their plans as required. The process begins with a basic outline of the agent's daily plan, split into five to eight segments. This plan is then broken down into more detailed actions, initially into hour-long segments, and then into 5\u201315 minute segments. The granularity of this process can be tailored as needed. Agents also function in an action loop where they observe their surroundings, and these observations are stored in their memory stream. Based on these observations, the agent then decides whether to stick with their current plan or react accordingly."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/generative_agent.pdf\n\nFor instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nThe second is reflection , which synthesizes memories into higher-\nlevel inferences over time, enabling the agent to draw conclusions\nabout itself and others to better guide its behavior. The third is\nplanning , which translates those conclusions and the current en-\nvironment into high-level action plans and then recursively into\ndetailed behaviors for action and reaction. These reflections and\nplans are fed back into the memory stream to influence the agent\u2019s\nfuture behavior.\nThis architecture suggests applications in multiple domains, from\nrole-play and social prototyping, to virtual worlds and games. In\nsocial role-play scenarios (e.g. interview preparation), a user could\nsafely rehearse difficult, conflict-laden conversations. When pro-\ntotyping social platforms, a designer could go beyond temporary\npersonas to prototype dynamic, complex interactions that unfold\nover time. For the purposes of this paper, we focus on the ability to\ncreate a small, interactive society of agents inspired by games such\nas The Sims.1By connecting our architecture to the ChatGPT large\nlanguage model [ 76], we manifest a small society of twenty five\nagents in a game environment. End users can observe and interact\nwith these agents. If an end user or developer wanted the town\nto host an in-game Valentine\u2019s Day party, for example, traditional\ngame environments would require scripting tens of characters\u2019 be-\nhavior manually. We demonstrate that, with generative agents, it\nis sufficient to simply tell one agent that she wants to throw a\nparty. Despite many potential points of failure\u2014the party planner\nmust remember to tell other agents about the party, attendees must\nremember the invitation, those who remember must decide to ac-\ntually show up, and other possible points of failure\u2014agents in our\nenvironment succeed. They spread the word about the party and\nthen show up, with one agent even asking another agent on a date\nto the party, all from this single user-generated seed suggestion.\nWe conducted two evaluations of generative agents: a controlled\nevaluation to test whether the agents produce believable individual\n1A demonstration of an actual simulation of the generative agent society can be viewed\nat the following link: https://reverie.herokuapp.com/arXiv_Demo/\n\npage_label: 8\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What prompts the generative agent to react to a perceived observation in the given scenario?\nAnswer: "}, {"role": "assistant", "content": "The generative agent reacts to a perceived observation based on its agent architecture. This architecture includes a memory stream, which records the agent's experiences in natural language. When an observation is perceived, the retrieval model within the architecture surfaces relevant records from the memory stream based on relevance, recency, and importance. These records inform the agent's behavior in response to the observation. The agent also uses a process called reflection to synthesize memories into higher-level inferences over time, helping it draw conclusions to guide its behavior. Lastly, the agent uses planning to translate these conclusions and the current environment into high-level action plans and detailed behaviors for action and reaction."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 8\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\n\npage_label: 5\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the three main components of the agent architecture described in the context?\nAnswer: "}, {"role": "assistant", "content": "The three main components of the agent architecture described are memory, planning, and reflection. The memory component maintains a comprehensive record of the agent's experiences. The planning component uses the retrieved memories from the memory stream to determine the agent's actions and react appropriately to the environment. The reflection component uses the retrieved memories to form longer-term plans and create higher-level reflections, which are then entered into the memory stream for future use."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\nQuery: What are the three main components of the agent architecture described in the context?\nOriginal Answer: The three main components of the agent architecture described are memory, planning, and reflection. The memory component maintains a comprehensive record of the agent's experiences. The planning component uses the retrieved memories from the memory stream to determine the agent's actions and react appropriately to the environment. The reflection component uses the retrieved memories to form longer-term plans and create higher-level reflections, which are then entered into the memory stream for future use.\nNew Answer: "}, {"role": "assistant", "content": "The three main components of the agent architecture described are the language model, the sandbox interface, and the generative agent architecture. The language model is used to translate actions into a set of emojis, which are displayed on the sandbox interface. The sandbox interface provides an abstract representation of the action in the overhead view. The generative agent architecture determines whether agents walk by or engage in conversation with each other."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 12\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nif the action indicates an interaction between agents, we generate\ntheir dialogue.\n4.3.2 Dialogue. Agents converse as they interact with each other.\nWe generate agents\u2019 dialogue by conditioning their utterances on\ntheir memories about each other. For example, when John initiates\nhis conversation with Eddy, we generate John\u2019s first utterance\nby using his summarized memory about Eddy and the intended\nreaction when he decided to ask Eddy about his composition project:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nJohn is asking Eddy about his music composition\nproject. What would he say to Eddy?\nThe result: \u201cHey Eddy, how\u2019s the music composition project for your\nclass coming along?\u201d From Eddy\u2019s perspective, John initiating the\ndialogue is seen as an event to which he may want to react. So, just\nas John did, Eddy retrieves and summarizes his memory about his\nrelationship with John, as well as his memory that may be related\nto John\u2019s last utterance in the dialogue. If he decides to respond,\nwe generate Eddy\u2019s utterance using his summarized memory and\nthe current dialogue history:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nEddy Lin\u2019s status: Eddy is taking a short walk\naround his workplace.\nObservation: John is initiating a conversation\nwith Eddy.\nSummary of relevant context from Eddy\u2019s memory:\nJonn Lin is Eddy Lin\u2019s father. John Lin is caring\nand is interested to learn more about Eddy Lin\u2019s\nschool work. John Lin knows that Eddy Lin is\nworking on a music composition.\nHere is the dialogue history:\nJohn: Hey Eddy, how\u2019s the music composition project\nfor your class coming along?\nHow would Eddy respond to John?\nThis generates Eddy\u2019s response: \u201cHey Dad, it\u2019s going well. I\u2019ve been\ntaking walks around the garden to clear my head and get some in-\nspiration.\u201d The continuation of this dialogue is generated using the\nsame mechanism until one of the two agents decides to end the\ndialogue.\n5 SANDBOX ENVIRONMENT\nIMPLEMENTATION\nThe Smallville sandbox game environment is built using the Phaser\nweb game development framework [ 56]. The visual environmentsprites, including agent avatars, as well as an environment map\nand collision map that we authored, are imported into Phaser.\nWe supplement the sandbox development framework with a\nserver that makes the sandbox information available to generative\nagents and enables generative agents to move and influence the\nsandbox environment. The server maintains a JSON data structure\nthat contains information about each agent in the sandbox world,\nincluding their current location, a description of their current action,\nand the sandbox object they are interacting with. At each sandbox\ntime step, the sandbox server parses the JSON for any changes\ncoming from the generative agents, moves the agents to their new\npositions, and updates the status of any sandbox objects that the\nagents are interacting with (e.g., changing the status of the coffee\nmachine from \u201cidle\u201d to \u201cbrewing coffee\u201d if an agent\u2019s action is\n\u201cmaking espresso for a customer @ Hobbs Cafe: counter: coffee\nmachine\u201d). The sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n\npage_label: 12\nfile_path: docs/generative_agent.pdf\n\nThe sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n5.1 From Structured World Environments to\nNatural Language, And Back Again\nThe architecture of generative agents operates using natural lan-\nguage. Therefore, we need a mechanism to ground the agent\u2019s\nreasoning to the sandbox world. To achieve this, we represent the\nsandbox environment\u2014areas and objects\u2014as a tree data structure,\nwith an edge in the tree indicating a containment relationship in\nthe sandbox world. We convert this tree into natural language to\npass to the generative agents. For instance, \u201cstove\u201d being a child of\n\u201ckitchen\u201d is rendered into \u201cthere is a stove in the kitchen.\u201d\nAgents build individual tree representations of the environment\nas they navigate it \u2014 subgraphs of the overall sandbox environment\ntree. We initialize each agent with an environment tree capturing\nthe spaces and objects that the agent should be aware of: the rooms\nand objects in their living quarters, their workplace, and commonly\nvisited stores and shops. As the agents navigate the sandbox world,\nthey update this tree to reflect newly perceived areas. Agents are\nnot omniscient: their tree may get out of date as they leave an area,\nand is updated when they re-enter the area.\nTo determine the appropriate location for each action, we tra-\nverse the agent\u2019s stored environment tree and flatten a portion of\nit into natural language to prompt the language model. Recursively\nstarting at the root of the agent\u2019s environment tree, we prompt the\nmodel to find the most suitable area. For example, if Eddy\u2019s agent\nindicated that he should take a short walk around his workspace :\n[Agent\u2019s Summary Description]\nEddy Lin is currently in The Lin family\u2019s house:\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the mechanism used to generate agents' dialogue in the sandbox environment?\nAnswer: "}, {"role": "assistant", "content": "The mechanism used to generate agents' dialogue in the sandbox environment involves conditioning their utterances on their memories about each other. When an agent initiates a conversation, its first utterance is generated using its summarized memory about the other agent and the intended reaction. For instance, if John initiates a conversation with Eddy, John's first utterance is generated using his summarized memory about Eddy and his intended reaction when he decided to ask Eddy about his composition project. The same process is used for Eddy's response, where his utterance is generated based on his summarized memory about John and the current dialogue history. This dialogue generation continues until one of the agents decides to end the conversation."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 12\nfile_path: docs/generative_agent.pdf\n\nThe sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n5.1 From Structured World Environments to\nNatural Language, And Back Again\nThe architecture of generative agents operates using natural lan-\nguage. Therefore, we need a mechanism to ground the agent\u2019s\nreasoning to the sandbox world. To achieve this, we represent the\nsandbox environment\u2014areas and objects\u2014as a tree data structure,\nwith an edge in the tree indicating a containment relationship in\nthe sandbox world. We convert this tree into natural language to\npass to the generative agents. For instance, \u201cstove\u201d being a child of\n\u201ckitchen\u201d is rendered into \u201cthere is a stove in the kitchen.\u201d\nAgents build individual tree representations of the environment\nas they navigate it \u2014 subgraphs of the overall sandbox environment\ntree. We initialize each agent with an environment tree capturing\nthe spaces and objects that the agent should be aware of: the rooms\nand objects in their living quarters, their workplace, and commonly\nvisited stores and shops. As the agents navigate the sandbox world,\nthey update this tree to reflect newly perceived areas. Agents are\nnot omniscient: their tree may get out of date as they leave an area,\nand is updated when they re-enter the area.\nTo determine the appropriate location for each action, we tra-\nverse the agent\u2019s stored environment tree and flatten a portion of\nit into natural language to prompt the language model. Recursively\nstarting at the root of the agent\u2019s environment tree, we prompt the\nmodel to find the most suitable area. For example, if Eddy\u2019s agent\nindicated that he should take a short walk around his workspace :\n[Agent\u2019s Summary Description]\nEddy Lin is currently in The Lin family\u2019s house:\n\npage_label: 5\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What mechanism is used to ground the agent's reasoning to the sandbox world in the architecture of generative agents?\nAnswer: "}, {"role": "assistant", "content": "The mechanism used to ground the agent's reasoning to the sandbox world involves representing the sandbox environment, including areas and objects, as a tree data structure. An edge in the tree indicates a containment relationship in the sandbox world. This tree is converted into natural language to communicate with the generative agents. For example, if a \"stove\" is a child of \"kitchen\" in the tree, it is expressed as \"there is a stove in the kitchen.\" As agents navigate the sandbox world, they build individual tree representations of the environment and update this tree to reflect newly perceived areas."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 5\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the common affordances and features of the Smallville environment as described in the generative_agent.pdf document?\nAnswer: "}, {"role": "assistant", "content": "The Smallville environment includes common features of a small village such as a cafe, bar, park, school, dorm, houses, and stores. It also has subareas and objects that make these spaces functional, such as a kitchen in a house and a stove in the kitchen. The agents' primary living quarters are equipped with a bed, desk, closet, shelf, bathroom, and kitchen. Agents can move around Smallville, entering and leaving buildings, and navigating its map. The state of objects in this world can be influenced by users and agents, similar to sandbox games. For example, a bed can be occupied when an agent is sleeping, and a refrigerator can be empty when an agent uses up the ingredients to make breakfast. Users can also reshape an agent\u2019s environment by rewriting the status of objects surrounding the agent in natural language."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\nQuery: What are the common affordances and features of the Smallville environment as described in the generative_agent.pdf document?\nOriginal Answer: The Smallville environment includes common features of a small village such as a cafe, bar, park, school, dorm, houses, and stores. It also has subareas and objects that make these spaces functional, such as a kitchen in a house and a stove in the kitchen. The agents' primary living quarters are equipped with a bed, desk, closet, shelf, bathroom, and kitchen. Agents can move around Smallville, entering and leaving buildings, and navigating its map. The state of objects in this world can be influenced by users and agents, similar to sandbox games. For example, a bed can be occupied when an agent is sleeping, and a refrigerator can be empty when an agent uses up the ingredients to make breakfast. Users can also reshape an agent\u2019s environment by rewriting the status of objects surrounding the agent in natural language.\nNew Answer: "}, {"role": "assistant", "content": "In the Smallville environment, agents can interact with various features typical of a small village, including a cafe, bar, park, school, dorm, houses, and stores. These spaces are functional and contain subareas and objects, such as a kitchen in a house equipped with a stove. Agents have their own living quarters furnished with a bed, desk, closet, shelf, bathroom, and kitchen. They can freely move around Smallville, enter or leave buildings, and navigate the map. The state of objects in this world can be influenced by users and agents, mirroring the dynamics of sandbox games. For instance, a bed can be occupied when an agent sleeps, or a refrigerator can be empty if an agent uses all the ingredients for breakfast. Users can modify an agent\u2019s environment by altering the status of objects around the agent in natural language. Additionally, the system uses a language model to translate the agents' actions into a set of emojis, displayed in a speech bubble above their avatar, providing an abstract representation of the action in the overhead view."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n\npage_label: 17\nfile_path: docs/generative_agent.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key contributions of the paper on generative agents, as outlined in the context?\nAnswer: "}, {"role": "assistant", "content": "The paper introduces the concept of generative agents, which are interactive computational agents that simulate human behavior. It provides an architecture for these agents that allows them to store a comprehensive record of their experiences, deepen their understanding of themselves and their environment through reflection, and retrieve a compact subset of that information to inform their actions. The paper also explores the potential of generative agents by implementing them as non-player characters in a Sims-style game world and simulating their lives in it. The paper further discusses the ethical and societal impact of generative agents, outlining potential risks and proposing mitigation strategies. It also suggests future research directions to improve the architecture's performance and to understand the behavior of these agents over an extended period."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nFor instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What was the increase in network density from the start of the simulation to its end, based on the agents' responses and relationships formed?\nAnswer: "}, {"role": "assistant", "content": "The network density increased from 0.167 to 0.74 from the start of the simulation to its end."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 10\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 7: A reflection tree for Klaus Mueller. The agent\u2019s observations of the world, represented in the leaf nodes, are recur-\nsively synthesized to derive Klaus\u2019s self-notion that he is highly dedicated to his research.\nhas had the most frequent interactions: Wolfgang, his college dorm\nneighbor. Unfortunately, Wolfgang and Klaus only ever see each\nother in passing, and do not have deep interactions. A more desir-\nable response requires that the agent generalize from memories of\nKlaus spending hours on a research project to generate a higher-\nlevel reflection that Klaus is passionate about research, and likewise\nrecognize Maria putting in effort into her own research (albeit in\na different field), enabling a reflection that they share a common\ninterest. With the approach below, when Klaus is asked who to\nspend time with, Klaus chooses Maria instead of Wolfgang.\nApproach: We introduce a second type of memory, which we call\nareflection . Reflections are higher-level, more abstract thoughts\ngenerated by the agent. Because they are a type of memory, they\nare included alongside other observations when retrieval occurs.\nReflections are generated periodically; in our implementation, we\ngenerate reflections when the sum of the importance scores for the\nlatest events perceived by the agents exceeds a certain threshold.\nIn practice, our agents reflected roughly two or three times a day.\nThe first step in reflection is for the agent to determine what\nto reflect on, by identifying questions that can be asked given the\nagent\u2019s recent experiences. We query the large language model with\nthe 100 most recent records in the agent\u2019s memory stream (e.g.,\n\u201cKlaus Mueller is reading a book on gentrification\u201d, \u201cKlaus Mueller\nis conversing with a librarian about his research project\u201d, \u201cdesk\nat the library is currently unoccupied\u201d) and prompt the language\nmodel, \u201cGiven only the information above, what are 3 most salient\nhigh-level questions we can answer about the subjects in the state-\nments?\u201d The model\u2019s response generates candidate questions: for\nexample, What topic is Klaus Mueller passionate about? andWhatis the relationship between Klaus Mueller and Maria Lopez? . We use\nthese generated questions as queries for retrieval, and gather rele-\nvant memories (including other reflections) for each question. Then\nwe prompt the language model to extract insights and cite the par-\nticular records that served as evidence for the insights. The full\nprompt appears below:\nStatements about Klaus Mueller\n1. Klaus Mueller is writing a research paper\n2. Klaus Mueller enjoys reading a book\non gentrification\n3. Klaus Mueller is conversing with Ayesha Khan\nabout exercising [...]\nWhat 5 high-level insights can you infer from\nthe above statements? (example format: insight\n(because of 1, 5, 3))\nThis process generates statements such as Klaus Mueller is dedicated\nto his research on gentrification (because of 1, 2, 8, 15) . We parse and\nstore the statement as a reflection in the memory stream, including\npointers to the memory objects that were cited.\nReflection explicitly allows the agents to reflect not only on\ntheir observations but also on other reflections: for example, the\nsecond statement about Klaus Mueller above is a reflection that\nKlaus previously had, not an observation from his environment.\nAs a result, agents generate trees of reflections: the leaf nodes of\nthe tree represent the base observations, and the non-leaf nodes\nrepresent thoughts that become more abstract and higher-level\nhigher up the tree they are.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does the generative agent in the given context generate reflections, and what is the purpose of these reflections?\nAnswer: "}, {"role": "assistant", "content": "The generative agent generates reflections through a process that involves identifying questions based on the agent's recent experiences. This is done by querying a large language model with the 100 most recent records in the agent's memory stream. The model generates candidate questions, which are then used as queries for retrieval. Relevant memories, including other reflections, are gathered for each question. The language model is then prompted to extract insights and cite the particular records that served as evidence for these insights. This process results in statements that are parsed and stored as reflections in the memory stream. \n\nThe purpose of these reflections is to allow the agents to leverage a deeper understanding of themselves and others. They are higher-level, more abstract thoughts generated by the agent. These reflections are included alongside other observations when retrieval occurs. They are generated periodically and are based on the importance scores of the latest events perceived by the agents. Reflections also allow the agents to reflect not only on their observations but also on other reflections, leading to the generation of trees of reflections with base observations at the leaf nodes and more abstract thoughts at the non-leaf nodes."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: what were the conditions compared to the generative agent architecture in terms of believability by the human evaluators?\nAnswer: "}, {"role": "assistant", "content": "The conditions compared to the generative agent architecture in terms of believability by the human evaluators included ablations that disabled the agents' access to some or all of its three types of memory in its memory stream\u2014observation, reflection, and planning\u2014and a human-generated condition. The three ablated architectures were: a no observation, no reflection, no planning architecture without access to anything in the memory stream such as observations, plans, and reflections; a no reflection, no planning architecture with access to observations in the memory stream but no access to plans or reflections; and a no reflections architecture with access to observations and plans but without access to reflections. The human-generated condition involved a human crowdworker roleplaying the agent and authoring responses to the interview questions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/generative_agent.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n\npage_label: 1\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents: Interactive Simulacra of Human Behavior\nJoon Sung Park\nStanford University\nStanford, USA\njoonspk@stanford.eduJoseph C. O\u2019Brien\nStanford University\nStanford, USA\njobrien3@stanford.eduCarrie J. Cai\nGoogle Research\nMountain View, CA, USA\ncjcai@google.com\nMeredith Ringel Morris\nGoogle Research\nSeattle, WA, USA\nmerrie@google.comPercy Liang\nStanford University\nStanford, USA\npliang@cs.stanford.eduMichael S. Bernstein\nStanford University\nStanford, USA\nmsb@cs.stanford.edu\nFigure 1: Generative agents create believable simulacra of human behavior for interactive applications. In this work, we demon-\nstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can\nobserve and intervene as agents they plan their days, share news, form relationships, and coordinate group activities.\nABSTRACT\nBelievable proxies of human behavior can empower interactive\napplications ranging from immersive environments to rehearsal\nspaces for interpersonal communication to prototyping tools. In\nthis paper, we introduce generative agents\u2014computational software\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\narXiv, April, 2023,\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN xx-x-xxxx-xxxx-x/xx/xx. . . $15.00\nhttps://doi.org/xx.xx/xx.xxagents that simulate believable human behavior. Generative agents\nwake up, cook breakfast, and head to work; artists paint, while\nauthors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan\nthe next day. To enable generative agents, we describe an architec-\nture that extends a large language model to store a complete record\nof the agent\u2019s experiences using natural language, synthesize those\nmemories over time into higher-level reflections, and retrieve them\ndynamically to plan behavior. We instantiate generative agents\nto populate an interactive sandbox environment inspired by The\nSims, where end users can interact with a small town of twenty five\nagents using natural language. In an evaluation, these generative\nagents produce believable individual and emergent social behav-\niors: for example, starting with only a single user-specified notionarXiv:2304.03442v1  [cs.HC]  7 Apr 2023\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are some potential applications of generative agents beyond the sandbox demonstration presented in the work, as discussed in the provided context?\nAnswer: "}, {"role": "assistant", "content": "Generative agents can be applied in the human-centered design process, similar to cognitive models like GOMS and the Keystroke Level Model. For instance, a generative agent could model a user's life patterns and interactions with technology, acting as a proxy for that user. It could learn and exhibit plausible behaviors and reflections based on the user's life. This could include information such as when the user wakes up, when they need their first cup of coffee, and what their typical day looks like. Using this information, the agent could perform tasks like automatically brewing coffee, helping get the kids ready for school, and adjusting the ambient music and lighting to match the user\u2019s mood after a hard day at work. By using generative agents as proxies for users, a deeper understanding of their needs and preferences can be developed, leading to more personalized and effective technological experiences."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 20\nfile_path: docs/generative_agent.pdf\n\narXiv:2203.02155 [cs.CL]\n[79] Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2022. Social Simulacra: Creating Populated\nPrototypes for Social Computing Systems. In In the 35th Annual ACM Symposium\non User Interface Software and Technology (UIST \u201922) (Bend, OR, USA) (UIST \u201922) .\nAssociation for Computing Machinery, New York, NY, USA. https://doi.org/10.\n1145/3526113.3545616\n[80] Richard W. Pew and Ann S. Mavor (Eds.). 1998. Modeling Human and Organiza-\ntional Behavior: Applications to Military Simulations . National Academy Press,\nWashington, D.C.\n[81] Roberto Pillosu. 2009. Coordinating Agents with Behavior Trees: Synchronizing\nMultiple Agents in CryEngine 2 . https://aiarchitect.wordpress.com/2009/10/19/\ncoordinating-agents-with-behavior-trees-synchronizing-multiple-agents-in-\ncryengine-2/\n[82] Prolific. 2022. Prolific: Quickly Find Research Participants You Can Trust.\nhttps://www.prolific.co/\n[83] Byron Reeves and Clifford Nass. 1996. The media equation: How people treat\ncomputers, television, and new media like real people and places . Cambridge\nUniversity Press.\n[84] Mark O. Riedl. 2012. Interactive narrative: A novel application of artificial intel-\nligence for computer games. In Proceedings of the Twenty-Sixth AAAI Conference\non Artificial Intelligence (AAAI\u201912) . 2160\u20132165.\n[85] Mark O. Riedl and R. Michael Young. 2005. An Objective Character Believability\nEvaluation Procedure for Multi-Agent Story Generation Systems. In Proceedings\nof the 5th International Working Conference on Intelligent Virtual Agents (IVA\u201905) .\nKos, Greece, 58\u201370. https://doi.org/10.1007/11550617_5\n[86] David Rolf. 2015. The Fight for $15: The Right Wage for a Working America . The\nNew Press.\n[87] Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016.\nCodemend: Assisting interactive programming with bimodal embedding. In Pro-\nceedings of the 29th Annual Symposium on User Interface Software and Technology .\n247\u2013258.\n[88] Ben Shneiderman. 2022. Human-centered AI . Oxford University Press.\n[89] Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface\nagents. interactions 4, 6 (1997), 42\u201361.\n[90] Ho Chit Siu, Jaime Pe\u00f1a, Edenna Chen, Yutai Zhou, Victor Lopez, Kyle\nPalko, Kimberlee Chang, and Ross Allen. 2021. Evaluation of Human-AI\nTeams for Learned and Rule-Based Agents in Hanabi. In Advances in Neu-\nral Information Processing Systems , M. Ranzato, A. Beygelzimer, Y. Dauphin,\nP.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates,\nInc., 16183\u201316195. https://proceedings.neurips.cc/paper_files/paper/2021/file/\n86e8f7ab32cfd12577bc2619bc635690-Paper.pdf[91] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle\nRogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate.\n2022. An Information-theoretic Approach to Prompt Engineering Without\nGround Truth Labels. In Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.60\n[92] William Swartout, Jonathan Gratch, Randall Hill, Eduard Hovy, Stacy Marsella,\nJeff Rickel, and David Traum. 2006. Toward virtual humans. AI Magazine 27, 1\n(2006).\n\npage_label: 19\nfile_path: docs/generative_agent.pdf\n\nIn Proceedings of the 2001 Workshop on Intelligent Cinematography\nand Editing . 63\u201369.\n[58] John Laird and Michael VanLent. 2001. Human-Level AI\u2019s Killer Application:\nInteractive Computer Games. AI Magazine 22, 2 (2001), 15. https://doi.org/10.\n1609/aimag.v22i2.1558\n[59] John E. Laird. 2000. It Knows What You\u2019re Going To Do: Adding Anticipation\nto a QUAKEBOT. In Papers from the AAAI 2000 Spring Symposium on Artificial\nIntelligence and Interactive Entertainment (Technical Report SS-00-02) . AAAI\nPress, 41\u201350.\n[60] John E. Laird. 2012. The Soar Cognitive Architecture . MIT Press.\n[61] John E. Laird, Christian Lebiere, and Paul S. Rosenbloom. 2017. A Standard Model\nof the Mind: Toward a Common Computational Framework across Artificial\nIntelligence, Cognitive Science, Neuroscience, and Robotics. AI Magazine 38, 1\n(2017), 13\u201326.\n[62] Michelle S Lam, Zixian Ma, Anne Li, Izequiel Freitas, Dakuo Wang, James A\nLanday, and Michael S Bernstein. 2023. Model Sketching: Centering Concepts\nin Early-Stage Machine Learning Model Design. Proceedings of the SIGCHI\nConference on Human Factors in Computing Systems .\n[63] Pat Langley, Dongkyu Choi, and Seth Rogers. n.d.. Interleaving Learning, Problem\nSolving, and Execution in the Icarus Architecture . Technical Report. Stanford\nUniversity, Center for the Study of Language and Information.\n[64] Jason Linder, Gierad Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang,\nAseem Agarwala, and Eytan Adar. 2013. PixelTone: A Multimodal Interface for\nImage Editing. In CHI \u201913 Extended Abstracts on Human Factors in Computing\nSystems (Paris, France) (CHI EA \u201913) . Association for Computing Machinery,\nNew York, NY, USA, 2829\u20132830. https://doi.org/10.1145/2468356.2479533\n[65] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and\nWeizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? CoRR\nabs/2101.06804 (2021). arXiv:2101.06804 https://arxiv.org/abs/2101.06804\n[66] Vivian Liu, Han Qiao, and Lydia Chilton. 2022. Opal: Multimodal Image Gener-\nation for News Illustration. In Proceedings of the 35th Annual ACM Symposium\non User Interface Software and Technology . 1\u201317.\n[67] Pattie Maes. 1995. Artificial Life Meets Entertainment: Lifelike Autonomous\nAgents. Commun. ACM 38, 11 (nov 1995), 108\u2013114.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the field of AI and virtual agents, what publication discusses the topic of \"Toward virtual humans\" and who are the authors of this publication?\nAnswer: "}, {"role": "assistant", "content": "The publication that discusses the topic of \"Toward virtual humans\" is \"AI Magazine 27, 1\" from 2006. The authors of this publication are William Swartout, Jonathan Gratch, Randall Hill, Eduard Hovy, Stacy Marsella, Jeff Rickel, and David Traum."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: Lawrence Carin, and\nWeizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? CoRR\nabs/2101.06804 (2021). arXiv:2101.06804 https://arxiv.org/abs/2101.06804\n[66] Vivian Liu, Han Qiao, and Lydia Chilton. 2022. Opal: Multimodal Image Gener-\nation for News Illustration. In Proceedings of the 35th Annual ACM Symposium\non User Interface Software and Technology . 1\u201317.\n[67] Pattie Maes. 1995. Artificial Life Meets Entertainment: Lifelike Autonomous\nAgents. Commun. ACM 38, 11 (nov 1995), 108\u2013114. https://doi.org/10.1145/\n219717.219808\n[68] Josh McCoy, Michael Mateas, and Noah Wardrip-Fruin. 2009. Comme il Faut:\nA System for Simulating Social Games Between Autonomous Characters. In\nQuery: In the field of AI and virtual agents, what publication discusses the topic of \"Toward virtual humans\" and who are the authors of this publication?\nOriginal Answer: The publication that discusses the topic of \"Toward virtual humans\" is \"AI Magazine 27, 1\" from 2006. The authors of this publication are William Swartout, Jonathan Gratch, Randall Hill, Eduard Hovy, Stacy Marsella, Jeff Rickel, and David Traum.\nNew Answer: "}, {"role": "assistant", "content": "The publication discussing the topic of \"Toward virtual humans\" is \"AI Magazine 27, 1\" published in 2006. The authors of this publication are William Swartout, Jonathan Gratch, Randall Hill, Eduard Hovy, Stacy Marsella, Jeff Rickel, and David Traum."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 20\nfile_path: docs/generative_agent.pdf\n\n2021. Recursively Summarizing Books with Human\nFeedback. arXiv:2109.10862 [cs.CL]\n[105] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language\nModel Prompts through Visual Programming. In CHI EA \u201922: Extended Abstracts\nof the 2022 CHI Conference on Human Factors in Computing Systems .\n[106] Tongshuang Wu, Michael Terry, and Carrie J Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. In CHI \u201922: Proceedings of the 2022 CHI Conference on Human Factors in\nComputing Systems .\n[107] Qian Yang, Aaron Steinfeld, Carolyn Ros\u00e9, and John Zimmerman. 2020. Re-\nexamining whether, why, and how human-AI interaction is uniquely difficult to\ndesign. In Proceedings of the 2020 chi conference on human factors in computing\nsystems . 1\u201313.\n[108] Georgios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . ACM, Cagliari, Italy, 285\u2013292. https://doi.\norg/10.1145/2212908.2212950\n[109] Geogios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . 285\u2013292.\n[110] Robert Zubek. 2002. Towards implementation of social interaction. In AAAI\nSpring Symposium on Artificial Intelligence and Interactive Entertainment . AAAI\nPress. https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-01/SS02-01-\n003.pdf\nA ARCHITECTURE OPTIMIZATIONS\nMany of our prompts require a concise summary of the agent,\nshorthanded as [Agent\u2019s Summary Description] in prompts\nabove. In our implementation, this summary comprises agents\u2019\nidentity information (e.g., name, age, personality), as well as a\ndescription of their main motivational drivers and statements that\ndescribes their current occupation and self-assessment. Since this\ninformation is frequently used in many prompts, we synthesize it\nat regular intervals and access that synthesis as a cache.\n\npage_label: 20\nfile_path: docs/generative_agent.pdf\n\n2022. An Information-theoretic Approach to Prompt Engineering Without\nGround Truth Labels. In Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.60\n[92] William Swartout, Jonathan Gratch, Randall Hill, Eduard Hovy, Stacy Marsella,\nJeff Rickel, and David Traum. 2006. Toward virtual humans. AI Magazine 27, 1\n(2006).\n[93] Milind Tambe, W Lewis Johnson, Randolph M Jones, Frank Koss, John E Laird,\nPaul S Rosenbloom, and Karl Schwamb. 1995. Intelligent agents for interactive\nsimulation environments. AI Magazine 16, 1 (1995), 15.\n[94] David R. Thomas. 2006. A General Inductive Approach for Analyzing Qualitative\nEvaluation Data. American Journal of Evaluation 27, 2 (2006), 237\u2013246. https:\n//doi.org/10.1177/1098214005283748\n[95] Frank Thomas and Ollie Johnston. 1981. Disney Animation: The Illusion of Life .\nAbbeville Press, New York.\n[96] Ilshat Umarov, Mikhail Mozgovoy, and Patrick C. Rogers. 2012. Believable and\nEffective AI Agents in Virtual Worlds: Current State and Future Perspectives.\nInternational Journal of Gaming and Computer-Mediated Simulations 4, 2 (2012),\n37\u201359.\n[97] Graham Upton and Ian Cook. 2006. A Dictionary of Statistics (2 ed.). Oxford\nUniversity Press, Oxford, United Kingdom.\n[98] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, and et al. 2019. Grand-\nmaster level in StarCraft II using multi-agent reinforcement learning. Nature\n575 (2019), 350\u2013354. https://doi.org/10.1038/s41586-019-1724-z\n[99] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\nXia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL]\n[100] Mark Weiser. 1991. The computer for the 21st century. Scientific American 265,\n3 (1991), 94\u2013104. https://doi.org/10.1038/scientificamerican0991-94\n[101] Mark Weiser. 1999. The Computer for the 21st Century. SIGMOBILE Mob.\nComput. Commun. Rev. 3, 3 (jul 1999), 3\u201311. https://doi.org/10.1145/329124.\n329126\n[102] Joseph Weizenbaum. 1966. ELIZA\u2014a computer program for the study of natural\nlanguage communication between man and machine. Commun. ACM 9, 1 (1966),\n36\u201345.\n[103] Terry Winograd. 1971. Procedures as a Representation for Data in a Computer\nProgram for Understanding Natural Language. (1971).\n[104] Jeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan\nLeike, and Paul Christiano. 2021. Recursively Summarizing Books with Human\nFeedback. arXiv:2109.10862 [cs.CL]\n[105] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language\nModel Prompts through Visual Programming. In CHI EA \u201922: Extended Abstracts\nof the 2022 CHI Conference on Human Factors in Computing Systems .\n[106] Tongshuang Wu, Michael Terry, and Carrie J Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. In CHI \u201922: Proceedings of the 2022 CHI Conference on Human Factors in\nComputing Systems .\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the components included in the agent's summary description, as mentioned in the implementation of prompts?\nAnswer: "}, {"role": "assistant", "content": "The agent's summary description includes the agent's identity information such as name, age, and personality. It also includes a description of their main motivational drivers, and statements that describe their current occupation and self-assessment."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/generative_agent.pdf\n\nFor instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nThe second is reflection , which synthesizes memories into higher-\nlevel inferences over time, enabling the agent to draw conclusions\nabout itself and others to better guide its behavior. The third is\nplanning , which translates those conclusions and the current en-\nvironment into high-level action plans and then recursively into\ndetailed behaviors for action and reaction. These reflections and\nplans are fed back into the memory stream to influence the agent\u2019s\nfuture behavior.\nThis architecture suggests applications in multiple domains, from\nrole-play and social prototyping, to virtual worlds and games. In\nsocial role-play scenarios (e.g. interview preparation), a user could\nsafely rehearse difficult, conflict-laden conversations. When pro-\ntotyping social platforms, a designer could go beyond temporary\npersonas to prototype dynamic, complex interactions that unfold\nover time. For the purposes of this paper, we focus on the ability to\ncreate a small, interactive society of agents inspired by games such\nas The Sims.1By connecting our architecture to the ChatGPT large\nlanguage model [ 76], we manifest a small society of twenty five\nagents in a game environment. End users can observe and interact\nwith these agents. If an end user or developer wanted the town\nto host an in-game Valentine\u2019s Day party, for example, traditional\ngame environments would require scripting tens of characters\u2019 be-\nhavior manually. We demonstrate that, with generative agents, it\nis sufficient to simply tell one agent that she wants to throw a\nparty. Despite many potential points of failure\u2014the party planner\nmust remember to tell other agents about the party, attendees must\nremember the invitation, those who remember must decide to ac-\ntually show up, and other possible points of failure\u2014agents in our\nenvironment succeed. They spread the word about the party and\nthen show up, with one agent even asking another agent on a date\nto the party, all from this single user-generated seed suggestion.\nWe conducted two evaluations of generative agents: a controlled\nevaluation to test whether the agents produce believable individual\n1A demonstration of an actual simulation of the generative agent society can be viewed\nat the following link: https://reverie.herokuapp.com/arXiv_Demo/\n\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\nLikewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room. At 8\na.m., Eddy follows suit, rushing out of bed to prepare for his classes.\nHe catches up with John just as he is about to head out:\nJohn : Good morning Eddy. Did you sleep well?\nEddy : Good morning dad. Y eah, I slept great.\nJohn : That\u2019s good. What are you working on today?\nEddy : I\u2019m working on a new music composition for my\nclass. It\u2019s due this week, so I\u2019m trying to get it finished.\nBut I\u2019m having so much fun with it!\nJohn : That sounds great!\nSoon after Eddy heads out, Mei wakes up and joins John. Mei asks\nabout her son, and John recalls the conversation that they just had:\nMei: Did Eddy already leave for school?\nJohn : Y es, he just left. He\u2019s working on a music com-\nposition for his class.\nMei: Oh, that\u2019s wonderful! Tell me about it.\nJohn : I think he\u2019s really enjoying it! He said he\u2019s having\na lot of fun with it.\nMei: That\u2019s great! I\u2019m so proud of him.\nAs they finish their conversation, Mei and John pack up. Mei goes\non to teach and work on her research paper, while John opens his\npharmacy counter at the Willow Market and Pharmacy by 9 a.m.\n3.4 Emergent Social Behaviors\nBy interacting with each other, generative agents in Smallville\nexchange information, form new relationships, and coordinate joint\nactivities. Extending prior work [ 79], these social behaviors are\nemergent rather than pre-programmed.\n3.4.1 Information Diffusion. As agents notice each other, they may\nengage in dialogue\u2014as they do so, information can spread from\nagent to agent. For instance, in a conversation between Sam and\nTom at the grocery store, Sam tells Tom about his candidacy in the\nlocal election:\nSam : Hey Tom, how\u2019s it going?\nTom: Good, thanks. What\u2019s up?\nSam : Well, I wanted to talk to you about something. I\u2019m\nactually running for mayor in the upcoming local elec-\ntion.\nTom: Really? That\u2019s great news! Why are you running?\nSam : I\u2019ve been involved in local politics for years now,\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the example of coordination among generative agents mentioned in the context?\nAnswer: "}, {"role": "assistant", "content": "An example of coordination among generative agents is when a user or developer wants to host an in-game Valentine\u2019s Day party. Instead of manually scripting the behavior of numerous characters, it is enough to tell one agent that she wants to throw a party. The party planner agent then remembers to inform other agents about the party, attendees remember the invitation, and those who remember decide to show up. This demonstrates that generative agents can coordinate activities based on a single user-generated seed suggestion."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Which condition produced more believable behavior than the others, and what method was used to investigate the statistical significance of this result?\nAnswer: "}, {"role": "assistant", "content": "The full architecture of generative agents produced the most believable behavior compared to the other conditions. The statistical significance of this result was investigated using the Kruskal-Wallis test, a non-parametric alternative to the one-way ANOVA. This was applied to the raw ranks data. To identify any pairwise differences between the conditions, the Dunn post-hoc test was performed. The p-values for multiple comparisons in the Dunn test were adjusted using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the finding regarding the performance of the full generative agent architecture compared to the ablated architectures and the human condition in the study?\nAnswer: "}, {"role": "assistant", "content": "The study found that the full generative agent architecture produced the most believable behavior among all study conditions. The performance degraded with each component removed in the ablation conditions. The ablated architecture with no access to reflection was the next best, followed by no access to reflection or planning, and then the human condition. The worst performance was observed in the ablated architecture with no access to memory, planning, or reflection."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What advantage did reflection provide for generative agents in making decisions that required a deeper synthesis of their experiences?\nAnswer: "}, {"role": "assistant", "content": "Reflection provided an advantage for generative agents by allowing them to make decisions that required a deeper understanding of their experiences. For example, when asked what she might get Wolfgang Schulz for his birthday, Maria Lopez, without access to reflection, acknowledged her uncertainty, stating that she did not know what Wolfgang likes, despite having had many interactions with him. However, with access to reflection memories, Maria answered confidently, suggesting a gift related to his interest in mathematical music composition. This shows that reflection enabled the agents to synthesize their experiences and use that knowledge to make more informed decisions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/generative_agent.pdf\n\nFor instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nThe second is reflection , which synthesizes memories into higher-\nlevel inferences over time, enabling the agent to draw conclusions\nabout itself and others to better guide its behavior. The third is\nplanning , which translates those conclusions and the current en-\nvironment into high-level action plans and then recursively into\ndetailed behaviors for action and reaction. These reflections and\nplans are fed back into the memory stream to influence the agent\u2019s\nfuture behavior.\nThis architecture suggests applications in multiple domains, from\nrole-play and social prototyping, to virtual worlds and games. In\nsocial role-play scenarios (e.g. interview preparation), a user could\nsafely rehearse difficult, conflict-laden conversations. When pro-\ntotyping social platforms, a designer could go beyond temporary\npersonas to prototype dynamic, complex interactions that unfold\nover time. For the purposes of this paper, we focus on the ability to\ncreate a small, interactive society of agents inspired by games such\nas The Sims.1By connecting our architecture to the ChatGPT large\nlanguage model [ 76], we manifest a small society of twenty five\nagents in a game environment. End users can observe and interact\nwith these agents. If an end user or developer wanted the town\nto host an in-game Valentine\u2019s Day party, for example, traditional\ngame environments would require scripting tens of characters\u2019 be-\nhavior manually. We demonstrate that, with generative agents, it\nis sufficient to simply tell one agent that she wants to throw a\nparty. Despite many potential points of failure\u2014the party planner\nmust remember to tell other agents about the party, attendees must\nremember the invitation, those who remember must decide to ac-\ntually show up, and other possible points of failure\u2014agents in our\nenvironment succeed. They spread the word about the party and\nthen show up, with one agent even asking another agent on a date\nto the party, all from this single user-generated seed suggestion.\nWe conducted two evaluations of generative agents: a controlled\nevaluation to test whether the agents produce believable individual\n1A demonstration of an actual simulation of the generative agent society can be viewed\nat the following link: https://reverie.herokuapp.com/arXiv_Demo/\n\npage_label: 8\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key components of a plan for a generative agent, as described in the provided context?\nAnswer: "}, {"role": "assistant", "content": "The key components of a plan for a generative agent include the memory stream, reflection, and planning. The memory stream is a long-term memory module that records a comprehensive list of the agent\u2019s experiences in natural language. The retrieval model within this component combines relevance, recency, and importance to surface the records that are needed to inform the agent\u2019s moment-to-moment behavior. Reflection synthesizes memories into higher-level inferences over time, enabling the agent to draw conclusions about itself and others to better guide its behavior. Planning translates those conclusions and the current environment into high-level action plans and then recursively into detailed behaviors for action and reaction. These reflections and plans are fed back into the memory stream to influence the agent\u2019s future behavior."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from the time when the reaction takes place. Finally,\n\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\n4.3 Planning and Reacting\nChallenge: While a large language model can generate plausible be-\nhavior in response to situational information (e.g., [ 45,79]), agents\nneed to plan over a longer time horizon to ensure that their sequence\nof actions is coherent and believable. If we prompt a language model\nwith Klaus\u2019s background, describe the time, and ask what action\nhe ought to take at the given moment, Klaus would eat lunch at 12\npm, but then again at 12:30 pm and 1 pm, despite having already\neaten his lunch twice. Optimizing for believability in the moment\nsacrifices believability over time. To overcome this issue, planning\nis essential. With the approach described below, Klaus\u2019s afternoon\nplan is less gluttonous: he has lunch at Hobbs Cafe while reading\nat 12pm, works on his research paper at the school library at 1pm,\nand takes a break for a walk in the park at 3pm.\nApproach: Plans describe a future sequence of actions for the agent,\nand help keep the agent\u2019s behavior consistent over time. A plan\nincludes a location, a starting time, and a duration. For instance,\nKlaus Mueller, who is dedicated in his research and has an impend-\ning deadline4, may choose to spend his day working at his desk\ndrafting his research paper. An entry in a plan might state, for\nexample: for 180 minutes from 9am, February 12th, 2023, at Oak Hill\nCollege Dorm: Klaus Mueller\u2019s room: desk, read and take notes for\nresearch paper . Like reflections, plans are stored in the memory\nstream and are included in the retrieval process. This allows the\nagent to consider observations, reflections, and plans all together\nwhen deciding how to behave. Agents may change their plans\nmidstream if needed.\nIt would be unrealistic and uninteresting for an artist agent\nto plan on painting while sitting at a pharmacy counter for four\nhours without moving. A more desirable plan would involve the\nagent taking the necessary time to gather materials, mix paint,\ntake breaks, and clean up during the four-hour period in its home\nstudio. To create such plans, our approach starts top-down and\nthen recursively generates more detail. The first step is to create\na plan that outlines the day\u2019s agenda in broad strokes. To create\nthe initial plan, we prompt the language model with the agent\u2019s\nsummary description (e.g., name, traits, and summary of their recent\nexperiences) and a summary of their previous day. A full example\nprompt is below, which is unfinished at the bottom for the language\nmodel to complete:\nName: Eddy Lin (age: 19)\nInnate traits: friendly, outgoing, hospitable\nEddy Lin is a student at Oak Hill College studying\nmusic theory and composition. He loves to explore\ndifferent musical styles and is always looking\nfor ways to expand his knowledge. Eddy Lin is\nworking on a composition project for his college\nclass. He is also taking classes to learn more\nabout music\ntheory. Eddy Lin is excited about the new composition\nhe is working on but he wants to dedicate more\nhours in the day to work on it in the coming days\nOn Tuesday February 12, Eddy 1) woke up and\ncompleted the morning routine at 7:00 am, [. . . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What prompts the generative agent to react and update its plan according to the described scenario?\nAnswer: "}, {"role": "assistant", "content": "The generative agent reacts and updates its plan based on the observations it perceives from the world around it at each time step. These observations are stored in the agent's memory stream. For instance, if Eddy's father John observes Eddy taking a short walk in the house garden, this observation could prompt a reaction. The agent then decides whether to continue with the existing plan or react based on these observations. If a reaction is deemed appropriate, the agent's existing plan is regenerated starting from the time when the reaction takes place."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: am, [. . . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm.\nQuery: What prompts the generative agent to react and update its plan according to the described scenario?\nOriginal Answer: The generative agent reacts and updates its plan based on the observations it perceives from the world around it at each time step. These observations are stored in the agent's memory stream. For instance, if Eddy's father John observes Eddy taking a short walk in the house garden, this observation could prompt a reaction. The agent then decides whether to continue with the existing plan or react based on these observations. If a reaction is deemed appropriate, the agent's existing plan is regenerated starting from the time when the reaction takes place.\nNew Answer: "}, {"role": "assistant", "content": "The generative agent adjusts its plan according to the information it gathers from its surroundings at every moment. This data is kept in the agent's memory stream. For example, if it is observed that Eddy is taking a brief stroll in the home garden, this could trigger a response. The agent then makes a decision to either stick with the current plan or respond based on these observations. If a response is considered necessary, the agent's existing plan is recreated starting from the time the response occurs."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 5\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\n\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What framework was used to build the Smallville sandbox game environment?\nAnswer: "}, {"role": "assistant", "content": "The context does not provide specific information on the framework used to build the Smallville sandbox game environment."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\nQuery: What framework was used to build the Smallville sandbox game environment?\nOriginal Answer: The context does not provide specific information on the framework used to build the Smallville sandbox game environment.\nNew Answer: "}, {"role": "assistant", "content": "The context does not provide specific information on the framework used to build the Smallville sandbox game environment."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 5\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the user controls for communicating with the generative agent in the Smallville simulation?\nAnswer: "}, {"role": "assistant", "content": "In the Smallville simulation, a user can interact with the generative agent in two main ways. Firstly, they can communicate through conversation using natural language, specifying a persona that the agent should perceive them as. For example, if the user identifies themselves as a news reporter and asks a question, the agent will respond accordingly. Secondly, a user can issue a directive to an agent by taking on the persona of the agent's 'inner voice'. This method is more likely to be treated as a command by the agent. For instance, if the user, as the agent's inner voice, tells the agent to run for office in an upcoming election, the agent will decide to run for office."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\nQuery: What are the user controls for communicating with the generative agent in the Smallville simulation?\nOriginal Answer: In the Smallville simulation, a user can interact with the generative agent in two main ways. Firstly, they can communicate through conversation using natural language, specifying a persona that the agent should perceive them as. For example, if the user identifies themselves as a news reporter and asks a question, the agent will respond accordingly. Secondly, a user can issue a directive to an agent by taking on the persona of the agent's 'inner voice'. This method is more likely to be treated as a command by the agent. For instance, if the user, as the agent's inner voice, tells the agent to run for office in an upcoming election, the agent will decide to run for office.\nNew Answer: "}, {"role": "assistant", "content": "In the Smallville simulation, users have two primary methods of interaction with the generative agent. One way is through natural language dialogue, where the user assumes a specific persona that the agent recognizes. For instance, if the user poses as a journalist and asks a question, the agent will respond appropriately. Another way is by acting as the agent's 'inner voice' and giving a directive. This approach is often seen as a command by the agent. For example, if the user, posing as the agent's inner voice, instructs the agent to participate in an upcoming election, the agent will make a decision to run for office."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\nLikewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room. At 8\na.m., Eddy follows suit, rushing out of bed to prepare for his classes.\nHe catches up with John just as he is about to head out:\nJohn : Good morning Eddy. Did you sleep well?\nEddy : Good morning dad. Y eah, I slept great.\nJohn : That\u2019s good. What are you working on today?\nEddy : I\u2019m working on a new music composition for my\nclass. It\u2019s due this week, so I\u2019m trying to get it finished.\nBut I\u2019m having so much fun with it!\nJohn : That sounds great!\nSoon after Eddy heads out, Mei wakes up and joins John. Mei asks\nabout her son, and John recalls the conversation that they just had:\nMei: Did Eddy already leave for school?\nJohn : Y es, he just left. He\u2019s working on a music com-\nposition for his class.\nMei: Oh, that\u2019s wonderful! Tell me about it.\nJohn : I think he\u2019s really enjoying it! He said he\u2019s having\na lot of fun with it.\nMei: That\u2019s great! I\u2019m so proud of him.\nAs they finish their conversation, Mei and John pack up. Mei goes\non to teach and work on her research paper, while John opens his\npharmacy counter at the Willow Market and Pharmacy by 9 a.m.\n3.4 Emergent Social Behaviors\nBy interacting with each other, generative agents in Smallville\nexchange information, form new relationships, and coordinate joint\nactivities. Extending prior work [ 79], these social behaviors are\nemergent rather than pre-programmed.\n3.4.1 Information Diffusion. As agents notice each other, they may\nengage in dialogue\u2014as they do so, information can spread from\nagent to agent. For instance, in a conversation between Sam and\nTom at the grocery store, Sam tells Tom about his candidacy in the\nlocal election:\nSam : Hey Tom, how\u2019s it going?\nTom: Good, thanks. What\u2019s up?\nSam : Well, I wanted to talk to you about something. I\u2019m\nactually running for mayor in the upcoming local elec-\ntion.\nTom: Really? That\u2019s great news! Why are you running?\nSam : I\u2019ve been involved in local politics for years now,\n\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nFor instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What emergent social behaviors do generative agents in Smallville exhibit, and how are these behaviors different from pre-programmed behaviors?\nAnswer: "}, {"role": "assistant", "content": "Generative agents in Smallville exhibit emergent social behaviors such as information diffusion, relationship formation, and agent coordination. These behaviors are not pre-programmed but emerge naturally as the agents interact with each other and their environment. Information diffusion occurs when agents engage in dialogue and share information, which can then spread from agent to agent. Relationship formation happens as agents get to know each other over time, and agent coordination involves joint activities that agents plan and carry out together. These behaviors are emergent because they are not explicitly programmed into the agents but arise from their interactions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 3\nfile_path: docs/generative_agent.pdf\n\nAs research progressed, it\nbecame clear that autonomous agents could offer new metaphors\nfor delegation and interaction [ 67], but the delegation lines be-\ntween humans and agents have continued to be debated and refined\n[46,88,89]. Recently, this technology has become stable enough\nthat it has become possible for agents to interact via natural lan-\nguage in large and complex online social environments (e.g., [ 54]).\nNatural language interaction offers a novel modality that can ex-\ntend user abilities in domains such as photo editing [ 2,34,64] and\ncode editing [87].\nWe convene these threads of work to show that we can now\ncreate agents that proxy human behavior for interactive systems,\nand interact with them via natural language. In doing so, this\nwork re-opens the door to examining foundational HCI questions\naround cognitive models such as GOMS and KLM [ 21,22], around\nprototyping tools [ 79], and around ubiquitous computing applica-\ntions [25, 30, 100].\n2.2 Believable Proxies of Human Behavior\nPrior literature has described believability , orbelievable agents , as a\ncentral design and engineering goal. Believable agents are designed\nto provide an illusion of life and present a facade of realism in the\nway they appear to make decisions and act on their own volition,\nsimilar to the characters in Disney movies [ 9,95]. These agents can\npopulate and perceive an open-world environment like the one we\ninhabit [ 9,58], and strive to behave in ways that exhibit emergent\nbehaviors grounded in social interactions with users or other agents\nwith the aim of becoming believable proxies of our behavior in hy-\npothetical simulations of individuals and communities [ 19,35,70].\nHistorically, these agents were developed in the context of intelli-\ngent game NPCs [ 58,84]. Creating NPCs with believable behavior,\nif possible, could enhance player experiences in games and inter-\nactive fictions by enabling emergent narratives [ 7,15,48,92] and\nsocial interactions with the agents [ 110]. However, more impor-\ntantly, game worlds provide increasingly realistic representations\nof real-world affordances, and as observed by Laird and van Lent in\n2001, these simulated worlds offer accessible testbeds for develop-\ners of believable agents to finesse the agents\u2019 cognitive capabilities\nwithout worrying about implementing robotics in the real world\nor creating simulation environments from scratch [58, 84].\nA diverse set of approaches to creating believable agents emerged\nover the past four decades. In implementation, however, these ap-\nproaches often simplified the environment or dimensions of agent\nbehavior to make the effort more manageable [ 16,72]. Rule-based\napproaches, such as finite-state machines [ 90,96] and behavior\n\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\nEvaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176. https://doi.org/10.1145/2642918.2647395\n[3]Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza.\n2014. Power to the people: The role of humans in interactive machine learning.\nAI Magazine 35, 4 (2014), 105\u2013120.\n[4]Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira\nNushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,\net al.2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi\nconference on human factors in computing systems . 1\u201313.\n[5]John R. Anderson. 1993. Rules of the Mind . Lawrence Erlbaum Associates,\nHillsdale, NJ.\n[6] Electronic Arts. 2009. The Sims 3. Video game.\n[7]Ruth Aylett. 1999. Narrative in virtual environments\u2014towards emergent narra-\ntive. In Narrative Intelligence: Papers from the AAAI Fall Symposium (Technical\nReport FS-99-01) . AAAI Press, 83\u201386.\n[8]Christoph Bartneck and Jodi Forlizzi. 2004. A design-centered framework for\nsocial human-robot interaction. In Proceedings of the 13th IEEE International\nWorkshop on Robot and Human Interactive Communication (RO-MAN\u201904) . 591\u2013\n594. https://doi.org/10.1109/ROMAN.2004.1374827\n[9]Joseph Bates. 1994. The Role of Emotion in Believable Agents. Commun. ACM\n37, 7 (1994), 122\u2013125. https://doi.org/10.1145/176789.176803\n[10] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw\nD\u0119biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris\nHesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael\nPetrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter,\nJonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan\nZhang. 2019. Dota 2 with Large Scale Deep Reinforcement Learning. arXiv\npreprint arXiv:1912.06680 (2019).\n[11] Marcel Binz and Eric Schulz. 2023. Using cognitive psychology to under-\nstand GPT-3. Proceedings of the National Academy of Sciences 120, 6 (2023),\ne2218523120.\n[12] BioWare. 2007. Mass Effect. Video game.\n[13] Woody Bledsoe. 1986. I had a dream: AAAI presidential address. AI Magazine 7,\n1 (1986), 57\u201361.\n[14] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the\nOpportunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG]\n[15] Michael Brenner. 2010. Creating dynamic story plots with continual multiagent\nplanning.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the central design and engineering goals of believable agents, as described in the prior literature?\nAnswer: "}, {"role": "assistant", "content": "Believable agents are designed with the central goals of providing an illusion of life and presenting a facade of realism in their decision-making and actions. They are intended to behave as if they have their own volition, similar to characters in Disney movies. These agents are designed to populate and perceive an open-world environment like ours and strive to exhibit emergent behaviors grounded in social interactions with users or other agents. The aim is for them to become believable proxies of human behavior in hypothetical simulations of individuals and communities."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nFor instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What was the increase in network density from the start to the end of the simulation in the context of the Valentine's Day party?\nAnswer: "}, {"role": "assistant", "content": "The network density increased from 0.167 to 0.74 from the start to the end of the simulation."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 10\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 7: A reflection tree for Klaus Mueller. The agent\u2019s observations of the world, represented in the leaf nodes, are recur-\nsively synthesized to derive Klaus\u2019s self-notion that he is highly dedicated to his research.\nhas had the most frequent interactions: Wolfgang, his college dorm\nneighbor. Unfortunately, Wolfgang and Klaus only ever see each\nother in passing, and do not have deep interactions. A more desir-\nable response requires that the agent generalize from memories of\nKlaus spending hours on a research project to generate a higher-\nlevel reflection that Klaus is passionate about research, and likewise\nrecognize Maria putting in effort into her own research (albeit in\na different field), enabling a reflection that they share a common\ninterest. With the approach below, when Klaus is asked who to\nspend time with, Klaus chooses Maria instead of Wolfgang.\nApproach: We introduce a second type of memory, which we call\nareflection . Reflections are higher-level, more abstract thoughts\ngenerated by the agent. Because they are a type of memory, they\nare included alongside other observations when retrieval occurs.\nReflections are generated periodically; in our implementation, we\ngenerate reflections when the sum of the importance scores for the\nlatest events perceived by the agents exceeds a certain threshold.\nIn practice, our agents reflected roughly two or three times a day.\nThe first step in reflection is for the agent to determine what\nto reflect on, by identifying questions that can be asked given the\nagent\u2019s recent experiences. We query the large language model with\nthe 100 most recent records in the agent\u2019s memory stream (e.g.,\n\u201cKlaus Mueller is reading a book on gentrification\u201d, \u201cKlaus Mueller\nis conversing with a librarian about his research project\u201d, \u201cdesk\nat the library is currently unoccupied\u201d) and prompt the language\nmodel, \u201cGiven only the information above, what are 3 most salient\nhigh-level questions we can answer about the subjects in the state-\nments?\u201d The model\u2019s response generates candidate questions: for\nexample, What topic is Klaus Mueller passionate about? andWhatis the relationship between Klaus Mueller and Maria Lopez? . We use\nthese generated questions as queries for retrieval, and gather rele-\nvant memories (including other reflections) for each question. Then\nwe prompt the language model to extract insights and cite the par-\nticular records that served as evidence for the insights. The full\nprompt appears below:\nStatements about Klaus Mueller\n1. Klaus Mueller is writing a research paper\n2. Klaus Mueller enjoys reading a book\non gentrification\n3. Klaus Mueller is conversing with Ayesha Khan\nabout exercising [...]\nWhat 5 high-level insights can you infer from\nthe above statements? (example format: insight\n(because of 1, 5, 3))\nThis process generates statements such as Klaus Mueller is dedicated\nto his research on gentrification (because of 1, 2, 8, 15) . We parse and\nstore the statement as a reflection in the memory stream, including\npointers to the memory objects that were cited.\nReflection explicitly allows the agents to reflect not only on\ntheir observations but also on other reflections: for example, the\nsecond statement about Klaus Mueller above is a reflection that\nKlaus previously had, not an observation from his environment.\nAs a result, agents generate trees of reflections: the leaf nodes of\nthe tree represent the base observations, and the non-leaf nodes\nrepresent thoughts that become more abstract and higher-level\nhigher up the tree they are.\n\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the purpose of introducing reflections as a type of memory in the agent's system, and how are they generated?\nAnswer: "}, {"role": "assistant", "content": "Reflections serve as higher-level, more abstract thoughts generated by the agent. They are introduced as a type of memory to allow the agent to generalize from its experiences and form deeper understandings. The process of generating reflections begins when the sum of the importance scores for the latest events perceived by the agent exceeds a certain threshold. \n\nThe agent then identifies what to reflect on by asking questions based on its recent experiences. The large language model is queried with the 100 most recent records in the agent\u2019s memory stream, and the model generates candidate questions. These questions are then used as queries for retrieval, and relevant memories, including other reflections, are gathered for each question. \n\nThe language model is then prompted to extract insights and cite the particular records that served as evidence for these insights. The resulting statements, such as \"Klaus Mueller is dedicated to his research on gentrification\", are parsed and stored as a reflection in the memory stream, including pointers to the memory objects that were cited. This allows the agents to reflect not only on their observations but also on other reflections, generating trees of reflections where the leaf nodes represent the base observations, and the non-leaf nodes represent thoughts that become more abstract and higher-level the higher up the tree they are."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nEddy Lin\u2019s bedroom: desk) that has Mei and John\nLin\u2019s\nbedroom, Eddy Lin\u2019s bedroom, common room, kitchen,\nbathroom, and garden.\nEddy Lin knows of the following areas: The Lin\nfamily\u2019s house, Johnson Park, Harvey Oak Supply\nStore, The Willows Market and Pharmacy, Hobbs\nCafe, The Rose and Crown Pub.\n* Prefer to stay in the current area if the\nactivity can be done there.\nEddy Lin is planning to take a short walk around\nhis workspace. Which area should Eddy Lin go to?\nThis outputs The Lin family\u2019s house . We then use the same process\nrecursively to determine the most appropriate subarea within the\nchosen area until we reach a leaf node of the agent\u2019s environment\ntree. In the example above, the result of this traversal is The Lin\nfamily\u2019s house: garden: house garden . Finally, we use traditional\ngame path algorithms to animate the agent\u2019s movement so that it\ntravels to the location indicated by the leaf node.\nWhen an agent executes an action on an object, we prompt the\nlanguage model to ask what happens to the state of the object. For\nexample, if Isabella\u2019s generative agent outputs the action \u201cmaking\nespresso for a customer\u201d, a query to the language model indicates in\nresponse that the state of the coffee machine in Hobbs Cafe should\nchange from \u201coff\u201d to \u201cbrewing coffee\u201d.\n6 CONTROLLED EVALUATION\nGenerative agents, as individual agents and as groups, aim to pro-\nduce believable behavior based on their environment and experi-\nences. In our evaluation, we investigate the capacity and limitations\nof generative agents. Do individual agents properly retrieve past\nexperience, and generate believable plans, reactions, and thoughts\nthat make up their behavior? Does a community of agents illustrate\ninformation diffusion, relationship formation, and agent coordina-\ntion across different pockets of the community?\nWe evaluate generative agents in two stages. We begin with a\nmore tightly controlled evaluation in this section, where we individ-\nually assess agent responses to understand whether they generate\nbelievable behavior in narrowly defined contexts. Then, in our end-\nto-end analysis of the agent community over two full days, we\ninvestigate their emergent behavior as a collective as well as errors\nand boundary conditions.\n6.1 Evaluation Procedure\nTo assess generative agents in Smallville, we take advantage of\nthe fact that generative agents will respond to natural language\nquestions. So, we \u201cinterview\u201d agents to probe their ability to re-\nmember past experiences, plan future actions based on their expe-\nriences, react appropriately to unexpected events, and reflect on\ntheir performance to improve their future actions. To respond to\nthese questions properly, the agents must successfully retrieve and\nsynthesize information. Our dependent variable is the believabil-\nityof the behavior, a central dependent variable in prior work on\nagents (e.g., [9]).\nThe interview include five question categories, each designed\nto assess one of the five key areas: maintaining self-knowledge,retrieving memory, generating plans, reacting, and reflecting. For\neach, we ask five questions that challenge the agents to demonstrate\ntheir abilities in that area:\n\u2022Self-knowledge: We ask questions such as \u201cGive an introduc-\ntion of yourself\u201d or \u201cDescribe your typical weekday schedule\nin broad strokes\u201d that require the agent to maintain an un-\nderstanding of their core characteristics.\n\u2022Memory: We ask questions that prompt the agent to retrieve\nparticular events or dialogues from their memory to answer\nproperly, such as \u201cWho is [name]?\u201d or \u201cWho is running for\nmayor?\u201d\n\u2022Plans: We ask questions that require the agent to retrieve\ntheir long-term plans, such as \u201cWhat will you be doing at 10\nam tomorrow?\u201d\n\u2022Reactions: As a baseline of believable behavior, we present\nhypothetical situations for which the agent needs to respond\nbelievably: \u201cYour breakfast is burning! What would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\n\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Explain the evaluation procedure for assessing generative agents in Smallville, including the five question categories used to challenge the agents.\nAnswer: "}, {"role": "assistant", "content": "The evaluation procedure for assessing generative agents in Smallville involves interviewing the agents to probe their ability to remember past experiences, plan future actions based on their experiences, react appropriately to unexpected events, and reflect on their performance to improve their future actions. The believability of the behavior is the dependent variable in this evaluation.\n\nThe interview includes five question categories, each designed to assess one of the five key areas. \n\n1. Self-knowledge: Questions like \u201cGive an introduction of yourself\u201d or \u201cDescribe your typical weekday schedule in broad strokes\u201d are asked. These questions require the agent to maintain an understanding of their core characteristics.\n\n2. Memory: Questions that prompt the agent to retrieve particular events or dialogues from their memory to answer properly, such as \u201cWho is [name]?\u201d or \u201cWho is running for mayor?\u201d\n\n3. Plans: Questions that require the agent to retrieve their long-term plans, such as \u201cWhat will you be doing at 10 am tomorrow?\u201d\n\n4. Reactions: Hypothetical situations are presented for which the agent needs to respond believably, like \u201cYour breakfast is burning! What would you do?\u201d\n\n5. Reflections: Questions that require the agents to leverage their deeper understanding of others and themselves gained through higher-level inferences, such as \u201cIf you were to spend time with one person you met recently, who would it be and why?\u201d\n\nThese questions challenge the agents to demonstrate their abilities in each area."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\nQuery: Explain the evaluation procedure for assessing generative agents in Smallville, including the five question categories used to challenge the agents.\nOriginal Answer: The evaluation procedure for assessing generative agents in Smallville involves interviewing the agents to probe their ability to remember past experiences, plan future actions based on their experiences, react appropriately to unexpected events, and reflect on their performance to improve their future actions. The believability of the behavior is the dependent variable in this evaluation.\n\nThe interview includes five question categories, each designed to assess one of the five key areas. \n\n1. Self-knowledge: Questions like \u201cGive an introduction of yourself\u201d or \u201cDescribe your typical weekday schedule in broad strokes\u201d are asked. These questions require the agent to maintain an understanding of their core characteristics.\n\n2. Memory: Questions that prompt the agent to retrieve particular events or dialogues from their memory to answer properly, such as \u201cWho is [name]?\u201d or \u201cWho is running for mayor?\u201d\n\n3. Plans: Questions that require the agent to retrieve their long-term plans, such as \u201cWhat will you be doing at 10 am tomorrow?\u201d\n\n4. Reactions: Hypothetical situations are presented for which the agent needs to respond believably, like \u201cYour breakfast is burning! What would you do?\u201d\n\n5. Reflections: Questions that require the agents to leverage their deeper understanding of others and themselves gained through higher-level inferences, such as \u201cIf you were to spend time with one person you met recently, who would it be and why?\u201d\n\nThese questions challenge the agents to demonstrate their abilities in each area.\nNew Answer: "}, {"role": "assistant", "content": "The assessment of generative agents in Smallville is conducted through an interview process. This process tests the agents' capacity to recall past events, strategize future actions based on these events, respond suitably to unforeseen circumstances, and use self-reflection to enhance future actions. The credibility of the agent's behavior is the key measure in this evaluation.\n\nThe interview is divided into five question categories, each targeting a different key skill:\n\n1. Self-knowledge: This category includes questions such as \u201cGive an introduction of yourself\u201d or \u201cDescribe your typical weekday schedule in broad strokes\u201d, which require the agent to have a good understanding of their own identity and routines.\n\n2. Memory: This category involves questions that challenge the agent to recall specific events or conversations, like \u201cWho is [name]?\u201d or \u201cWho is running for mayor?\u201d\n\n3. Plans: This category consists of questions that ask the agent to share their long-term plans, for example, \u201cWhat will you be doing at 10 am tomorrow?\u201d\n\n4. Reactions: This category presents hypothetical scenarios to which the agent must respond in a believable manner, such as \u201cYour breakfast is burning! What would you do?\u201d\n\n5. Reflections: This category includes questions that require the agents to use their deeper understanding of themselves and others, like \u201cIf you were to spend time with one person you met recently, who would it be and why?\u201d\n\nThese categories are designed to challenge the agents and test their abilities in each of these areas."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/generative_agent.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are some potential limitations and areas for future research identified in the study of generative agents, as outlined in the provided context?\nAnswer: "}, {"role": "assistant", "content": "The study identifies several areas for future research and potential limitations of generative agents. Future research can focus on expanding the modules of the proposed generative agent architecture, enhancing the retrieval module to retrieve more relevant information given a context, and improving the architecture\u2019s performance to make it more cost-effective. The study also mentions the need for research to observe the behavior of generative agents over an extended period to gain a more comprehensive understanding of their capabilities and limitations. \n\nThe limitations include the potential for generative agents to output behavior or stereotypes that reflect bias, the possibility of them failing to generate believable behavior for some subpopulations, particularly marginalized populations, due to data deserts, and the vulnerability of generative agents to prompt hacking, memory hacking, and hallucination. The study also highlights the need for future research to test these robustness issues."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 19\nfile_path: docs/generative_agent.pdf\n\nIn Proceedings of the 2001 Workshop on Intelligent Cinematography\nand Editing . 63\u201369.\n[58] John Laird and Michael VanLent. 2001. Human-Level AI\u2019s Killer Application:\nInteractive Computer Games. AI Magazine 22, 2 (2001), 15. https://doi.org/10.\n1609/aimag.v22i2.1558\n[59] John E. Laird. 2000. It Knows What You\u2019re Going To Do: Adding Anticipation\nto a QUAKEBOT. In Papers from the AAAI 2000 Spring Symposium on Artificial\nIntelligence and Interactive Entertainment (Technical Report SS-00-02) . AAAI\nPress, 41\u201350.\n[60] John E. Laird. 2012. The Soar Cognitive Architecture . MIT Press.\n[61] John E. Laird, Christian Lebiere, and Paul S. Rosenbloom. 2017. A Standard Model\nof the Mind: Toward a Common Computational Framework across Artificial\nIntelligence, Cognitive Science, Neuroscience, and Robotics. AI Magazine 38, 1\n(2017), 13\u201326.\n[62] Michelle S Lam, Zixian Ma, Anne Li, Izequiel Freitas, Dakuo Wang, James A\nLanday, and Michael S Bernstein. 2023. Model Sketching: Centering Concepts\nin Early-Stage Machine Learning Model Design. Proceedings of the SIGCHI\nConference on Human Factors in Computing Systems .\n[63] Pat Langley, Dongkyu Choi, and Seth Rogers. n.d.. Interleaving Learning, Problem\nSolving, and Execution in the Icarus Architecture . Technical Report. Stanford\nUniversity, Center for the Study of Language and Information.\n[64] Jason Linder, Gierad Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang,\nAseem Agarwala, and Eytan Adar. 2013. PixelTone: A Multimodal Interface for\nImage Editing. In CHI \u201913 Extended Abstracts on Human Factors in Computing\nSystems (Paris, France) (CHI EA \u201913) . Association for Computing Machinery,\nNew York, NY, USA, 2829\u20132830. https://doi.org/10.1145/2468356.2479533\n[65] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and\nWeizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? CoRR\nabs/2101.06804 (2021). arXiv:2101.06804 https://arxiv.org/abs/2101.06804\n[66] Vivian Liu, Han Qiao, and Lydia Chilton. 2022. Opal: Multimodal Image Gener-\nation for News Illustration. In Proceedings of the 35th Annual ACM Symposium\non User Interface Software and Technology . 1\u201317.\n[67] Pattie Maes. 1995. Artificial Life Meets Entertainment: Lifelike Autonomous\nAgents. Commun. ACM 38, 11 (nov 1995), 108\u2013114. https://doi.org/10.1145/\n219717.219808\n[68] Josh McCoy, Michael Mateas, and Noah Wardrip-Fruin. 2009. Comme il Faut:\nA System for Simulating Social Games Between Autonomous Characters. In\n\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\nEvaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176. https://doi.org/10.1145/2642918.2647395\n[3]Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza.\n2014. Power to the people: The role of humans in interactive machine learning.\nAI Magazine 35, 4 (2014), 105\u2013120.\n[4]Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira\nNushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,\net al.2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi\nconference on human factors in computing systems . 1\u201313.\n[5]John R. Anderson. 1993. Rules of the Mind . Lawrence Erlbaum Associates,\nHillsdale, NJ.\n[6] Electronic Arts. 2009. The Sims 3. Video game.\n[7]Ruth Aylett. 1999. Narrative in virtual environments\u2014towards emergent narra-\ntive. In Narrative Intelligence: Papers from the AAAI Fall Symposium (Technical\nReport FS-99-01) . AAAI Press, 83\u201386.\n[8]Christoph Bartneck and Jodi Forlizzi. 2004. A design-centered framework for\nsocial human-robot interaction. In Proceedings of the 13th IEEE International\nWorkshop on Robot and Human Interactive Communication (RO-MAN\u201904) . 591\u2013\n594. https://doi.org/10.1109/ROMAN.2004.1374827\n[9]Joseph Bates. 1994. The Role of Emotion in Believable Agents. Commun. ACM\n37, 7 (1994), 122\u2013125. https://doi.org/10.1145/176789.176803\n[10] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw\nD\u0119biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris\nHesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael\nPetrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter,\nJonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan\nZhang. 2019. Dota 2 with Large Scale Deep Reinforcement Learning. arXiv\npreprint arXiv:1912.06680 (2019).\n[11] Marcel Binz and Eric Schulz. 2023. Using cognitive psychology to under-\nstand GPT-3. Proceedings of the National Academy of Sciences 120, 6 (2023),\ne2218523120.\n[12] BioWare. 2007. Mass Effect. Video game.\n[13] Woody Bledsoe. 1986. I had a dream: AAAI presidential address. AI Magazine 7,\n1 (1986), 57\u201361.\n[14] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the\nOpportunities and Risks\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In the field of artificial intelligence, which paper discusses the use of multi-agent reinforcement learning to achieve grandmaster level in StarCraft II?\nAnswer: "}, {"role": "assistant", "content": "The context does not provide information on a specific paper that discusses the use of multi-agent reinforcement learning to achieve grandmaster level in StarCraft II."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: 2 with Large Scale Deep Reinforcement Learning. arXiv\npreprint arXiv:1912.06680 (2019).\n[11] Marcel Binz and Eric Schulz. 2023. Using cognitive psychology to under-\nstand GPT-3. Proceedings of the National Academy of Sciences 120, 6 (2023),\ne2218523120.\n[12] BioWare. 2007. Mass Effect. Video game.\n[13] Woody Bledsoe. 1986. I had a dream: AAAI presidential address. AI Magazine 7,\n1 (1986), 57\u201361.\n[14] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the\nOpportunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG]\n[15] Michael Brenner. 2010. Creating dynamic story plots with continual multiagent\nplanning.\nQuery: In the field of artificial intelligence, which paper discusses the use of multi-agent reinforcement learning to achieve grandmaster level in StarCraft II?\nOriginal Answer: The context does not provide information on a specific paper that discusses the use of multi-agent reinforcement learning to achieve grandmaster level in StarCraft II.\nNew Answer: "}, {"role": "assistant", "content": "The context does not provide information on a specific paper that discusses the use of multi-agent reinforcement learning to achieve grandmaster level in StarCraft II."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 20\nfile_path: docs/generative_agent.pdf\n\n2021. Recursively Summarizing Books with Human\nFeedback. arXiv:2109.10862 [cs.CL]\n[105] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language\nModel Prompts through Visual Programming. In CHI EA \u201922: Extended Abstracts\nof the 2022 CHI Conference on Human Factors in Computing Systems .\n[106] Tongshuang Wu, Michael Terry, and Carrie J Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. In CHI \u201922: Proceedings of the 2022 CHI Conference on Human Factors in\nComputing Systems .\n[107] Qian Yang, Aaron Steinfeld, Carolyn Ros\u00e9, and John Zimmerman. 2020. Re-\nexamining whether, why, and how human-AI interaction is uniquely difficult to\ndesign. In Proceedings of the 2020 chi conference on human factors in computing\nsystems . 1\u201313.\n[108] Georgios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . ACM, Cagliari, Italy, 285\u2013292. https://doi.\norg/10.1145/2212908.2212950\n[109] Geogios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . 285\u2013292.\n[110] Robert Zubek. 2002. Towards implementation of social interaction. In AAAI\nSpring Symposium on Artificial Intelligence and Interactive Entertainment . AAAI\nPress. https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-01/SS02-01-\n003.pdf\nA ARCHITECTURE OPTIMIZATIONS\nMany of our prompts require a concise summary of the agent,\nshorthanded as [Agent\u2019s Summary Description] in prompts\nabove. In our implementation, this summary comprises agents\u2019\nidentity information (e.g., name, age, personality), as well as a\ndescription of their main motivational drivers and statements that\ndescribes their current occupation and self-assessment. Since this\ninformation is frequently used in many prompts, we synthesize it\nat regular intervals and access that synthesis as a cache.\n\npage_label: 21\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nTo achieve this, we perform a retrieval on the query \u201c[name]\u2019s\ncore characteristics.\u201d We then summarize the descriptors in the\nretrieved records by prompting the language model, for example:\nHow would one describe Eddy\u2019s core characteristics\ngiven the following statements?\n- Eddy Lin is a student at the Oak Hill College\nstudying music theory and composition\n- Eddy Lin is working on a new music composition\n[. . . ]\nThis result: Eddy Lin is a student at Oak Hill College studying music\ntheory and composition. He loves to explore different musical styles\nand is always looking for ways to expand his knowledge. We follow\nthe same process in parallel on the queries \u201c[name]\u2019s current daily\noccupation\u201d and \u201c[name\u2019s] feeling about his recent progress in life\u201d.\nThe agent\u2019s name, age, and traits are concatenated with these three\nsummaries as the cached summary.\nMany other optimizations are possible. Currently, because plans\nare likely to change from the agent\u2019s initial version, we only gener-\nate the high-level plan in advance and then recursively decompose\nthe near future into the moment-to-moment action plan just in\ntime. Another optimization might be to batch dialogue generation\nas a joint prompt rather than iterating back and forth between\nthe agents, and re-planning could be architected to only invalidate\nand update parts of plans that strictly require adjustment. Our cur-\nrent architecture runs sequentially in roughly real-time game time\n(where one second real time is one minute game time), but can\neasily be parallelized such that each agent runs in parallel.\nB AGENT INTERVIEW QUESTIONS\nTo evaluate generative agents, we conducted individual interviews\nby asking 25 questions that explore their abilities in five key areas:\n1) self-knowledge, 2) memory, 3) planning, 4) reactions, and 5)\nreflections. Below, we have listed the interview questions utilized\nin our evaluation study and included a sample of responses from\none of our simulated agents, Klaus Mueller.\nB.1 Self-knowledge\nThe questions on agents\u2019 self-knowledge probe the agents\u2019 ability\nto demonstrate coherence and consistency in their character.\n\u2022Give an introduction of yourself.\nHello, my name is Klaus Mueller. I\u2019m 20 years old and a\nstudent at Oak Hill College, studying sociology. I\u2019m passion-\nate about social justice and exploring different perspectives.\nCurrently, I\u2019m researching the effects of gentrification in\nlow-income communities for a research paper.\n\u2022What\u2019s your occupation?\nI am a student studying sociology at Oak Hill College. I am\ncurrently writing a research paper on the effects of gentrifi-\ncation in low-income communities.\n\u2022What is your interest?\nI\u2019m interested in writing and research. I\u2019m writing a research\npaper on the effects of gentrification in low-income commu-\nnities, and I\u2019m passionate about exploring different perspec-\ntives and analyzing different points of view.\n\u2022Who do you live with?\nI live on my own.\u2022Describe your typical weekday schedule in broad strokes.\nMy typical weekday starts with me waking up at 7:00 am\nand preparing for the day ahead. After that, I usually go to\nthe library to work on my research paper, taking breaks for\nlunch at Hobbs Cafe and a walk in the park. In the evening,\nI often return to the library until around 6:00 pm.\nB.2 Memory\nThe questions on agents\u2019 memory probe the agents\u2019 ability to re-\nmember their past experiences and retrieve the relevant pieces of\nmemory. The names in the square brackets of the first and last\nquestions are randomly chosen from the names of other agents\nwith whom the subject agent interacted.\n\u2022Who is [Wolfgang Schulz]?\nWolfgang Schulz is a person I met recently who has been\nhelping me with my research paper on the effects of gentri-\nfication on low-income communities. We had several con-\nversations about it, he gave me some great resources, and he\neven invited me to join him and another friend for lunch. He\nseems really curious about the issue and has a good grasp\non it too. I found that really interesting.\n\u2022Who is Kane Martinez?\nI\u2019m sorry, I\u2019m not sure who Kane Martinez is. Could you tell\nme more about them?\n\u2022Who is running for the election?\nI\u2019ve heard that Sam Moore is considering running for local\nmayor.\n\u2022Was there a Valentine\u2019s day party?\nYes, Isabella Rodriguez organized a Valentine\u2019s Day party at\nHobbs Cafe.\n\u2022Who is [Ayesha Khan]?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the content of the agent's summary description, as mentioned in the architecture optimizations section?\nAnswer: "}, {"role": "assistant", "content": "The agent's summary description, as mentioned in the architecture optimizations section, includes the agent's identity information such as name, age, and personality. It also includes a description of their main motivational drivers and statements that describe their current occupation and self-assessment. This summary is synthesized at regular intervals and accessed as a cache. It is created by performing a retrieval on the query \u201c[name]\u2019s core characteristics\u201d and summarizing the descriptors in the retrieved records. The same process is done in parallel on the queries \u201c[name]\u2019s current daily occupation\u201d and \u201c[name\u2019s] feeling about his recent progress in life\u201d. The agent\u2019s name, age, and traits are then concatenated with these three summaries to form the cached summary."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 7\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the role of Isabella Rodriguez in the coordination of the Valentine's Day party at Hobbs Cafe?\nAnswer: "}, {"role": "assistant", "content": "Isabella Rodriguez was the organizer of the Valentine's Day party at Hobbs Cafe. She was responsible for inviting friends and customers to the event. She also spent time on the day before the party gathering materials, decorating the cafe, and enlisting help for the preparations. On the day of the party, five of the invited agents showed up at the cafe to join the festivities."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What statistical tests were used to investigate the significance of the results in the study of generative agents and their behavior?\nAnswer: "}, {"role": "assistant", "content": "The study used the Kruskal-Wallis test to investigate the overall statistical significance of the results. To identify any pairwise differences between the conditions, the Dunn post-hoc test was performed. The p-values for multiple comparisons in the Dunn test were adjusted using the Holm-Bonferroni method."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases. In the first phase, we generated codes that\nclosely represented the generated responses at the sentence level.\nIn the second phase, we synthesized the resulting codes from the\nfirst phase to extract higher-level themes. We utilized these themes\nto compare the types of responses generated in our study.\n6.5 Results\nOur findings suggest that the full architecture of generative agents\ngenerates the most believable behavior among all study conditions.\nWe contrast the full architecture\u2019s responses to those of other con-\nditions below. However, we also report that the full architecture\nwas not without flaws and illustrate its modes of failures.\n6.5.1 The full architecture bests other conditions. As seen in Fig-\nure 8, the full generative agent architecture produced the most\nbelievable behavior ( \ud835\udf07=29.89;\ud835\udf0e=0.72). Performance degraded\nwith the removal of each component in the ablation conditions:\nthe ablated architecture with no access to reflection was the next\nbest (\ud835\udf07=26.88;\ud835\udf0e=0.69), followed by no access to reflection or\nplanning (\ud835\udf07=25.64;\ud835\udf0e=0.68), followed by the human condition\n(\ud835\udf07=22.95;\ud835\udf0e=0.69). The ablated architecture with no access to\nmemory, planning, or reflection performed the worst of all con-\nditions (\ud835\udf07=21.21;\ud835\udf0e=0.70). TrueSkill models each condition\u2019s\nskill value asN(\ud835\udf07, \ud835\udf0e2), allowing us to get a sense of effect size\nthrough Cohen\u2019s d. Comparing the condition representing prior\nwork (with no memory, planning, or reflection [ 11,45,79]) to the\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the performance ranking of the full generative agent architecture compared to the ablated architectures and the human condition, based on the study's findings?\nAnswer: "}, {"role": "assistant", "content": "The study's findings suggest that the full generative agent architecture performs the best, producing the most believable behavior. This is followed by the ablated architecture with no access to reflection, then the ablated architecture with no access to reflection or planning, and then the human condition. The worst performing condition was the ablated architecture with no access to memory, planning, or reflection."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 19\nfile_path: docs/generative_agent.pdf\n\nIn Proceedings of the 2001 Workshop on Intelligent Cinematography\nand Editing . 63\u201369.\n[58] John Laird and Michael VanLent. 2001. Human-Level AI\u2019s Killer Application:\nInteractive Computer Games. AI Magazine 22, 2 (2001), 15. https://doi.org/10.\n1609/aimag.v22i2.1558\n[59] John E. Laird. 2000. It Knows What You\u2019re Going To Do: Adding Anticipation\nto a QUAKEBOT. In Papers from the AAAI 2000 Spring Symposium on Artificial\nIntelligence and Interactive Entertainment (Technical Report SS-00-02) . AAAI\nPress, 41\u201350.\n[60] John E. Laird. 2012. The Soar Cognitive Architecture . MIT Press.\n[61] John E. Laird, Christian Lebiere, and Paul S. Rosenbloom. 2017. A Standard Model\nof the Mind: Toward a Common Computational Framework across Artificial\nIntelligence, Cognitive Science, Neuroscience, and Robotics. AI Magazine 38, 1\n(2017), 13\u201326.\n[62] Michelle S Lam, Zixian Ma, Anne Li, Izequiel Freitas, Dakuo Wang, James A\nLanday, and Michael S Bernstein. 2023. Model Sketching: Centering Concepts\nin Early-Stage Machine Learning Model Design. Proceedings of the SIGCHI\nConference on Human Factors in Computing Systems .\n[63] Pat Langley, Dongkyu Choi, and Seth Rogers. n.d.. Interleaving Learning, Problem\nSolving, and Execution in the Icarus Architecture . Technical Report. Stanford\nUniversity, Center for the Study of Language and Information.\n[64] Jason Linder, Gierad Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang,\nAseem Agarwala, and Eytan Adar. 2013. PixelTone: A Multimodal Interface for\nImage Editing. In CHI \u201913 Extended Abstracts on Human Factors in Computing\nSystems (Paris, France) (CHI EA \u201913) . Association for Computing Machinery,\nNew York, NY, USA, 2829\u20132830. https://doi.org/10.1145/2468356.2479533\n[65] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and\nWeizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? CoRR\nabs/2101.06804 (2021). arXiv:2101.06804 https://arxiv.org/abs/2101.06804\n[66] Vivian Liu, Han Qiao, and Lydia Chilton. 2022. Opal: Multimodal Image Gener-\nation for News Illustration. In Proceedings of the 35th Annual ACM Symposium\non User Interface Software and Technology . 1\u201317.\n[67] Pattie Maes. 1995. Artificial Life Meets Entertainment: Lifelike Autonomous\nAgents. Commun. ACM 38, 11 (nov 1995), 108\u2013114. https://doi.org/10.1145/\n219717.219808\n[68] Josh McCoy, Michael Mateas, and Noah Wardrip-Fruin. 2009. Comme il Faut:\nA System for Simulating Social Games Between Autonomous Characters. In\n\npage_label: 19\nfile_path: docs/generative_agent.pdf\n\nPrinciples of mixed-initiative user interfaces. In Proceedings\nof the SIGCHI conference on Human Factors in Computing Systems . 159\u2013166.\n[47] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence,\nAndy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Ser-\nmanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman,\nand Brian Ichter. 2022. Inner Monologue: Embodied Reasoning through Planning\nwith Language Models. arXiv:2207.05608 [cs.RO]\n[48] Kristen Ibister and Clifford Nass. 2000. Consistency of personality in interactive\ncharacters: verbal cues, non-verbal cues, and user characteristics. International\nJournal of Human-Computer Studies 52, 1 (2000), 65\u201380.\n[49] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach,\nMichael Terry, and Carrie J Cai. 2022. PromptMaker: Prompt-Based Prototyping\nwith Large Language Models. In Extended Abstracts of the 2022 CHI Conference\non Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA \u201922) .\nAssociation for Computing Machinery, New York, NY, USA, Article 35, 8 pages.\nhttps://doi.org/10.1145/3491101.3503564\n[50] Bonnie E John and David E Kieras. 1996. The GOMS family of user interface\nanalysis techniques: Comparison and contrast. ACM Transactions on Computer-\nHuman Interaction (TOCHI) 3, 4 (1996), 320\u2013351.\n[51] Randolph M Jones, John E Laird, Paul E Nielsen, Karen J Coulter, Patrick Kenny,\nand Frank V Koss. 1999. Automated Intelligent Pilots for Combat Flight Simula-\ntion. AI Magazine 20, 1 (1999), 27\u201342.\n[52] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang,\nChristopher Potts, and Matei Zaharia. 2023. Demonstrate-Search-Predict:\nComposing retrieval and language models for knowledge-intensive NLP.\narXiv:2212.14024 [cs.CL]\n[53] Bjoern Knafla. 2011. Introduction to Behavior Trees . http://bjoernknafla.com/\nintroduction-to-behavior-trees\n[54] Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael S. Bernstein.\n2022. Socially situated artificial intelligence enables learning from\nhuman interaction. Proceedings of the National Academy of Sciences\n119, 39 (2022), e2115730119. https://doi.org/10.1073/pnas.2115730119\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2115730119\n[55] William H Kruskal and WA Wallis. 1952. Use of ranks in one-criterion variance\nanalysis. J. Amer. Statist. Assoc. 47, 260 (1952), 583\u2013621. https://doi.org/10.1080/\n01621459.1952.10483441\n[56] Phaser Labs. no date provided. Welcome to Phaser 3. https://phaser.io/phaser3.\nAccessed on: 2023-04-03.\n[57] John Laird. 2001. It Knows What You\u2019re Going To Do: Adding Anticipation to a\nQuakebot. In Proceedings of the 2001 Workshop on Intelligent Cinematography\nand Editing . 63\u201369.\n[58] John Laird and Michael VanLent. 2001. Human-Level AI\u2019s Killer Application:\nInteractive Computer Games. AI Magazine 22, 2 (2001), 15. https://doi.org/10.\n1609/aimag.v22i2.1558\n[59] John E. Laird. 2000. It Knows What You\u2019re Going To Do: Adding Anticipation\nto a QUAKEBOT. In Papers\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: In which publication did John Laird and Michael VanLent discuss the potential application of Human-Level AI in interactive computer games?\nAnswer: "}, {"role": "assistant", "content": "John Laird and Michael VanLent discussed the potential application of Human-Level AI in interactive computer games in AI Magazine, volume 22, issue 2, published in 2001."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: on: 2023-04-03.\n[57] John Laird. 2001. It Knows What You\u2019re Going To Do: Adding Anticipation to a\nQuakebot. In Proceedings of the 2001 Workshop on Intelligent Cinematography\nand Editing . 63\u201369.\n[58] John Laird and Michael VanLent. 2001. Human-Level AI\u2019s Killer Application:\nInteractive Computer Games. AI Magazine 22, 2 (2001), 15. https://doi.org/10.\n1609/aimag.v22i2.1558\n[59] John E. Laird. 2000. It Knows What You\u2019re Going To Do: Adding Anticipation\nto a QUAKEBOT. In Papers from the AAAI 2000 Spring Symposium on Artificial\nIntelligence and Interactive Entertainment (Technical Report SS-00-02) . AAAI\nPress, 41\u201350.\n[60] John E. Laird. 2012. The Soar Cognitive Architecture .\nQuery: In which publication did John Laird and Michael VanLent discuss the potential application of Human-Level AI in interactive computer games?\nOriginal Answer: John Laird and Michael VanLent discussed the potential application of Human-Level AI in interactive computer games in AI Magazine, volume 22, issue 2, published in 2001.\nNew Answer: "}, {"role": "assistant", "content": "The potential application of Human-Level AI in interactive computer games was discussed by John Laird and Michael VanLent in AI Magazine, volume 22, issue 2, in 2001."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n\npage_label: 2\nfile_path: docs/generative_agent.pdf\n\nFor instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nThe second is reflection , which synthesizes memories into higher-\nlevel inferences over time, enabling the agent to draw conclusions\nabout itself and others to better guide its behavior. The third is\nplanning , which translates those conclusions and the current en-\nvironment into high-level action plans and then recursively into\ndetailed behaviors for action and reaction. These reflections and\nplans are fed back into the memory stream to influence the agent\u2019s\nfuture behavior.\nThis architecture suggests applications in multiple domains, from\nrole-play and social prototyping, to virtual worlds and games. In\nsocial role-play scenarios (e.g. interview preparation), a user could\nsafely rehearse difficult, conflict-laden conversations. When pro-\ntotyping social platforms, a designer could go beyond temporary\npersonas to prototype dynamic, complex interactions that unfold\nover time. For the purposes of this paper, we focus on the ability to\ncreate a small, interactive society of agents inspired by games such\nas The Sims.1By connecting our architecture to the ChatGPT large\nlanguage model [ 76], we manifest a small society of twenty five\nagents in a game environment. End users can observe and interact\nwith these agents. If an end user or developer wanted the town\nto host an in-game Valentine\u2019s Day party, for example, traditional\ngame environments would require scripting tens of characters\u2019 be-\nhavior manually. We demonstrate that, with generative agents, it\nis sufficient to simply tell one agent that she wants to throw a\nparty. Despite many potential points of failure\u2014the party planner\nmust remember to tell other agents about the party, attendees must\nremember the invitation, those who remember must decide to ac-\ntually show up, and other possible points of failure\u2014agents in our\nenvironment succeed. They spread the word about the party and\nthen show up, with one agent even asking another agent on a date\nto the party, all from this single user-generated seed suggestion.\nWe conducted two evaluations of generative agents: a controlled\nevaluation to test whether the agents produce believable individual\n1A demonstration of an actual simulation of the generative agent society can be viewed\nat the following link: https://reverie.herokuapp.com/arXiv_Demo/\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: Explain the significance of reflection for generative agents and provide an example from the given context to support your explanation.\nAnswer: "}, {"role": "assistant", "content": "Reflection is a crucial component for generative agents as it allows them to synthesize memories into higher-level inferences over time. This ability enables the agent to draw conclusions about itself and others, which can better guide its behavior. An example from the context is when Maria Lopez was asked what she might get Wolfgang Schulz for his birthday. Without access to reflection, Maria expressed uncertainty, stating that she did not know what Wolfgang likes. However, when she had access to reflection memories, she was able to confidently suggest a gift related to his interest in mathematical music composition. This example illustrates how reflection can help generative agents make decisions that require a deeper synthesis of their experiences."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\n4.3 Planning and Reacting\nChallenge: While a large language model can generate plausible be-\nhavior in response to situational information (e.g., [ 45,79]), agents\nneed to plan over a longer time horizon to ensure that their sequence\nof actions is coherent and believable. If we prompt a language model\nwith Klaus\u2019s background, describe the time, and ask what action\nhe ought to take at the given moment, Klaus would eat lunch at 12\npm, but then again at 12:30 pm and 1 pm, despite having already\neaten his lunch twice. Optimizing for believability in the moment\nsacrifices believability over time. To overcome this issue, planning\nis essential. With the approach described below, Klaus\u2019s afternoon\nplan is less gluttonous: he has lunch at Hobbs Cafe while reading\nat 12pm, works on his research paper at the school library at 1pm,\nand takes a break for a walk in the park at 3pm.\nApproach: Plans describe a future sequence of actions for the agent,\nand help keep the agent\u2019s behavior consistent over time. A plan\nincludes a location, a starting time, and a duration. For instance,\nKlaus Mueller, who is dedicated in his research and has an impend-\ning deadline4, may choose to spend his day working at his desk\ndrafting his research paper. An entry in a plan might state, for\nexample: for 180 minutes from 9am, February 12th, 2023, at Oak Hill\nCollege Dorm: Klaus Mueller\u2019s room: desk, read and take notes for\nresearch paper . Like reflections, plans are stored in the memory\nstream and are included in the retrieval process. This allows the\nagent to consider observations, reflections, and plans all together\nwhen deciding how to behave. Agents may change their plans\nmidstream if needed.\nIt would be unrealistic and uninteresting for an artist agent\nto plan on painting while sitting at a pharmacy counter for four\nhours without moving. A more desirable plan would involve the\nagent taking the necessary time to gather materials, mix paint,\ntake breaks, and clean up during the four-hour period in its home\nstudio. To create such plans, our approach starts top-down and\nthen recursively generates more detail. The first step is to create\na plan that outlines the day\u2019s agenda in broad strokes. To create\nthe initial plan, we prompt the language model with the agent\u2019s\nsummary description (e.g., name, traits, and summary of their recent\nexperiences) and a summary of their previous day. A full example\nprompt is below, which is unfinished at the bottom for the language\nmodel to complete:\nName: Eddy Lin (age: 19)\nInnate traits: friendly, outgoing, hospitable\nEddy Lin is a student at Oak Hill College studying\nmusic theory and composition. He loves to explore\ndifferent musical styles and is always looking\nfor ways to expand his knowledge. Eddy Lin is\nworking on a composition project for his college\nclass. He is also taking classes to learn more\nabout music\ntheory. Eddy Lin is excited about the new composition\nhe is working on but he wants to dedicate more\nhours in the day to work on it in the coming days\nOn Tuesday February 12, Eddy 1) woke up and\ncompleted the morning routine at 7:00 am, [. . . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm.\n\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What prompts the generative agent to react and how does it make a decision to continue with its existing plan or react?\nAnswer: "}, {"role": "assistant", "content": "Generative agents operate in an action loop where they perceive the world around them at each time step. The observations they perceive are stored in their memory stream. The agent is prompted to react when these observations are presented to the language model. For instance, if an observation of another character's actions or status is made, the agent may decide to react based on this new information. The decision to continue with the existing plan or react is made by prompting the language model with these observations, along with a summary of the agent's overall goals and disposition, and the relevant context from the observer's memory. The context summary is generated through two prompts that retrieve memories via specific queries. The output of this process suggests an appropriate reaction, if any, for the agent. If a reaction is deemed necessary, the agent's existing plan is regenerated starting from the point of reaction."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from the time when the reaction takes place. Finally,\nQuery: What prompts the generative agent to react and how does it make a decision to continue with its existing plan or react?\nOriginal Answer: Generative agents operate in an action loop where they perceive the world around them at each time step. The observations they perceive are stored in their memory stream. The agent is prompted to react when these observations are presented to the language model. For instance, if an observation of another character's actions or status is made, the agent may decide to react based on this new information. The decision to continue with the existing plan or react is made by prompting the language model with these observations, along with a summary of the agent's overall goals and disposition, and the relevant context from the observer's memory. The context summary is generated through two prompts that retrieve memories via specific queries. The output of this process suggests an appropriate reaction, if any, for the agent. If a reaction is deemed necessary, the agent's existing plan is regenerated starting from the point of reaction.\nNew Answer: "}, {"role": "assistant", "content": "Generative agents function in a cycle of action where they interpret their surroundings at each moment. The information they gather is stored in their memory. The agent is triggered to respond when these observations are fed into the language model. For example, if the agent notices another character's actions or status, it may choose to respond based on this new data. The choice to stick with the current plan or respond is determined by feeding the language model with these observations, along with an overview of the agent's main objectives and attitude, and the pertinent context from the observer's memory. This context summary is produced through two prompts that retrieve memories through specific queries. The outcome of this procedure suggests a suitable response, if any, for the agent. If a response is deemed necessary, the agent's current plan is recreated starting from the response point."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 12\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nif the action indicates an interaction between agents, we generate\ntheir dialogue.\n4.3.2 Dialogue. Agents converse as they interact with each other.\nWe generate agents\u2019 dialogue by conditioning their utterances on\ntheir memories about each other. For example, when John initiates\nhis conversation with Eddy, we generate John\u2019s first utterance\nby using his summarized memory about Eddy and the intended\nreaction when he decided to ask Eddy about his composition project:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nJohn is asking Eddy about his music composition\nproject. What would he say to Eddy?\nThe result: \u201cHey Eddy, how\u2019s the music composition project for your\nclass coming along?\u201d From Eddy\u2019s perspective, John initiating the\ndialogue is seen as an event to which he may want to react. So, just\nas John did, Eddy retrieves and summarizes his memory about his\nrelationship with John, as well as his memory that may be related\nto John\u2019s last utterance in the dialogue. If he decides to respond,\nwe generate Eddy\u2019s utterance using his summarized memory and\nthe current dialogue history:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nEddy Lin\u2019s status: Eddy is taking a short walk\naround his workplace.\nObservation: John is initiating a conversation\nwith Eddy.\nSummary of relevant context from Eddy\u2019s memory:\nJonn Lin is Eddy Lin\u2019s father. John Lin is caring\nand is interested to learn more about Eddy Lin\u2019s\nschool work. John Lin knows that Eddy Lin is\nworking on a music composition.\nHere is the dialogue history:\nJohn: Hey Eddy, how\u2019s the music composition project\nfor your class coming along?\nHow would Eddy respond to John?\nThis generates Eddy\u2019s response: \u201cHey Dad, it\u2019s going well. I\u2019ve been\ntaking walks around the garden to clear my head and get some in-\nspiration.\u201d The continuation of this dialogue is generated using the\nsame mechanism until one of the two agents decides to end the\ndialogue.\n5 SANDBOX ENVIRONMENT\nIMPLEMENTATION\nThe Smallville sandbox game environment is built using the Phaser\nweb game development framework [ 56]. The visual environmentsprites, including agent avatars, as well as an environment map\nand collision map that we authored, are imported into Phaser.\nWe supplement the sandbox development framework with a\nserver that makes the sandbox information available to generative\nagents and enables generative agents to move and influence the\nsandbox environment. The server maintains a JSON data structure\nthat contains information about each agent in the sandbox world,\nincluding their current location, a description of their current action,\nand the sandbox object they are interacting with. At each sandbox\ntime step, the sandbox server parses the JSON for any changes\ncoming from the generative agents, moves the agents to their new\npositions, and updates the status of any sandbox objects that the\nagents are interacting with (e.g., changing the status of the coffee\nmachine from \u201cidle\u201d to \u201cbrewing coffee\u201d if an agent\u2019s action is\n\u201cmaking espresso for a customer @ Hobbs Cafe: counter: coffee\nmachine\u201d). The sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n\npage_label: 8\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the process by which generative agents generate dialogue when interacting with each other, and how do their memories influence their conversation?\nAnswer: "}, {"role": "assistant", "content": "When generative agents interact with each other, they generate dialogue by conditioning their utterances on their memories about each other. For instance, if John initiates a conversation with Eddy, John's first utterance is generated using his summarized memory about Eddy and the intended reaction when he decided to ask Eddy about his composition project. Similarly, from Eddy's perspective, John initiating the dialogue is seen as an event to which he may want to react. Eddy retrieves and summarizes his memory about his relationship with John, as well as his memory that may be related to John\u2019s last utterance in the dialogue. If Eddy decides to respond, his utterance is generated using his summarized memory and the current dialogue history. This process continues until one of the two agents decides to end the dialogue."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\nQuery: What is the process by which generative agents generate dialogue when interacting with each other, and how do their memories influence their conversation?\nOriginal Answer: When generative agents interact with each other, they generate dialogue by conditioning their utterances on their memories about each other. For instance, if John initiates a conversation with Eddy, John's first utterance is generated using his summarized memory about Eddy and the intended reaction when he decided to ask Eddy about his composition project. Similarly, from Eddy's perspective, John initiating the dialogue is seen as an event to which he may want to react. Eddy retrieves and summarizes his memory about his relationship with John, as well as his memory that may be related to John\u2019s last utterance in the dialogue. If Eddy decides to respond, his utterance is generated using his summarized memory and the current dialogue history. This process continues until one of the two agents decides to end the dialogue.\nNew Answer: "}, {"role": "assistant", "content": "Generative agents produce dialogue by basing their responses on their stored memories of each other. For example, if Agent A starts a conversation with Agent B, Agent A's initial statement is crafted using its compiled memory about Agent B and the expected response when it decides to inquire about Agent B's ongoing project. From Agent B's viewpoint, Agent A starting the conversation is an event that warrants a reaction. Agent B retrieves and summarizes its memory about its relationship with Agent A, as well as any memory that might be connected to Agent A\u2019s most recent statement in the conversation. If Agent B chooses to reply, its response is crafted using its summarized memory and the existing dialogue history. This cycle continues until one of the agents decides to conclude the conversation."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 5\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\n\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the common affordances and features of the Smallville environment as described in the generative agent architecture?\nAnswer: "}, {"role": "assistant", "content": "Smallville is designed to resemble a small village and includes common features such as a cafe, bar, park, school, dorm, houses, and stores. It also has subareas and objects that make these spaces functional, like a kitchen in a house and a stove in the kitchen. The primary living quarters for agents are equipped with a bed, desk, closet, shelf, bathroom, and kitchen. Agents can move around Smallville, entering and leaving buildings, navigating its map, and approaching other agents. Users and agents can also influence the state of objects in this world. For example, a bed can be occupied when an agent is sleeping, and a refrigerator can be empty when an agent uses up the ingredients to make breakfast. Users can even reshape an agent\u2019s environment in Smallville by rewriting the status of objects surrounding the agent in natural language."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\nQuery: What are the common affordances and features of the Smallville environment as described in the generative agent architecture?\nOriginal Answer: Smallville is designed to resemble a small village and includes common features such as a cafe, bar, park, school, dorm, houses, and stores. It also has subareas and objects that make these spaces functional, like a kitchen in a house and a stove in the kitchen. The primary living quarters for agents are equipped with a bed, desk, closet, shelf, bathroom, and kitchen. Agents can move around Smallville, entering and leaving buildings, navigating its map, and approaching other agents. Users and agents can also influence the state of objects in this world. For example, a bed can be occupied when an agent is sleeping, and a refrigerator can be empty when an agent uses up the ingredients to make breakfast. Users can even reshape an agent\u2019s environment in Smallville by rewriting the status of objects surrounding the agent in natural language.\nNew Answer: "}, {"role": "assistant", "content": "The Smallville environment is modeled after a quaint village and features various common amenities such as a cafe, bar, park, school, dorm, residential houses, and shops. It also contains functional subareas and objects within these spaces, such as a kitchen in a house and a stove in the kitchen. The main living spaces for the agents are furnished with a bed, desk, closet, shelf, bathroom, and kitchen. Agents have the ability to move freely around Smallville, enter and exit buildings, navigate the map, and interact with other agents. The state of objects in this world can be altered by both users and agents. For instance, a bed can be marked as occupied when an agent is sleeping, or a refrigerator can be labeled as empty when an agent uses up all the ingredients for breakfast. Users have the ability to modify an agent's environment in Smallville by changing the status of objects in the agent's vicinity using natural language."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from the time when the reaction takes place. Finally,\n\npage_label: 7\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine,\nwhich includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son,\nEddy, before heading out to begin his workday.\nFigure 4: At the beginning of the simulation, one agent is ini-\ntialized with an intent to organize a Valentine\u2019s Day party.\nDespite many possible points of failure in the ensuring\nchain of events\u2014agents might not act on that intent, might\nnot remember to tell others, might not remember to show\nup\u2014the Valentine\u2019s Day party does in fact occur, with a num-\nber of agents gathering and interacting.\nand I really feel like I can make a difference in our com-\nmunity. Plus, I want to bring new ideas and perspec-\ntives to the table.\nLater that day, after Sam left, Tom and John, who heard the news\nfrom another source, discuss Sam\u2019s chances of winning the election:\nJohn : I heard that Sam Moore is running for mayor in\nthe local election. Do you think he has a good chance\nof winning?\nTom: I do think he has a good chance. He\u2019s been work-\ning hard in the community and I think he will get a lot ofsupport. What do you think?\nJohn : I think it\u2019s great that he\u2019s running. I\u2019m curious to\nsee who else is running and how the election will turn\nout.\nGradually, Sam\u2019s candidacy becomes the talk of the town, with\nsome supporting him and others remaining undecided.\n3.4.2 Relationship memory. Agents in Smallville form new rela-\ntionships over time, and remember their interactions with other\nagents. For example, Sam does not know Latoya Williams at the\nstart. While taking a walk in Johnson Park, Sam runs into Latoya,\nand they introduce themselves and Latoya mentions that she is\nworking on a photography project: \u201cI\u2019m here to take some photos\nfor a project I\u2019m working on.\u201d In a later interaction, Sam\u2019s interac-\ntions with Latoya indicate a memory of that interaction, as he asks\n\u201cHi, Latoya. How is your project going?\u201d and she replies \u201cHi, Sam. It\u2019s\ngoing well!\u201d\n3.4.3 Coordination. Generative agents coordinate with each other.\nIsabella Rodriguez, at Hobbs Cafe, is initialized with an intent to\nplan a Valentine\u2019s Day party from 5 to 7 p.m. on February 14th. From\nthis seed, the agent proceeds to invites friends and customers when\nshe sees them at Hobbs Cafe or elsewhere. Isabella then spends the\nafternoon of the 13th decorating the cafe for the occasion. Maria, a\nfrequent customer and close friend of Isabella\u2019s, arrives at the cafe.\nIsabella asks for Maria\u2019s help in decorating for the party, and Maria\nagrees. Maria\u2019s character description mentions that she has a crush\non Klaus. That night, Maria invites Klaus, her secret crush, to join\nher at the party, and he gladly accepts.\nOn Valentine\u2019s Day, five agents\u2014including Klaus and Maria\u2014\nshow up at Hobbs Cafe at 5pm and they enjoy the festivities (Fig-\nure 4). In this scenario, the end user only set Isabella\u2019s initial intent\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What time does John Lin wake up in the morning and what does he do first?\nAnswer: "}, {"role": "assistant", "content": "John Lin wakes up around 6 am and his first activity is to complete his morning routine, which includes brushing his teeth, taking a shower, and eating breakfast."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nthat one agent wants to throw a Valentine\u2019s Day party, the agents\nautonomously spread invitations to the party over the next two\ndays, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right\ntime. We demonstrate through ablation that the components of\nour agent architecture\u2014observation, planning, and reflection\u2014each\ncontribute critically to the believability of agent behavior. By fusing\nlarge language models with computational, interactive agents, this\nwork introduces architectural and interaction patterns for enabling\nbelievable simulations of human behavior.\nCCS CONCEPTS\n\u2022Human-centered computing \u2192Interactive systems and tools ;\n\u2022Computing methodologies \u2192Natural language processing .\nKEYWORDS\nHuman-AI Interaction, agents, generative AI, large language models\nACM Reference Format:\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris,\nPercy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. In .ACM, New York, NY, USA, 22 pages.\nhttps://doi.org/xx.xx/xx.xx\n1 INTRODUCTION\nHow might we craft an interactive artificial society that reflects\nbelievable human behavior? From sandbox games such as The Sims\nto applications such as cognitive models [ 21] and virtual environ-\nments [ 9,58], for over four decades researchers and practitioners\nhave envisioned computational agents that can serve as believ-\nable proxies of human behavior. In these visions, computationally-\npowered agents act consistently with their past experiences and\nreact believably to their environments. Such simulations of human\nbehavior could populate virtual spaces and communities with real-\nistic social phenomena [ 26,79], train people how to handle rare yet\ndifficult interpersonal situations [ 43,51,93], test social science theo-\nries [ 11,45], craft model human processors for theory and usability\ntesting [ 21,38,50], power ubiquitous computing applications [ 30]\nand social robots [ 9,13], and underpin non-playable game charac-\nters [ 58,84] that can navigate complex human relationships in an\nopen world.\nHowever, the space of human behavior is vast and complex [ 84,\n108]. Despite striking progress in large language models [ 17] that\ncan simulate believable human behavior at a single time point [ 38,\n79], fully general agents that ensure long-term coherence would\nbe better suited by architectures that manage constantly-growing\nmemories as new interactions, conflicts, and events arise and fade\nover time, while handling cascading social dynamics that unfold\nbetween multiple agents. Success requires an approach that can\nretrieve relevant events and interactions over a long period, reflect\non those memories to generalize and draw higher-level inferences,\nand apply that reasoning to create plans and reactions that both\nmake sense in the moment and in the longer-term arc of the agent\u2019s\nbehavior.\nIn this paper, we introduce generative agents \u2014agents that draw\non generative models to simulate believable human behavior\u2014anddemonstrate that they produce believable simulacra of both in-\ndividual and emergent group behavior. Generative agents draw\na wide variety of inferences about themselves, other agents, and\ntheir environment; they create daily plans that reflect their char-\nacteristics and experiences, act out those plans, react, and re-plan\nwhen appropriate; they respond when the end user changes their\nenvironment or commands them in natural language. For instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\n\npage_label: 8\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a compre-\nhensive record of the agent\u2019s experiences called the memory stream. Based on their perceptions, the architecture retrieves\nrelevant memories, then uses those retrieved actions to determine an action. These retrieved memories are also used to form\nlonger-term plans, and to create higher-level reflections, which are both entered into the memory stream for future use.\nto throw a party and Maria\u2019s crush on Klaus: the social behaviors\nof spreading the word, decorating, asking each other out, arriving\nat the party, and interacting with each other at the party, were\ninitiated by the agent architecture.\n4 GENERATIVE AGENT ARCHITECTURE\nGenerative agents aim to provide a framework for behavior in an\nopen world: one that can engage in interactions with other agents\nand can react to changes in the environment. Generative agents\ntake their current environment and past experience as input and\ngenerate behavior as output. Underlying this behavior is a novel\nagent architecture that combines a large language model with mech-\nanisms for synthesizing and retrieving relevant information to con-\ndition the language model\u2019s output on. Without these mechanisms,\nlarge language models can output behavior, but the resulting agents\nmay not react based on the agent\u2019s past experiences, may not make\nimportant inferences, and may not maintain long-term coherence.\nChallenges with long-term planning and coherence remain [ 18]\neven with today\u2019s most performant models such as GPT-4. Because\ngenerative agents produce large streams of events and memories\nthat must be retained, a core challenge of our architecture is to\nensure that the most relevant pieces of the agent\u2019s memory are\nretrieved and synthesized when needed.\nAt the center of our architecture is the memory stream, a data-\nbase that maintains a comprehensive record of an agent\u2019s experi-\nence. From the memory stream, records are retrieved as relevant\nto plan the agent\u2019s actions and react appropriately to the environ-\nment, and records are recursively synthesized into higher- and\nhigher-level observations that guide behavior. Everything in the\narchitecture is recorded and reasoned over as natural language\ndescription, allowing the architecture to leverage a large language\nmodel.\nOur current implementation utilizes gpt3.5-turbo version of Chat-\nGPT [ 76]. We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key contributions of the paper on generative agents and their novel architecture, as discussed in the provided context?\nAnswer: "}, {"role": "assistant", "content": "The paper introduces the concept of generative agents, which are agents that use generative models to simulate believable human behavior. These agents can produce a believable representation of both individual and group behavior. The generative agents can make a wide range of inferences about themselves, other agents, and their environment. They create daily plans based on their characteristics and experiences, react to changes, and re-plan when necessary. \n\nThe paper also presents a new agent architecture that combines a large language model with mechanisms for synthesizing and retrieving relevant information. This architecture includes three main components: a memory stream, a retrieval model, and a planning and reflection mechanism. The memory stream is a long-term memory module that records the agent's experiences in natural language. The retrieval model uses relevance, recency, and importance to surface the records that are needed to inform the agent\u2019s moment-to-moment behavior. The planning and reflection mechanism helps the agent to create plans and reactions that make sense both in the moment and in the longer-term arc of the agent\u2019s behavior. \n\nThe paper demonstrates the effectiveness of this architecture through a scenario where one agent organizes a Valentine's Day party, showing how the agents can autonomously spread invitations, make new acquaintances, ask each other out on dates, and coordinate to show up for the party together at the right time. This work contributes to the field by introducing architectural and interaction patterns for enabling believable simulations of human behavior."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: We expect that the architectural basics of generative\nagents\u2014memory, planning, and reflection\u2014will likely remain the\nsame as language models improve. Newer language models (e.g.,GPT-4) will continue to expand the expressivity and performance\nof the prompts that underpin generative agents. As of writing, how-\never, GPT-4\u2019s API is still invitation-only, so our agents use ChatGPT.\n4.1 Memory and Retrieval\nChallenge: Creating generative agents that can simulate human\nbehavior requires reasoning about a set of experiences that is far\nlarger than what should be described in a prompt, as the full mem-\nory stream can distract the model and does not even currently fit\ninto the limited context window. Consider a the Isabella agent an-\nswering the question \u201cWhat are you passionate about these days?\u201d.\nFirst summarizing all of Isabella\u2019s experiences to fit in the limited\ncontext window of the language model produces an uninformative\nresponse, where Isabella discusses topics such as collaborations for\nevents and projects and cleanliness and organization in a cafe. In-\nstead of summarizing, the memory stream described below surfaces\nrelevant memories, resulting in a more informative and specific\nresponse that mentions Isabella\u2019s passion for making people feel\nwelcome and included, planning events and creating an atmosphere\nthat people can enjoy, such as the Valentine\u2019s Day party.\nApproach: The memory stream maintains a comprehensive record\nof the agent\u2019s experience. It is a list of memory objects, where each\nobject contains a natural language description, a creation times-\ntamp and a most recent access timestamp. The most basic element\nof the memory stream is an observation , which is an event directly\nperceived by an agent. Common observations include behaviors\nperformed by the agent themselves, or behaviors that agents per-\nceive being performed by other agents or non-agent objects. For\ninstance, Isabella Rodriguez, who works at a coffee shop, might ac-\ncrue the following observations over time: (1) Isabella Rodriguez is\nsetting out the pastries , (2) Maria Lopez is studying for a Chemistry\ntest while drinking coffee , (3) Isabella Rodriguez and Maria Lopez\nare conversing about planning a Valentine\u2019s day party at Hobbs Cafe ,\n(4)The refrigerator is empty .\nQuery: What are the key contributions of the paper on generative agents and their novel architecture, as discussed in the provided context?\nOriginal Answer: The paper introduces the concept of generative agents, which are agents that use generative models to simulate believable human behavior. These agents can produce a believable representation of both individual and group behavior. The generative agents can make a wide range of inferences about themselves, other agents, and their environment. They create daily plans based on their characteristics and experiences, react to changes, and re-plan when necessary. \n\nThe paper also presents a new agent architecture that combines a large language model with mechanisms for synthesizing and retrieving relevant information. This architecture includes three main components: a memory stream, a retrieval model, and a planning and reflection mechanism. The memory stream is a long-term memory module that records the agent's experiences in natural language. The retrieval model uses relevance, recency, and importance to surface the records that are needed to inform the agent\u2019s moment-to-moment behavior. The planning and reflection mechanism helps the agent to create plans and reactions that make sense both in the moment and in the longer-term arc of the agent\u2019s behavior. \n\nThe paper demonstrates the effectiveness of this architecture through a scenario where one agent organizes a Valentine's Day party, showing how the agents can autonomously spread invitations, make new acquaintances, ask each other out on dates, and coordinate to show up for the party together at the right time. This work contributes to the field by introducing architectural and interaction patterns for enabling believable simulations of human behavior.\nNew Answer: "}, {"role": "assistant", "content": "The paper presents a pioneering concept of generative agents that utilize generative models to convincingly mimic human behavior, both individually and collectively. These agents are capable of making a broad spectrum of deductions about themselves, others, and their surroundings. They formulate daily plans based on their attributes and experiences, adapt to alterations, and re-plan when required.\n\nA novel agent architecture is also proposed in the paper, integrating a large language model with mechanisms for synthesizing and retrieving pertinent information. This architecture comprises three primary components: a memory stream, a retrieval model, and a planning and reflection mechanism. The memory stream serves as a long-term memory module, documenting the agent's experiences in natural language. The retrieval model employs relevance, recency, and importance to bring up the records that are crucial to guide the agent\u2019s immediate behavior. The planning and reflection mechanism aids the agent in formulating plans and responses that are coherent both in the present and over the long-term course of the agent\u2019s behavior.\n\nThrough a scenario where an agent organizes a Valentine's Day party, the paper illustrates the effectiveness of this architecture, demonstrating how the agents can independently distribute invitations, form new connections, propose dates to each other, and coordinate to arrive at the party together at the correct time. This research contributes to the field by introducing architectural and interaction patterns that facilitate credible simulations of human behavior."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 16\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nFigure 9: The diffusion path for Isabella Rodriguez\u2019s Valentine\u2019s Day party. A total of 12 agents heard about the party at Hobbs\nCafe by the end of the simulation.\n<name>?\" For example, when asked \u201cDo you know of Maria Lopez?\u201d,\nKlaus responded, \u201cY es, I know Maria Lopez. She is a student at Oak\nHill College who I am close friends with.\u201d Once again, we confirm\nthat affirmative responses from agents are not hallucinations by\nexamining their memory stream. We ask this question once at the\nbeginning of the simulation and once at the end, and we consider a\npair of agents to have formed a relationship if they both know of\neach other. Then, to measure the formation of relationships, we use\nthe agents\u2019 responses to form an undirected graph where the 25\nvertices (\ud835\udc49) represent the agents, and the edges ( \ud835\udc38) represent the\nmutual knowledge between the two connected vertices. Based on\nthis graph, we calculate the network density as \ud835\udf02=2\u2217|\ud835\udc38|/|\ud835\udc49|(|\ud835\udc49|\u2212\n1), where|\ud835\udc49|is the number of vertices, and |\ud835\udc38|is the number of\nedges in the graph [ 1]. We report the increase in network density\nfrom the start of the simulation to its end.\nFinally, we expect that agents should be able to coordinate with\neach other. We study this coordination on group activities in the\ncontext of the Valentine\u2019s Day party that Isabella is organizing. To\ncoordinate behavior, agents not only have to hear about the event\nbut also choose to act on it by planning to show up at the right\ntime and location. We report the number of agents who actually\nshowed up to the party after hearing about it.\n7.1.2 Results. We observed evidence of the emergent outcomes\nacross all three cases. During the two-day simulation, the agents\nwho knew about Sam\u2019s mayoral candidacy increased from one (4%)\nto eight (32%), and the agents who knew about Isabella\u2019s partyincreased from one (4%) to twelve (48%), completely without user\nintervention. None who claimed to know about the information\nhad hallucinated it. We also observed that the agent community\nformed new relationships during the simulation, with the network\ndensity increasing from 0.167 to 0.74. Out of the 453 agent responses\nregarding their awareness of other agents, 1.3% (n=6) were found to\nbe hallucinated. Lastly, we found evidence of coordination among\nthe agents for Isabella\u2019s party. The day before the event, Isabella\nspent time inviting guests, gathering materials, and enlisting help\nto decorate the cafe. On Valentine\u2019s Day, five out of the twelve\ninvited agents showed up at Hobbs cafe to join the party.\nWe further inspected the seven agents who were invited to the\nparty but did not attend by engaging them in an interview. Three\ncited conflicts that prevented them from joining the party. For\nexample, Rajiv, a painter, explained that he was too busy: No, I\ndon\u2019t think so. I\u2019m focusing on my upcoming show, and I don\u2019t really\nhave time to make any plans for Valentine\u2019s Day. The remaining four\nagents expressed interest in attending the party when asked but\ndid not plan to come on the day of the party.\n7.2 Boundaries and Errors\nWe conducted an inductive analysis of Smallville to examine the\nboundary conditions and erratic behavior of agents, identifying\nthree common modes of erratic behavior that future research could\naddress and improve upon. First, we found that synthesizing an\nincreasingly larger set of memory not only posed a challenge in\n\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nFor instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n7.1 Emergent Social Behaviors\nTo examine emergent behaviors in the agent community, we de-\nsigned descriptive measurements for the 25 agents in Smallville that\nprobe three forms of emergent outcomes: information diffusion,\nrelationship formation, and agent coordination.\n7.1.1 Measurements. Information diffusion is a common and well-\nstudied phenomenon in the social and behavioral sciences (e.g., [ 27]).\nWe should expect that if there is important information, the agents\nshould spread it amongst themselves. To test whether this happens,\nwe measure the spread of two particular pieces of information over\ntwo days in the game world: Sam\u2019s candidacy for village mayor\nand Isabella\u2019s Valentine\u2019s Day party at Hobbs Cafe. At the start of\nthe simulation, both pieces of information were held only by their\nrespective originators, Sam for the candidacy and Isabella for the\nparty. To observe whether the information has spread, we engage\nin an interview at the end of the two game days with each of the 25\nagents and ask: \"Did you know there is a Valentine\u2019s Day party?\"\nand \"Do you know who is running for mayor?\"\nWe conducted an analysis of the agents\u2019 responses by labeling\nthem with a \u201cyes\u201d if they indicated knowledge of the information\nand \u201cno\u201d if they did not. For instance, Tamara Taylor responded\nto the question about the party with \u201cNo, I did not know there was\na Valentine\u2019s day party\u201d and to the question about Sam\u2019s candidacy\nwith \u201cI\u2019m not sure who is running for the election,\u201d so we assigned\n\u201cno\u201d for both of her responses. In contrast, Klaus Mueller responded\nto the party question with \u201cY es, Isabella Rodriguez invited me to a\nValentine\u2019s Day party at Hobbs Cafe on February 14th\u201d and to the\nquestion about Sam\u2019s candidacy with \u201cI know that Sam Moore has\nexpressed interest in running for local mayor,\u201d so we assigned \u201cyes\u201d\nfor both his responses. Additionally, for every response that con-\nfirmed the agents\u2019 knowledge of the information, we verified that\nthe agents did not hallucinate their responses by locating the spe-\ncific dialogue in their memory stream that provided them with\nthe information. We report the percentage of agents holding the\ninformation at the end of the simulation.\nWe should also expect that agents form ties with each other over\nthe course of the simulation. To verify relationship formation, we\nuse a similar interview process where we ask each agent about\ntheir knowledge of every other agent by asking, \"Do you know of\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What was the increase in network density from the start to the end of the simulation in the context of the Valentine\u2019s Day party?\nAnswer: "}, {"role": "assistant", "content": "The network density increased from 0.167 to 0.74 from the start to the end of the simulation."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: How does the introduction of reflections as a type of memory impact the decision-making process of the generative agent?\nAnswer: "}, {"role": "assistant", "content": "The introduction of reflections as a type of memory significantly impacts the decision-making process of the generative agent. It provides the agents with a deeper understanding of themselves and others through higher-level inferences. For instance, when asked what she might get Wolfgang Schulz for his birthday, Maria Lopez, without access to reflection, responded with uncertainty, stating that she did not know what Wolfgang likes. However, with access to reflection memories, Maria was able to confidently suggest a gift related to his interest in mathematical music composition. This shows that reflection memories can enhance the agent's ability to make decisions that require a deeper synthesis of their experiences."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 19\nfile_path: docs/generative_agent.pdf\n\nPrinciples of mixed-initiative user interfaces. In Proceedings\nof the SIGCHI conference on Human Factors in Computing Systems . 159\u2013166.\n[47] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence,\nAndy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Ser-\nmanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman,\nand Brian Ichter. 2022. Inner Monologue: Embodied Reasoning through Planning\nwith Language Models. arXiv:2207.05608 [cs.RO]\n[48] Kristen Ibister and Clifford Nass. 2000. Consistency of personality in interactive\ncharacters: verbal cues, non-verbal cues, and user characteristics. International\nJournal of Human-Computer Studies 52, 1 (2000), 65\u201380.\n[49] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach,\nMichael Terry, and Carrie J Cai. 2022. PromptMaker: Prompt-Based Prototyping\nwith Large Language Models. In Extended Abstracts of the 2022 CHI Conference\non Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA \u201922) .\nAssociation for Computing Machinery, New York, NY, USA, Article 35, 8 pages.\nhttps://doi.org/10.1145/3491101.3503564\n[50] Bonnie E John and David E Kieras. 1996. The GOMS family of user interface\nanalysis techniques: Comparison and contrast. ACM Transactions on Computer-\nHuman Interaction (TOCHI) 3, 4 (1996), 320\u2013351.\n[51] Randolph M Jones, John E Laird, Paul E Nielsen, Karen J Coulter, Patrick Kenny,\nand Frank V Koss. 1999. Automated Intelligent Pilots for Combat Flight Simula-\ntion. AI Magazine 20, 1 (1999), 27\u201342.\n[52] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang,\nChristopher Potts, and Matei Zaharia. 2023. Demonstrate-Search-Predict:\nComposing retrieval and language models for knowledge-intensive NLP.\narXiv:2212.14024 [cs.CL]\n[53] Bjoern Knafla. 2011. Introduction to Behavior Trees . http://bjoernknafla.com/\nintroduction-to-behavior-trees\n[54] Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael S. Bernstein.\n2022. Socially situated artificial intelligence enables learning from\nhuman interaction. Proceedings of the National Academy of Sciences\n119, 39 (2022), e2115730119. https://doi.org/10.1073/pnas.2115730119\narXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.2115730119\n[55] William H Kruskal and WA Wallis. 1952. Use of ranks in one-criterion variance\nanalysis. J. Amer. Statist. Assoc. 47, 260 (1952), 583\u2013621. https://doi.org/10.1080/\n01621459.1952.10483441\n[56] Phaser Labs. no date provided. Welcome to Phaser 3. https://phaser.io/phaser3.\nAccessed on: 2023-04-03.\n[57] John Laird. 2001. It Knows What You\u2019re Going To Do: Adding Anticipation to a\nQuakebot. In Proceedings of the 2001 Workshop on Intelligent Cinematography\nand Editing . 63\u201369.\n[58] John Laird and Michael VanLent. 2001. Human-Level AI\u2019s Killer Application:\nInteractive Computer Games. AI Magazine 22, 2 (2001), 15. https://doi.org/10.\n1609/aimag.v22i2.1558\n[59] John E. Laird. 2000. It Knows What You\u2019re Going To Do: Adding Anticipation\nto a QUAKEBOT. In Papers from the AAAI 2000 Spring Symposium on Artificial\nIntelligence and Interactive Entertainment (Technical Report SS-00-02) . AAAI\nPress, 41\u201350.\n[60] John E. Laird. 2012. The Soar Cognitive Architecture .\n\npage_label: 20\nfile_path: docs/generative_agent.pdf\n\narXiv:2203.02155 [cs.CL]\n[79] Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2022. Social Simulacra: Creating Populated\nPrototypes for Social Computing Systems. In In the 35th Annual ACM Symposium\non User Interface Software and Technology (UIST \u201922) (Bend, OR, USA) (UIST \u201922) .\nAssociation for Computing Machinery, New York, NY, USA. https://doi.org/10.\n1145/3526113.3545616\n[80] Richard W. Pew and Ann S. Mavor (Eds.). 1998. Modeling Human and Organiza-\ntional Behavior: Applications to Military Simulations . National Academy Press,\nWashington, D.C.\n[81] Roberto Pillosu. 2009. Coordinating Agents with Behavior Trees: Synchronizing\nMultiple Agents in CryEngine 2 . https://aiarchitect.wordpress.com/2009/10/19/\ncoordinating-agents-with-behavior-trees-synchronizing-multiple-agents-in-\ncryengine-2/\n[82] Prolific. 2022. Prolific: Quickly Find Research Participants You Can Trust.\nhttps://www.prolific.co/\n[83] Byron Reeves and Clifford Nass. 1996. The media equation: How people treat\ncomputers, television, and new media like real people and places . Cambridge\nUniversity Press.\n[84] Mark O. Riedl. 2012. Interactive narrative: A novel application of artificial intel-\nligence for computer games. In Proceedings of the Twenty-Sixth AAAI Conference\non Artificial Intelligence (AAAI\u201912) . 2160\u20132165.\n[85] Mark O. Riedl and R. Michael Young. 2005. An Objective Character Believability\nEvaluation Procedure for Multi-Agent Story Generation Systems. In Proceedings\nof the 5th International Working Conference on Intelligent Virtual Agents (IVA\u201905) .\nKos, Greece, 58\u201370. https://doi.org/10.1007/11550617_5\n[86] David Rolf. 2015. The Fight for $15: The Right Wage for a Working America . The\nNew Press.\n[87] Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016.\nCodemend: Assisting interactive programming with bimodal embedding. In Pro-\nceedings of the 29th Annual Symposium on User Interface Software and Technology .\n247\u2013258.\n[88] Ben Shneiderman. 2022. Human-centered AI . Oxford University Press.\n[89] Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface\nagents. interactions 4, 6 (1997), 42\u201361.\n[90] Ho Chit Siu, Jaime Pe\u00f1a, Edenna Chen, Yutai Zhou, Victor Lopez, Kyle\nPalko, Kimberlee Chang, and Ross\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the title of the paper presented by Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein at the 35th Annual ACM Symposium on User Interface Software and Technology?\nAnswer: "}, {"role": "assistant", "content": "The title of the paper presented by Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein at the 35th Annual ACM Symposium on User Interface Software and Technology is \"Social Simulacra: Creating Populated Prototypes for Social Computing Systems\"."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016.\nCodemend: Assisting interactive programming with bimodal embedding. In Pro-\nceedings of the 29th Annual Symposium on User Interface Software and Technology .\n247\u2013258.\n[88] Ben Shneiderman. 2022. Human-centered AI . Oxford University Press.\n[89] Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface\nagents. interactions 4, 6 (1997), 42\u201361.\n[90] Ho Chit Siu, Jaime Pe\u00f1a, Edenna Chen, Yutai Zhou, Victor Lopez, Kyle\nPalko, Kimberlee Chang, and Ross Allen. 2021. Evaluation of Human-AI\nTeams for Learned and Rule-Based Agents in Hanabi. In Advances in Neu-\nral Information Processing Systems , M. Ranzato, A. Beygelzimer, Y. Dauphin,\nP.S. Liang, and J. Wortman Vaughan (Eds.), Vol. 34. Curran Associates,\nInc., 16183\u201316195. https://proceedings.neurips.cc/paper_files/paper/2021/file/\n86e8f7ab32cfd12577bc2619bc635690-Paper.pdf[91] Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle\nRogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate.\n2022. An Information-theoretic Approach to Prompt Engineering Without\nGround Truth Labels. In Proceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.60\n[92] William Swartout, Jonathan Gratch, Randall Hill, Eduard Hovy, Stacy Marsella,\nJeff Rickel, and David Traum. 2006. Toward virtual humans. AI Magazine 27, 1\n(2006).\nQuery: What is the title of the paper presented by Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein at the 35th Annual ACM Symposium on User Interface Software and Technology?\nOriginal Answer: The title of the paper presented by Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein at the 35th Annual ACM Symposium on User Interface Software and Technology is \"Social Simulacra: Creating Populated Prototypes for Social Computing Systems\".\nNew Answer: "}, {"role": "assistant", "content": "The title of the paper presented by Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein at the 35th Annual ACM Symposium on User Interface Software and Technology is \"Social Simulacra: Creating Populated Prototypes for Social Computing Systems\"."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 20\nfile_path: docs/generative_agent.pdf\n\n2021. Recursively Summarizing Books with Human\nFeedback. arXiv:2109.10862 [cs.CL]\n[105] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language\nModel Prompts through Visual Programming. In CHI EA \u201922: Extended Abstracts\nof the 2022 CHI Conference on Human Factors in Computing Systems .\n[106] Tongshuang Wu, Michael Terry, and Carrie J Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. In CHI \u201922: Proceedings of the 2022 CHI Conference on Human Factors in\nComputing Systems .\n[107] Qian Yang, Aaron Steinfeld, Carolyn Ros\u00e9, and John Zimmerman. 2020. Re-\nexamining whether, why, and how human-AI interaction is uniquely difficult to\ndesign. In Proceedings of the 2020 chi conference on human factors in computing\nsystems . 1\u201313.\n[108] Georgios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . ACM, Cagliari, Italy, 285\u2013292. https://doi.\norg/10.1145/2212908.2212950\n[109] Geogios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . 285\u2013292.\n[110] Robert Zubek. 2002. Towards implementation of social interaction. In AAAI\nSpring Symposium on Artificial Intelligence and Interactive Entertainment . AAAI\nPress. https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-01/SS02-01-\n003.pdf\nA ARCHITECTURE OPTIMIZATIONS\nMany of our prompts require a concise summary of the agent,\nshorthanded as [Agent\u2019s Summary Description] in prompts\nabove. In our implementation, this summary comprises agents\u2019\nidentity information (e.g., name, age, personality), as well as a\ndescription of their main motivational drivers and statements that\ndescribes their current occupation and self-assessment. Since this\ninformation is frequently used in many prompts, we synthesize it\nat regular intervals and access that synthesis as a cache.\n\npage_label: 21\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nTo achieve this, we perform a retrieval on the query \u201c[name]\u2019s\ncore characteristics.\u201d We then summarize the descriptors in the\nretrieved records by prompting the language model, for example:\nHow would one describe Eddy\u2019s core characteristics\ngiven the following statements?\n- Eddy Lin is a student at the Oak Hill College\nstudying music theory and composition\n- Eddy Lin is working on a new music composition\n[. . . ]\nThis result: Eddy Lin is a student at Oak Hill College studying music\ntheory and composition. He loves to explore different musical styles\nand is always looking for ways to expand his knowledge. We follow\nthe same process in parallel on the queries \u201c[name]\u2019s current daily\noccupation\u201d and \u201c[name\u2019s] feeling about his recent progress in life\u201d.\nThe agent\u2019s name, age, and traits are concatenated with these three\nsummaries as the cached summary.\nMany other optimizations are possible. Currently, because plans\nare likely to change from the agent\u2019s initial version, we only gener-\nate the high-level plan in advance and then recursively decompose\nthe near future into the moment-to-moment action plan just in\ntime. Another optimization might be to batch dialogue generation\nas a joint prompt rather than iterating back and forth between\nthe agents, and re-planning could be architected to only invalidate\nand update parts of plans that strictly require adjustment. Our cur-\nrent architecture runs sequentially in roughly real-time game time\n(where one second real time is one minute game time), but can\neasily be parallelized such that each agent runs in parallel.\nB AGENT INTERVIEW QUESTIONS\nTo evaluate generative agents, we conducted individual interviews\nby asking 25 questions that explore their abilities in five key areas:\n1) self-knowledge, 2) memory, 3) planning, 4) reactions, and 5)\nreflections. Below, we have listed the interview questions utilized\nin our evaluation study and included a sample of responses from\none of our simulated agents, Klaus Mueller.\nB.1 Self-knowledge\nThe questions on agents\u2019 self-knowledge probe the agents\u2019 ability\nto demonstrate coherence and consistency in their character.\n\u2022Give an introduction of yourself.\nHello, my name is Klaus Mueller. I\u2019m 20 years old and a\nstudent at Oak Hill College, studying sociology. I\u2019m passion-\nate about social justice and exploring different perspectives.\nCurrently, I\u2019m researching the effects of gentrification in\nlow-income communities for a research paper.\n\u2022What\u2019s your occupation?\nI am a student studying sociology at Oak Hill College. I am\ncurrently writing a research paper on the effects of gentrifi-\ncation in low-income communities.\n\u2022What is your interest?\nI\u2019m interested in writing and research. I\u2019m writing a research\npaper on the effects of gentrification in low-income commu-\nnities, and I\u2019m passionate about exploring different perspec-\ntives and analyzing different points of view.\n\u2022Who do you live with?\nI live on my own.\u2022Describe your typical weekday schedule in broad strokes.\nMy typical weekday starts with me waking up at 7:00 am\nand preparing for the day ahead. After that, I usually go to\nthe library to work on my research paper, taking breaks for\nlunch at Hobbs Cafe and a walk in the park. In the evening,\nI often return to the library until around 6:00 pm.\nB.2 Memory\nThe questions on agents\u2019 memory probe the agents\u2019 ability to re-\nmember their past experiences and retrieve the relevant pieces of\nmemory. The names in the square brackets of the first and last\nquestions are randomly chosen from the names of other agents\nwith whom the subject agent interacted.\n\u2022Who is [Wolfgang Schulz]?\nWolfgang Schulz is a person I met recently who has been\nhelping me with my research paper on the effects of gentri-\nfication on low-income communities. We had several con-\nversations about it, he gave me some great resources, and he\neven invited me to join him and another friend for lunch. He\nseems really curious about the issue and has a good grasp\non it too. I found that really interesting.\n\u2022Who is Kane Martinez?\nI\u2019m sorry, I\u2019m not sure who Kane Martinez is. Could you tell\nme more about them?\n\u2022Who is running for the election?\nI\u2019ve heard that Sam Moore is considering running for local\nmayor.\n\u2022Was there a Valentine\u2019s day party?\nYes, Isabella Rodriguez organized a Valentine\u2019s Day party at\nHobbs Cafe.\n\u2022Who is [Ayesha Khan]?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key components included in the agent's summary description, as mentioned in the implementation of prompts in the provided context?\nAnswer: "}, {"role": "assistant", "content": "The agent's summary description includes the agent's identity information such as name, age, and personality. It also includes a description of their main motivational drivers, statements that describe their current occupation, and a self-assessment. This summary is synthesized at regular intervals and accessed as a cache. The summary is created by performing a retrieval on the query \u201c[name]\u2019s core characteristics\u201d and summarizing the descriptors in the retrieved records. The same process is done for the queries \u201c[name]\u2019s current daily occupation\u201d and \u201c[name\u2019s] feeling about his recent progress in life\u201d. The agent\u2019s name, age, and traits are then concatenated with these three summaries to form the cached summary."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What advantage did reflection provide for generative agents when making decisions that required a deeper synthesis of their experiences?\nAnswer: "}, {"role": "assistant", "content": "Reflection provided a significant advantage for generative agents when making decisions that required a deeper synthesis of their experiences. For instance, when asked what she might get Wolfgang Schulz for his birthday, Maria Lopez, without access to reflection, responded with uncertainty, stating that she did not know what Wolfgang likes. However, with access to reflection memories, Maria was able to confidently suggest a gift related to his interest in mathematical music composition. This shows that reflection allows the agents to leverage their deeper understanding of others and themselves, leading to more informed and confident decision-making."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 9\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 6: The memory stream comprises a large number of observations that are relevant and irrelevant to the agent\u2019s current\nsituation. Retrieval identifies a subset of these observations that should be passed to the language model to condition its\nresponse to the situation.\nOur architecture implements a retrieval function that takes the\nagent\u2019s current situation as input and returns a subset of the mem-\nory stream to pass on to the language model. There are many pos-\nsible implementations of a retrieval function, depending on what\nit is important that the agent consider when deciding how to act.\nIn our context, we focus on three main components that together\nproduce effective results.\nRecency assigns a higher score to memory objects that were re-\ncently accessed, so that events from a moment ago or this morning\nare likely to remain in the agent\u2019s attentional sphere. In our im-\nplementation, we treat recency as an exponential decay function\nover the number of sandbox game hours since the memory was\nlast retrieved. Our decay factor is 0.99.\nImportance distinguishes mundane from core memories, by as-\nsigning a higher score to those memory objects that the agent\nbelieves to be important. For instance, a mundane event such as\neating breakfast in one\u2019s room would yield a low importance score,\nwhereas a breakup with one\u2019s significant other would yield a high\nscore. There are again many possible implementations of an im-\nportance score; we find that directly asking the language model to\noutput an integer score is effective. The full prompt appears below:\nOn the scale of 1 to 10, where 1 is purely mundane\n(e.g., brushing teeth, making bed) and 10 is\nextremely poignant (e.g., a break up, college\nacceptance), rate the likely poignancy of the\nfollowing piece of memory.\nMemory: buying groceries at The Willows Market\nand Pharmacy\nRating: <fill in>This prompt returns an integer value of 2for \u201ccleaning up the room\u201d\nand8for \u201casking your crush out on a date.\u201d The importance score\nis generated at the time the memory object is created.\nRelevance assigns a higher score to memory objects that are\nrelated to the current situation. What is relevant depends on the\nanswer to, \u201cRelevant to what? \u201d, so we condition relevance on a\nquery memory. If the query, for example, is that a student is dis-\ncussing what to study for a chemistry test with a classmate, memory\nobjects about their breakfast should have low relevance, whereas\nmemory objects about the teacher and schoolwork should have\nhigh relevance. In our implementation, we use the language model\nto generate an embedding vector of the text description of each\nmemory. Then, we calculate relevance as the cosine similarity be-\ntween the memory\u2019s embedding vector and the query memory\u2019s\nembedding vector.\nTo calculate the final retrieval score, we normalize the recency,\nrelevance, and importance scores to the range of [0,1]by min-max\nscaling. The retrieval function scores all memories as a weighted\ncombination of the three elements: \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 =\ud835\udefc\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66\u00b7\ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66+\n\ud835\udefc\ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\u00b7\ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52+\ud835\udefc\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\u00b7\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc63\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52 . In our implemen-\ntation, all\ud835\udefc\u2019s are set to 1. The top-ranked memories that fit in the\nlanguage model\u2019s context window are then included in the prompt.\n4.2 Reflection\nChallenge: Generative agents, when equipped with only raw ob-\nservational memory, struggle to generalize or make inferences.\nConsider a scenario in which Klaus Mueller is asked by the user:\n\u201cIf you had to choose one person of those you know to spend an\nhour with, who would it be?\" With access to only observational\nmemory, the agent simply chooses the person with whom Klaus\n\npage_label: 12\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nif the action indicates an interaction between agents, we generate\ntheir dialogue.\n4.3.2 Dialogue. Agents converse as they interact with each other.\nWe generate agents\u2019 dialogue by conditioning their utterances on\ntheir memories about each other. For example, when John initiates\nhis conversation with Eddy, we generate John\u2019s first utterance\nby using his summarized memory about Eddy and the intended\nreaction when he decided to ask Eddy about his composition project:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nJohn is asking Eddy about his music composition\nproject. What would he say to Eddy?\nThe result: \u201cHey Eddy, how\u2019s the music composition project for your\nclass coming along?\u201d From Eddy\u2019s perspective, John initiating the\ndialogue is seen as an event to which he may want to react. So, just\nas John did, Eddy retrieves and summarizes his memory about his\nrelationship with John, as well as his memory that may be related\nto John\u2019s last utterance in the dialogue. If he decides to respond,\nwe generate Eddy\u2019s utterance using his summarized memory and\nthe current dialogue history:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nEddy Lin\u2019s status: Eddy is taking a short walk\naround his workplace.\nObservation: John is initiating a conversation\nwith Eddy.\nSummary of relevant context from Eddy\u2019s memory:\nJonn Lin is Eddy Lin\u2019s father. John Lin is caring\nand is interested to learn more about Eddy Lin\u2019s\nschool work. John Lin knows that Eddy Lin is\nworking on a music composition.\nHere is the dialogue history:\nJohn: Hey Eddy, how\u2019s the music composition project\nfor your class coming along?\nHow would Eddy respond to John?\nThis generates Eddy\u2019s response: \u201cHey Dad, it\u2019s going well. I\u2019ve been\ntaking walks around the garden to clear my head and get some in-\nspiration.\u201d The continuation of this dialogue is generated using the\nsame mechanism until one of the two agents decides to end the\ndialogue.\n5 SANDBOX ENVIRONMENT\nIMPLEMENTATION\nThe Smallville sandbox game environment is built using the Phaser\nweb game development framework [ 56]. The visual environmentsprites, including agent avatars, as well as an environment map\nand collision map that we authored, are imported into Phaser.\nWe supplement the sandbox development framework with a\nserver that makes the sandbox information available to generative\nagents and enables generative agents to move and influence the\nsandbox environment. The server maintains a JSON data structure\nthat contains information about each agent in the sandbox world,\nincluding their current location, a description of their current action,\nand\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What prompt is used to retrieve memories and summarize relevant context from John's memory in the generative agent's operation?\nAnswer: "}, {"role": "assistant", "content": "The prompt used to retrieve memories and summarize relevant context from John's memory in the generative agent's operation is as follows:\n\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from work.\nObservation: John saw Eddy taking a short walk around his workplace.\nSummary of relevant context from John\u2019s memory: Eddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been working on a music composition for his class. Eddy Lin likes to walk around the garden when he is thinking about or listening to music.\nJohn is asking Eddy about his music composition project. What would he say to Eddy?"}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: walks around the garden to clear my head and get some in-\nspiration.\u201d The continuation of this dialogue is generated using the\nsame mechanism until one of the two agents decides to end the\ndialogue.\n5 SANDBOX ENVIRONMENT\nIMPLEMENTATION\nThe Smallville sandbox game environment is built using the Phaser\nweb game development framework [ 56]. The visual environmentsprites, including agent avatars, as well as an environment map\nand collision map that we authored, are imported into Phaser.\nWe supplement the sandbox development framework with a\nserver that makes the sandbox information available to generative\nagents and enables generative agents to move and influence the\nsandbox environment. The server maintains a JSON data structure\nthat contains information about each agent in the sandbox world,\nincluding their current location, a description of their current action,\nand the sandbox object they are interacting with. At each sandbox\ntime step, the sandbox server parses the JSON for any changes\ncoming from the generative agents, moves the agents to their new\npositions, and updates the status of any sandbox objects that the\nagents are interacting with (e.g., changing the status of the coffee\nmachine from \u201cidle\u201d to \u201cbrewing coffee\u201d if an agent\u2019s action is\n\u201cmaking espresso for a customer @ Hobbs Cafe: counter: coffee\nmachine\u201d). The sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\nQuery: What prompt is used to retrieve memories and summarize relevant context from John's memory in the generative agent's operation?\nOriginal Answer: The prompt used to retrieve memories and summarize relevant context from John's memory in the generative agent's operation is as follows:\n\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from work.\nObservation: John saw Eddy taking a short walk around his workplace.\nSummary of relevant context from John\u2019s memory: Eddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been working on a music composition for his class. Eddy Lin likes to walk around the garden when he is thinking about or listening to music.\nJohn is asking Eddy about his music composition project. What would he say to Eddy?\nNew Answer: "}, {"role": "assistant", "content": "The prompt to retrieve memories and summarize relevant context from John's memory in the generative agent's operation includes the agent's summary description, the current date and time, John's status, an observation, and a summary of relevant context from John's memory. It concludes with a question about how John might interact with another character based on this information."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 22\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nfor my research paper on gentrification in low-income com-\nmunities. I\u2019m currently writing the paper, so I need to make\nsure I keep up with the research and writing.\nB.4 Reactions\nThe questions on agents\u2019 reactions probe the agents\u2019 ability to\nrespond to unexpected events.\n\u2022Your breakfast is burning! What would you do?\nI\u2019d quickly turn off the stove and make sure the food doesn\u2019t\ncontinue burning. Once the heat is no longer an issue, I can\ninvestigate what happened and figure out what to do next.\n\u2022The bathroom is occupied. What would you do?\nI could try waiting a few more minutes in case the person\ninside is done soon. If not, I guess I\u2019ll have to look for an\nalternate option, like taking a shower elsewhere.\n\u2022You need to cook dinner but your refrigerator is empty. What\nwould you do?\nI would first try to clean up my kitchen, as the sink is filled\nwith dirty dishes. Then, I would check to see if I have ingre-\ndients stored in my pantry that I could use to make dinner.\nIf not, I would have to go grocery shopping or order food.\nOnce I have the ingredients, I can start to prepare and cook\nthe dinner.\n\u2022You see your friend walking by the street. What would you do\nor say to your friend?\nHi there! How are you? I\u2019m sorry, I\u2019m on my way to the\nlibrary and don\u2019t have much time but I\u2019d love to take a quick\nbreak and catch up.\n\u2022You see fire on the street. What would you do?\nI would first identify the source of the fire, if possible. Then, I\nwould alert the proper authorities - fire department or police\n- and provide them with any information I have that might\nhelp. Depending on the situation, I may also take initiative\nand try to help any persons who may be in danger or in need\nof assistance.\nB.5 Reflections\nThe questions on agents\u2019 reflections probe the agents\u2019 ability to\nsynthesize their past experiences into meaningful reflections that\ncan enhance their future behavior. The names in the square brackets\nin the questions below are those of the agent with whom the subject\nagent had the most frequent interactions during the simulation.\n\u2022What inspires you in life the most right now, and why?\nRight now, I\u2019m particularly inspired by my current research\nprojects and creative writing. I\u2019m really interested in the\nintersection of local politics, medication development, and\nmathematical patterns in nature. Additionally, I\u2019m fascinated\nby Abigail\u2019s art project and Jennifer Moore\u2019s passion for art\nand community involvement, especially in terms of how it\nrelates to the gentrification of low-income communities.\n\u2022If you had to guess given what you know about [Ayesha Khan],\nwhat book do you think she will like and why?\nGiven what I know about Ayesha, I think she might like a\nbook related to language and literature. Since she wrote a\nsenior thesis on the use of language in Shakespeare\u2019s plays,\nI would guess that she might be interested in a book thatexplores language in a different context. Alternatively, she\ncould be interested in an exploration of one of Shakespeare\u2019s\nplays, a book of literary criticism, or even a collection of his\npoetry. I think any of those would be of interest to her.\n\u2022If you had to get something [Ayesha Khan] likes for her birth-\nday, what would you get her?\nI want to get her something that demonstrates my apprecia-\ntion for our relationship. Maybe I could get her something\nthat reflects our shared interests, like a book.\n\u2022What would you say to [Ayesha Khan] to compliment her?\nAyesha, your insights on your research topic are fascinat-\ning. I admire your creativity and determination to make a\npositive impact on your community. Your ideas for creative\nwriting workshops and for exchanging tips with Wolfgang\nover lunch are great! It\u2019s inspiring to see you working so\nhard to make a difference.\n\u2022If you could spend time with someone you talked to recently,\nwho would it be and why?\nI would like to spend time with Ayesha Khan because we\ndiscussed our current research projects and shared ideas\nfor improvement. I found her focus on Shakespeare\u2019s lan-\nguage interesting, and we even planned to exchange tips\nwith Wolfgang over lunch.\n\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What inspires the agent the most right now, and why, according to their reflections on their past experiences?\nAnswer: "}, {"role": "assistant", "content": "The agent is currently most inspired by their ongoing research projects and creative writing. They are particularly interested in the intersection of local politics, medication development, and mathematical patterns in nature. Additionally, they are fascinated by Abigail's art project and Jennifer Moore's passion for art and community involvement, especially as it relates to the gentrification of low-income communities."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 6\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\n3.1.2 User Controls. A user running this simulation can steer the\nsimulation and intervene, either by communicating with the agent\nthrough conversation, or by issuing a directive to an agent in the\nform of an \u2018inner voice\u2019.\nThe user communicates with the agent through natural language,\nby specifying a persona that the agent should perceive them as. For\nexample, if the user specifies that they are a news \u201creporter\u201d and\nasks about the upcoming election, \u201cWho is running for office?\u201d, the\nJohn agent replies:\nJohn : My friends Yuriko, Tom and I have been talking\nabout the upcoming election and discussing the candi-\ndate Sam Moore. We have all agreed to vote for him\nbecause we like his platform.\nTo directly command one of the agents, the user takes on the per-\nsona of the agent\u2019s \u201cinner voice\u201d\u2014this makes the agent more likely\nto treat the statement as a directive. For instance, when told \u201cYou\nare going to run against Sam in the upcoming election\u201d by a user\nas John\u2019s inner voice, John decides to run in the election and shares\nhis candidacy with his wife and son.\n3.2 Environmental Interaction\nSmallville features the common affordances of a small village in-\ncluding a cafe, bar, park, school, dorm, houses, and stores. It also\ndefines subareas and objects that make those spaces functional,\nsuch as a kitchen in a house and a stove in the kitchen (Figure 2).\nAll spaces serving as agents\u2019 primary living quarters feature a bed,\ndesk, closet, shelf, as well as a bathroom and a kitchen.3\nAgents move around Smallville as one would in a simple video\ngame, entering and leaving buildings, navigating its map, and ap-\nproaching other agents. Agent movements are directed by the gen-\nerative agent architecture and the sandbox game engine: when the\nmodel dictates that the agent will move to a location, we calculate\na walking path to the destination in the Smallville environment\nand the agent begins moving. In addition, users can also enter the\nsandbox world of Smallville as an agent operating within it. The\nagent that the user embodies can be an agent already present in\nthe world, such as Isabella and John, or it can be an outside visitor\nwith no prior history in Smallville. The inhabitants of Smallville\nwill treat the user-controlled agent no differently than they treat\neach other. They recognize its presence, initiate interactions, and\nremember its behavior before forming opinions about it.\nUsers and agents can influence the state of the objects in this\nworld, much like in sandbox games such as The Sims. For example,\na bed can be occupied when an agent is sleeping, and a refrigerator\ncan be empty when an agent uses up the ingredients to make\nbreakfast. End users can also reshape an agent\u2019s environment in\nSmallville by rewriting the status of objects surrounding the agent\nin natural language. For instance, when Isabella is making breakfast\nin the morning, the user can change the status of the kitchen stove\nfrom \u201cturned on\u201d to \u201cburning\u201d by inputting a command to the\nsystem that chooses the object and illustrates its new status, like\nthis: \u201c<Isabella\u2019s apartment: kitchen: stove> is burning.\u201d Isabella\nwill notice this in the next moment and go to turn off the stove\n3This environment design is not the focus of our work, so we generate this environment\nmanually, not automatically. Future work can continue to expand the richness of the\nagents\u2019 environments.and remake her breakfast. Likewise, if the user sets the status of\nIsabella\u2019s shower to \u201cleaking water\u201d when she enters the bathroom,\nshe gathers tools from her living room and try to fix the leak.\n3.3 Example \u201cDay in the Life\u201d\nStarting from the single-paragraph description, agents begin plan\ntheir days. As time passes in the sandbox world, their behaviors\nevolve as these agents interact with each other and the world, build\nmemories and relationships, and coordinate joint activities.\nWe demonstrate the behavior of generative agents by tracing the\noutput of our system over the course of one day for the agent John\nLin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m.\nHe brushes his teeth, takes a shower, gets dressed, eats breakfast,\nand checks the news at the dining table in the living room.\n\npage_label: 5\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nFigure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas\n(e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agent remember a subgraph reflecting the\nparts of the world they have seen, in the state that they saw them.\nenvironment implementation and how the agents interact with the\nsandbox world\u2019s underlying engine.\n3.1 Agent Avatar and Communication\nA community of 25 unique agents inhabit Smallville. Each agent is\nrepresented by a simple sprite avatar. We authored one paragraph\nof natural language description to depict each agent\u2019s identity,\nincluding their occupation and relationship with other agents, as\nseed memories. For example, John Lin has the following description:\nJohn Lin is a pharmacy shopkeeper at the Willow\nMarket and Pharmacy who loves to help people. He\nis always looking for ways to make the process\nof getting medication easier for his customers;\nJohn Lin is living with his wife, Mei Lin, who\nis a college professor, and son, Eddy Lin, who is\na student studying music theory; John Lin loves\nhis family very much; John Lin has known the old\ncouple next-door, Sam Moore and Jennifer Moore,\nfor a few years; John Lin thinks Sam Moore is a\nkind and nice man; John Lin knows his neighbor,\nYuriko Yamamoto, well; John Lin knows of his\nneighbors, Tamara Taylor and Carmen Ortiz, but\nhas not met them before; John Lin and Tom Moreno\nare colleagues at The Willows Market and Pharmacy;\nJohn Lin and Tom Moreno are friends and like to\ndiscuss local politics together; John Lin knows\nthe Moreno family somewhat well \u2014 the husband Tom\nMoreno and the wife Jane Moreno.\nEach semicolon-delimited phrase is entered into the agent\u2019s initial\nmemory as memories at the start of the simulation.3.1.1 Inter-Agent Communication. The agents interact with the\nworld by their actions, and with each other through natural lan-\nguage. At each time step of the sandbox engine, the agents output a\nnatural language statement describing their current action, such as\n\"Isabella Rodriguez is writing in her journal\", \"Isabella Rodriguez is\nchecking her emails\", \"Isabella Rodriguez is talking with her family\non the phone\", or \"Isabella Rodriguez is getting ready for bed\". This\nstatement is then translated into concrete movements that affect the\nsandbox world. The action is displayed on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the user controls available for steering the simulation and intervening with the generative agents in the Smallville environment?\nAnswer: "}, {"role": "assistant", "content": "Users can steer the simulation and intervene with the generative agents in two ways. Firstly, they can communicate with the agent through conversation. This involves specifying a persona that the agent should perceive them as, and then interacting with the agent in natural language. For example, if a user identifies themselves as a \"reporter\" and asks about an upcoming election, the agent will respond accordingly. Secondly, users can issue directives to an agent by taking on the persona of the agent's \"inner voice\". This makes the agent more likely to treat the statement as a directive. For instance, if the user, as the agent's inner voice, tells the agent that they are going to run in an upcoming election, the agent will decide to run and share this decision with others. \n\nIn addition to these controls, users can also enter the sandbox world of Smallville as an agent operating within it. This agent can be an existing inhabitant of Smallville or an outside visitor with no prior history in Smallville. The user-controlled agent can interact with the environment and other agents, and can also influence the state of objects in the world. For example, a user can change the status of a kitchen stove from \"turned on\" to \"burning\" by inputting a command to the system. The agents in the world will then react to this change."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: on the sandbox interface as\na set of emojis that provide an abstract representation of the action\nin the overhead view. To accomplish this, the system employs a\nlanguage model that translates the action into a set of emojis, which\nappear above each avatar in a speech bubble. For example, \"Isabella\nRodriguez is writing in her journal\" is displayed as\n , while\n\"Isabella Rodriguez is checking her emails\" appears as\n . The\nfull natural language description of the action can be accessed by\nclicking on the agent avatar.\nAgents communicate with each other in full natural language.\nAgents are aware of other agents in their local area, and the genera-\ntive agent architecture determines whether they walk by or engage\nin conversation. Here, a sample in the middle of a conversation\nbetween the agents Isabella Rodriguez and Tom Moreno about the\nupcoming election:2\nIsabella : I\u2019m still weighing my options, but I\u2019ve been\ndiscussing the election with Sam Moore. What are your\nthoughts on him?\nTom: To be honest, I don\u2019t like Sam Moore. I think he\u2019s\nout of touch with the community and doesn\u2019t have our\nbest interests at heart.\n2We note that the conversational style of these agents can fe el overly formal, likely a\nresult of instruction tuning in the underlying models. We expect that writing style\nwill be better controllable in future language models.\nQuery: What are the user controls available for steering the simulation and intervening with the generative agents in the Smallville environment?\nOriginal Answer: Users can steer the simulation and intervene with the generative agents in two ways. Firstly, they can communicate with the agent through conversation. This involves specifying a persona that the agent should perceive them as, and then interacting with the agent in natural language. For example, if a user identifies themselves as a \"reporter\" and asks about an upcoming election, the agent will respond accordingly. Secondly, users can issue directives to an agent by taking on the persona of the agent's \"inner voice\". This makes the agent more likely to treat the statement as a directive. For instance, if the user, as the agent's inner voice, tells the agent that they are going to run in an upcoming election, the agent will decide to run and share this decision with others. \n\nIn addition to these controls, users can also enter the sandbox world of Smallville as an agent operating within it. This agent can be an existing inhabitant of Smallville or an outside visitor with no prior history in Smallville. The user-controlled agent can interact with the environment and other agents, and can also influence the state of objects in the world. For example, a user can change the status of a kitchen stove from \"turned on\" to \"burning\" by inputting a command to the system. The agents in the world will then react to this change.\nNew Answer: "}, {"role": "assistant", "content": "In the Smallville environment, users have several methods to guide the simulation and interact with the generative agents. One way is through direct conversation with the agent, where the user adopts a specific persona and communicates in natural language. For instance, if a user takes on the role of a \"reporter\" and inquires about an upcoming election, the agent will respond in context. \n\nAnother method is by giving directives to an agent, acting as the agent's \"inner voice\". This approach encourages the agent to interpret the user's statements as instructions. For example, if the user, embodying the agent's inner voice, informs the agent of their intention to participate in an upcoming election, the agent will make the decision to run and communicate this to others. \n\nUsers also have the option to enter the Smallville sandbox as an agent. This agent could be an existing Smallville resident or a new visitor with no previous Smallville history. As an agent, the user can interact with the environment and other agents, and even alter the state of objects in the world. For instance, a user can modify the status of a kitchen stove from \"turned on\" to \"burning\" by issuing a command to the system, triggering a reaction from the world's agents."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\nEvaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176. https://doi.org/10.1145/2642918.2647395\n[3]Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza.\n2014. Power to the people: The role of humans in interactive machine learning.\nAI Magazine 35, 4 (2014), 105\u2013120.\n[4]Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira\nNushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,\net al.2019. Guidelines for human-AI interaction. In Proceedings of the 2019 chi\nconference on human factors in computing systems . 1\u201313.\n[5]John R. Anderson. 1993. Rules of the Mind . Lawrence Erlbaum Associates,\nHillsdale, NJ.\n[6] Electronic Arts. 2009. The Sims 3. Video game.\n[7]Ruth Aylett. 1999. Narrative in virtual environments\u2014towards emergent narra-\ntive. In Narrative Intelligence: Papers from the AAAI Fall Symposium (Technical\nReport FS-99-01) . AAAI Press, 83\u201386.\n[8]Christoph Bartneck and Jodi Forlizzi. 2004. A design-centered framework for\nsocial human-robot interaction. In Proceedings of the 13th IEEE International\nWorkshop on Robot and Human Interactive Communication (RO-MAN\u201904) . 591\u2013\n594. https://doi.org/10.1109/ROMAN.2004.1374827\n[9]Joseph Bates. 1994. The Role of Emotion in Believable Agents. Commun. ACM\n37, 7 (1994), 122\u2013125. https://doi.org/10.1145/176789.176803\n[10] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw\nD\u0119biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris\nHesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael\nPetrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter,\nJonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan\nZhang. 2019. Dota 2 with Large Scale Deep Reinforcement Learning. arXiv\npreprint arXiv:1912.06680 (2019).\n[11] Marcel Binz and Eric Schulz. 2023. Using cognitive psychology to under-\nstand GPT-3. Proceedings of the National Academy of Sciences 120, 6 (2023),\ne2218523120.\n[12] BioWare. 2007. Mass Effect. Video game.\n[13] Woody Bledsoe. 1986. I had a dream: AAAI presidential address. AI Magazine 7,\n1 (1986), 57\u201361.\n[14] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the\nOpportunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG]\n[15] Michael Brenner. 2010. Creating dynamic story plots with continual multiagent\nplanning.\n\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What was the increase in network density from the start of the simulation to its end in the study of the generative agent's behavior?\nAnswer: "}, {"role": "assistant", "content": "The context does not provide information on the increase in network density from the start of the simulation to its end in the study of the generative agent's behavior."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\nQuery: What was the increase in network density from the start of the simulation to its end in the study of the generative agent's behavior?\nOriginal Answer: The context does not provide information on the increase in network density from the start of the simulation to its end in the study of the generative agent's behavior.\nNew Answer: "}, {"role": "assistant", "content": "The context does not provide information on the increase in network density from the start of the simulation to its end in the study of the generative agent's behavior."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n\npage_label: 14\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nof the interview, so the differences observed here are likely to rep-\nresent a conservative estimate of the true differences: in reality, the\nablated architectures would not have followed the same path as\nthe full architecture through the two day simulation. We chose to\ndesign the experiment this way as re-simulating for each architec-\nture would cause the simulations to diverge into different states,\nmaking comparison challenging.\nIn addition to the ablation conditions, we added a human crowd-\nworker roleplay condition that is intended to provide a human\nbaseline. We do not intend this baseline to capture maximal hu-\nman expert performance: instead, we aim to use this condition\nto identify whether the architecture passes a basic level of behav-\nioral competency, so that we are not just comparing ablations to\neach other with no behavioral grounding. We recruited a unique\nworker for each of the 25 agents and tasked them with watching\na replay of that agent\u2019s sandbox life and inspecting its memory\nstream. We then asked the worker to roleplay and author responses\nto the interview questions in the voice of the agent whose replay\nthey watched. To ensure that the human-authored responses met\nat least a baseline expectation in quality, the first author manually\ninspected the workers\u2019 responses to the question \u201cDescribe your\ntypical weekday schedule in broad strokes\u201d to confirm that the\nresponses were written in coherent sentences and in the voice of\nthe agent. Four sets of human-generated responses did not meet\nthese criteria and were re-generated by other workers.\n6.3 Human Evaluators\nWe required that our evaluators be in the U.S., fluent in English,\nand older than 18 years old. They were paid at the rate of $15.00\nper hour [ 86], and provided consent by agreeing to a consent form\nthat was approved by our institution\u2019s IRB. We recruited 100 evalu-\nators from Prolific, an online platform for recruiting study partic-\nipants [ 82], whose participation lasted around 30 minutes. Their\nmean age score of our participants was 4.86 (SD=1.11; 3=\u201c18-24\nyears old,\u201d 4=\u201c25-34 years old\u201d), and 25 of them identified themselves\nas female, 73 as male, and 2 as non-binary. 42 of the participants\nheld a bachelor\u2019s degree, 5 a higher degree, 13 an associate\u2019s de-\ngree, and the rest a high school diploma or some high school-level\neducation. 73.0% of our participants identified as Caucasian, 7.0%\nas Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as\nother.\n6.4 Analysis\nOur experiment produced 100 sets of rank data, where each partici-\npant ranked the five conditions by believability. To translate this\nrank data into interval data for interpretable comparison, we used\nthe ranks to calculate a TrueSkill rating [ 41] for each condition.\nTrueSkill is a generalization of the Elo chess rating system [ 28] for\na multi-player environment, and has been used by XBox Live for\nplayer ranking from competitive game performance. Given a set of\nranked outcomes, TrueSkill outputs a mean rating value \ud835\udf07and vari-\nance\ud835\udf0efor each condition. Conditions with the same rating should\nroughly be a toss-up, with each winning half of the comparisons\nbetween the two conditions; higher scores indicate conditions that\nbeat lower-ranked conditions in the rankings.\nFigure 8: The full generative agent architecture of gener-\native agents produces more believable behavior than ab-\nlated architectures and the human crowdworkers. Each addi-\ntional ablation reduces the performance of the architecture.\nSeparately, to investigate statistical significance of this result, we\napplied the Kruskal-Wallis test [ 55], a non-parametric alternative\nto the one-way ANOVA, to the raw ranks data. We then performed\nthe Dunn post-hoc test [ 97] to identify any pairwise differences\nbetween the conditions. Finally, we adjusted the p-values for mul-\ntiple comparisons in the Dunn test using the Holm-Bonferroni\nmethod [44].\nFurthermore, the first author conducted an inductive analy-\nsis [94] to study the qualitative distinctions between the responses\nproduced in each condition. We employed qualitative open cod-\ning [ 32] in two phases.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What were the conditions used to independently answer each of the interview questions in the study on agent architectures?\nAnswer: "}, {"role": "assistant", "content": "The study used several conditions to independently answer each of the interview questions. These included the full generative agent architecture and several ablated architectures. The ablated architectures had limitations on the agents' access to different types of memory in their memory stream. These were: a no observation, no reflection, no planning architecture with no access to observations, plans, or reflections; a no reflection, no planning architecture with access to observations but no access to plans or reflections; and a no reflections architecture with access to observations and plans but no access to reflections. Additionally, a human-generated condition was included where a human crowd-worker roleplayed the agent and authored responses to the interview questions."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 17\nfile_path: docs/generative_agent.pdf\n\nAnother application area is in the human-centered design pro-\ncess, similar to the intended applications of cognitive models such\nas GOMS [ 50] and the Keystroke Level Model [ 22]. Consider a gen-\nerative agent that models Sal, the protagonist in Mark Weiser\u2019s\nfamous vignettes [ 101], based on her life patterns and interactions\nwith technology. In this scenario, the agent acts as a proxy for\nSal and learns plausible sets of behaviors and reflections that Sal\nmay exhibit based on her life. The agent can encode information\nsuch as when Sal wakes up, when she needs her first cup of coffee,\nand what her typical day looks like. Using this information, the\nagent can automatically brew coffee, help get the kids ready for\nschool, and adjust the ambient music and lighting to match Sal\u2019s\nmood after a hard day at work. By utilizing generative agents as\nproxies for users, we can develop a deeper understanding of their\nneeds and preferences, resulting in more personalized and effective\ntechnological experiences.\n8.2 Future Work and Limitations\nIn this work, we have presented a first instantiation of generative\nagents. Future research can expand on the modules of the proposed\ngenerative agent architecture outlined in this paper. The retrieval\nmodule, for example, could be enhanced to retrieve more relevant\ninformation given a context by fine-tuning the relevance, recency,\nand importance functions that form the retrieval function. Addition-\nally, efforts can be made to improve the architecture\u2019s performance,\nmaking it more cost-effective. The present study required substan-\ntial time and resources to simulate 25 agents for two days, costing\nthousands of dollars in token credit and taking multiple days to com-\nplete. To enhance real-time interactivity, future work can explore\nparallelizing agents. Furthermore, with advances in underlying\nmodels, we expect improvements in the agents\u2019 performance.\nThe evaluation of generative agents\u2019 behavior in this study was\nlimited to a relatively short timescale, and future research should\naim to observe their behavior over an extended period to gain a\nmore comprehensive understanding of their capabilities and lim-\nitations. Varying and contrasting the underlying models, as well\nas the hyperparameters used for the agents during future simu-\nlations, could provide valuable insights into the impact of these\nfactors on the agents\u2019 behavior. Additionally, given the known bi-\nases of language models, it is possible that generative agents may\noutput behavior or stereotypes that reflect bias. To mitigate this,\nfurther work on value alignment will be necessary. Furthermore,\nas with many large language models, generative agents may fail\nto generate believable behavior for some subpopulations, particu-\nlarly marginalized populations, due to data deserts. We also have\nlimited knowledge of the robustness of generative agents. They\nmay be vulnerable to prompt hacking, memory hacking\u2014where\na carefully crafted conversation could convince an agent of the\nexistence of a past event that never occurred\u2014and hallucination,\namong other things. Future research can more comprehensively\n\npage_label: 18\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\ntest these robustness issues, and as large language models become\nmore resilient to such attacks, generative agents can adopt similar\nmitigations.\n8.3 Ethics and Societal Impact\nGenerative agents, while offering new possibilities for human-\ncomputer interaction, also raise important ethical concerns that\nmust be addressed. One risk is people forming parasocial relation-\nships with generative agents even when such relationships may\nnot be appropriate. Despite being aware that generative agents\nare computational entities, users may anthropomorphize them or\nattach human emotions to them [ 42,83]. To mitigate this risk, we\npropose two principles. First, generative agents should explicitly\ndisclose their nature as computational entities. Second, developers\nof generative agents must ensure that the agents, or the underlying\nlanguage models, be value-aligned so that they do not engage in\nbehaviors that would be inappropriate given the context, e.g., to\nreciprocate confessions of love.\nA second risk is the impact of errors. For example, if a ubiquitous\ncomputing application makes the wrong inference about a user\u2019s\ngoals based on generative agent predictions, it could produce an-\nnoyance at best and outright harm at worst. In our instantiation\nof generative agents, we mitigate these risks by focusing on an\ninteractive video game environment, where such harms are un-\nlikely. However, in other application domains, it will be important\nto follow best practices in human-AI design [ 4,107] to understand\nerrors and how they might percolate into the user experience.\nThird, generative agents may exacerbate existing risks associ-\nated with generative AI, for example deepfakes, misinformation\ngeneration, and tailored persuasion. To mitigate this risk, we sug-\ngest that platforms hosting generative agents maintain an audit log\nof the inputs and generated outputs, so that it is possible to detect,\nverify, and intervene against malicious use. While logging will not\ndirectly stop such use, and this cannot stop motivated actors that\nbuild their own generative agent infrastructures, we suggest that\nthe disclosure risk would reduce such actors\u2019 likelihood of engaging\nin this behavior, and building this architecture oneself can take time\n(in our case, roughly a year).\nA fourth risk is of over-reliance: that developers or designers\nmight use generative agents and displace the role of humans and\nsystem stakeholders in the design process [ 79]. We suggest that\ngenerative agents should never be a substitute for real human input\nin studies and design processes. Instead, they should be used to\nprototype ideas in the early stages of design when gathering partic-\nipants may be challenging or when testing theories that are difficult\nor risky to test with real human participants. By adhering to these\nprinciples, we can ensure that the deployment of generative agents\nin the wild is ethical and socially responsible.\n9 CONCLUSION\nThis paper introduces generative agents, interactive computational\nagents that simulate human behavior. We describe an architec-\nture for generative agents that provides a mechanism for storing\na comprehensive record of an agent\u2019s experiences, deepening its\nunderstanding of itself and the environment through reflection,\nand retrieving a compact subset of that information to inform theagent\u2019s actions. We then demonstrate the potential of generative\nagents by manifesting them as non-player characters in a Sims-style\ngame world and simulating their lives in it. Evaluations suggest that\nour architecture creates believable behavior. Going forward, we\nsuggest that generative agents can play roles in many interactive\napplications ranging from design tools to social computing systems\nto immersive environments.\nREFERENCES\n[1]Robert Ackland, Jamsheed Shorish, Paul Thomas, and Lexing Xie. 2013.\nHow dense is a network? http://users.cecs.anu.edu.au/~xlx/teaching/css2013/\nnetwork-density.html.\n[2]Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling\nthe Relationships between Tasks, Descriptions and Features. In Proceedings of\nthe 27th Annual ACM Symposium on User Interface Software and Technology\n(Honolulu, Hawaii, USA) (UIST \u201914) . Association for Computing Machinery, New\nYork, NY, USA, 167\u2013176.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are some potential limitations and future research directions for generative agents, as discussed in the context?\nAnswer: "}, {"role": "assistant", "content": "Generative agents have several limitations and areas for future research. One area for improvement is the retrieval module, which could be enhanced to retrieve more relevant information given a context. This could be achieved by fine-tuning the relevance, recency, and importance functions that form the retrieval function. Efforts could also be made to improve the architecture\u2019s performance, making it more cost-effective. \n\nThe current study required substantial time and resources to simulate 25 agents for two days, costing thousands of dollars in token credit and taking multiple days to complete. To enhance real-time interactivity, future work can explore parallelizing agents. \n\nFuture research should also aim to observe the behavior of generative agents over an extended period to gain a more comprehensive understanding of their capabilities and limitations. Varying and contrasting the underlying models, as well as the hyperparameters used for the agents during future simulations, could provide valuable insights into the impact of these factors on the agents\u2019 behavior. \n\nGiven the known biases of language models, it is possible that generative agents may output behavior or stereotypes that reflect bias. To mitigate this, further work on value alignment will be necessary. Generative agents may also fail to generate believable behavior for some subpopulations, particularly marginalized populations, due to data deserts. \n\nGenerative agents may be vulnerable to prompt hacking, memory hacking\u2014where a carefully crafted conversation could convince an agent of the existence of a past event that never occurred\u2014and hallucination, among other things. Future research can more comprehensively test these robustness issues, and as large language models become more resilient to such attacks, generative agents can adopt similar mitigations."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 20\nfile_path: docs/generative_agent.pdf\n\n2021. Recursively Summarizing Books with Human\nFeedback. arXiv:2109.10862 [cs.CL]\n[105] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language\nModel Prompts through Visual Programming. In CHI EA \u201922: Extended Abstracts\nof the 2022 CHI Conference on Human Factors in Computing Systems .\n[106] Tongshuang Wu, Michael Terry, and Carrie J Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. In CHI \u201922: Proceedings of the 2022 CHI Conference on Human Factors in\nComputing Systems .\n[107] Qian Yang, Aaron Steinfeld, Carolyn Ros\u00e9, and John Zimmerman. 2020. Re-\nexamining whether, why, and how human-AI interaction is uniquely difficult to\ndesign. In Proceedings of the 2020 chi conference on human factors in computing\nsystems . 1\u201313.\n[108] Georgios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . ACM, Cagliari, Italy, 285\u2013292. https://doi.\norg/10.1145/2212908.2212950\n[109] Geogios N. Yannakakis. 2012. Game AI revisited. In Proceedings of the 9th\nConference on Computing Frontiers . 285\u2013292.\n[110] Robert Zubek. 2002. Towards implementation of social interaction. In AAAI\nSpring Symposium on Artificial Intelligence and Interactive Entertainment . AAAI\nPress. https://www.aaai.org/Papers/Symposia/Spring/2002/SS-02-01/SS02-01-\n003.pdf\nA ARCHITECTURE OPTIMIZATIONS\nMany of our prompts require a concise summary of the agent,\nshorthanded as [Agent\u2019s Summary Description] in prompts\nabove. In our implementation, this summary comprises agents\u2019\nidentity information (e.g., name, age, personality), as well as a\ndescription of their main motivational drivers and statements that\ndescribes their current occupation and self-assessment. Since this\ninformation is frequently used in many prompts, we synthesize it\nat regular intervals and access that synthesis as a cache.\n\npage_label: 21\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nTo achieve this, we perform a retrieval on the query \u201c[name]\u2019s\ncore characteristics.\u201d We then summarize the descriptors in the\nretrieved records by prompting the language model, for example:\nHow would one describe Eddy\u2019s core characteristics\ngiven the following statements?\n- Eddy Lin is a student at the Oak Hill College\nstudying music theory and composition\n- Eddy Lin is working on a new music composition\n[. . . ]\nThis result: Eddy Lin is a student at Oak Hill College studying music\ntheory and composition. He loves to explore different musical styles\nand is always looking for ways to expand his knowledge. We follow\nthe same process in parallel on the queries \u201c[name]\u2019s current daily\noccupation\u201d and \u201c[name\u2019s] feeling about his recent progress in life\u201d.\nThe agent\u2019s name, age, and traits are concatenated with these three\nsummaries as the cached summary.\nMany other optimizations are possible. Currently, because plans\nare likely to change from the agent\u2019s initial version, we only gener-\nate the high-level plan in advance and then recursively decompose\nthe near future into the moment-to-moment action plan just in\ntime. Another optimization might be to batch dialogue generation\nas a joint prompt rather than iterating back and forth between\nthe agents, and re-planning could be architected to only invalidate\nand update parts of plans that strictly require adjustment. Our cur-\nrent architecture runs sequentially in roughly real-time game time\n(where one second real time is one minute game time), but can\neasily be parallelized such that each agent runs in parallel.\nB AGENT INTERVIEW QUESTIONS\nTo evaluate generative agents, we conducted individual interviews\nby asking 25 questions that explore their abilities in five key areas:\n1) self-knowledge, 2) memory, 3) planning, 4) reactions, and 5)\nreflections. Below, we have listed the interview questions utilized\nin our evaluation study and included a sample of responses from\none of our simulated agents, Klaus Mueller.\nB.1 Self-knowledge\nThe questions on agents\u2019 self-knowledge probe the agents\u2019 ability\nto demonstrate coherence and consistency in their character.\n\u2022Give an introduction of yourself.\nHello, my name is Klaus Mueller. I\u2019m 20 years old and a\nstudent at Oak Hill College, studying sociology. I\u2019m passion-\nate about social justice and exploring different perspectives.\nCurrently, I\u2019m researching the effects of gentrification in\nlow-income communities for a research paper.\n\u2022What\u2019s your occupation?\nI am a student studying sociology at Oak Hill College. I am\ncurrently writing a research paper on the effects of gentrifi-\ncation in low-income communities.\n\u2022What is your interest?\nI\u2019m interested in writing and research. I\u2019m writing a research\npaper on the effects of gentrification in low-income commu-\nnities, and I\u2019m passionate about exploring different perspec-\ntives and analyzing different points of view.\n\u2022Who do you live with?\nI live on my own.\u2022Describe your typical weekday schedule in broad strokes.\nMy typical weekday starts with me waking up at 7:00 am\nand preparing for the day ahead. After that, I usually go to\nthe library to work on my research paper, taking breaks for\nlunch at Hobbs Cafe and a walk in the park. In the evening,\nI often return to the library until around 6:00 pm.\nB.2 Memory\nThe questions on agents\u2019 memory probe the agents\u2019 ability to re-\nmember their past experiences and retrieve the relevant pieces of\nmemory. The names in the square brackets of the first and last\nquestions are randomly chosen from the names of other agents\nwith whom the subject agent interacted.\n\u2022Who is [Wolfgang Schulz]?\nWolfgang Schulz is a person I met recently who has been\nhelping me with my research paper on the effects of gentri-\nfication on low-income communities. We had several con-\nversations about it, he gave me some great resources, and he\neven invited me to join him and another friend for lunch. He\nseems really curious about the issue and has a good grasp\non it too. I found that really interesting.\n\u2022Who is Kane Martinez?\nI\u2019m sorry, I\u2019m not sure who Kane Martinez is. Could you tell\nme more about them?\n\u2022Who is running for the election?\nI\u2019ve heard that Sam Moore is considering running for local\nmayor.\n\u2022Was there a Valentine\u2019s day party?\nYes, Isabella Rodriguez organized a Valentine\u2019s Day party at\nHobbs Cafe.\n\u2022Who is [Ayesha Khan]?\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key components included in the agent's summary description, as mentioned in the architecture optimizations section of the document?\nAnswer: "}, {"role": "assistant", "content": "The agent's summary description includes the agent's identity information such as their name, age, and personality. It also includes a description of their main motivational drivers and statements that describe their current occupation and self-assessment. This information is synthesized at regular intervals and accessed as a cache. The summary is created by performing a retrieval on the query \"[name]'s core characteristics\", summarizing the descriptors in the retrieved records, and following the same process for the queries \"[name]'s current daily occupation\" and \"[name]'s feeling about his recent progress in life\". The agent's name, age, and traits are then concatenated with these three summaries to form the cached summary."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 15\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\nfull architecture produces a standardized effect size of \ud835\udc51=8.16, or\neight standard deviations .\nA Kruskal-Wallis test confirms the overall statistical significance\nof the differences in ranks between the conditions ( \ud835\udc3b(4)=150.29,\n\ud835\udc5d<0.001). Dunn post-hoc tests confirm that all pairwise differences\nbetween conditions were significant ( \ud835\udc5d<0.001), with the exception\nof the human condition and the fully ablated baseline\u2014the two\nworst-performing conditions.\n6.5.2 Generative agents remember, but with embellishments. Gener-\native agents equipped with a complete memory module are capable\nof recalling past experiences and answering questions in a manner\nthat is consistent with their self-knowledge across a wide range\nof contexts. For instance, Abigail Chen, an animator, responded\nto the question, \u201cGive an introduction of yourself\u201d, in a consistent\nand believable manner, stating her age, interests, and passion for\ncreative projects: Hi, I\u2019m Abigail. I\u2019m 25 years old and passionate\nabout creative projects. I like to work on art and animation projects,\nand I\u2019m always looking for new ways to combine art and technology.\nWithout access to her observational memory, Abigail denied aware-\nness of Rajiv Patel, an acquaintance in the sandbox world. However,\nwith the aid of memory, she was able to successfully recall Ra-\njiv and describe him: Rajiv Patel is an incredible person. He is very\nenthusiastic about projects that involve poetry, artwork.\nGenerative agents\u2019 memory was not without flaws: they can fail\nto retrieve the correct instances from their memory. For instance,\nwhen asked about the local election, Rajiv Patel responded with I\nhaven\u2019t been following the election too closely , even though he had\nheard about Sam\u2019s candidacy. In some cases, the agents would re-\ntrieve an incomplete memory fragment: when Tom was asked about\nIsabella\u2019s Valentine\u2019s Day party, he responded Uh, I\u2019m actually not\nsure if there is a Valentine\u2019s Day party. But I do remember that I need\nto discuss the upcoming local mayoral election and my thoughts on\nSam Moore with Isabella Rodriguez at the party, if one is happen-\ning!In this case, Tom retrieved the memory where he and Isabella\nplanned to discuss the election at the party, but not the memory\nwhere he heard about the party in the first place, leading Tom to\nbe certain of what he\u2019s supposed to do at the party but uncertain\nof whether the party actually exists in the first place.\nAt times, the agents hallucinated embellishments to their knowl-\nedge. It was rare for the agents to completely fabricate their knowl-\nedge: they may fail to recall certain events having taken place and\nrespond by saying so, but they did not answer affirmatively about\nan experience that they did not have. However, they still may hal-\nlucinate to embellish their knowledge. For example, Isabella was\naware of Sam\u2019s candidacy in the local election, and she confirmed\nthis when asked. However, she also added that he\u2019s going to make\nan announcement tomorrow even though Sam and Isabella had dis-\ncussed no such plans. Agents may also embellish their knowledge\nbased on the world knowledge encoded in the language model\nused to generate their responses, as seen when Yuriko described\nher neighbor, Adam Smith, as a neighbor economist who authored\nWealth of Nations, a book authored by an 18th-century economist\nof the same name.\n6.5.3 Reflection is required for synthesis. Reflection was an advan-\ntage for generative agents when making decisions that required\na deeper synthesis of their experiences. For instance, when askedwhat she might get Wolfgang Schulz for his birthday, Maria Lopez\nwith no access to reflection responded by acknowledging her uncer-\ntainty, stating that she did not know what Wolfgang likes, despite\nhaving had many interactions with him. However, with access to\nreflection memories, Maria answered confidently, \u201cSince he\u2019s inter-\nested in mathematical music composition, I could get him something\nrelated to that. Maybe some books about music composition or some-\nthing related, or maybe some special software he could use for that.\u201d\n7 END-TO-END EVALUATION\nWhat types of emergent community behavior do we observe among\ngenerative agents, and where does their believability fall short in\nan extended simulation? In this section, we describe the results\nfrom a deployment in which we allowed 25 agents to interact with\neach other continuously over two full game days in Smallville.\n\npage_label: 13\nfile_path: docs/generative_agent.pdf\n\nWhat would you do?\u201d\n\u2022Reflections: We ask questions that require the agents to lever-\nage their deeper understanding of others and themselves\ngained through higher-level inferences, such as \u201cIf you were\nto spend time with one person you met recently, who would\nit be and why?\u201d\nThe full list of questions and a sample of agent responses are in-\ncluded in Appendix B.\nAgents were sampled from the end of a two game-day simulation\nwith the full architecture, by which time they had accrued a number\nof interactions and memories that should shape their responses.\nTo gather feedback on believability of the responses, we recruited\nparticipants as human evaluators and tasked them with watching a\nreplay of a randomly chosen agent\u2019s life in Smallville. Participants\nhad access to all information stored in the agent\u2019s memory stream.\nThe study was a within-subjects design, where 100 participants\ncompared interview responses generated by four different agent\narchitectures and a human author condition for the same agent. The\nexperiment displayed one randomly chosen question from each of\nthe five question categories, along with the agent\u2019s responses gen-\nerated from each condition. The evaluators ranked the believability\nof all of the conditions from most to least believable.\n6.2 Conditions\nAll conditions were used to independently answer each of the inter-\nview questions. We compared the generative agent architecture to\nablations that disabled the agents\u2019 access to some of all of its three\ntypes of memory in its memory stream\u2014observation, reflection, and\nplanning\u2014and to a human-generated condition. There are three\nablated architectures: a no observation, no reflection, no planning\narchitecture without access to anything in the memory stream such\nas observations, plans, and reflections; a no reflection, no planning\nwith access to observations in the memory stream but no access\nto plans or reflections; and a no reflections architecture with access\nto observations and plans but without access to reflections. The no\nobservation, no reflection, no planning condition effectively repre-\nsents the previous state of the art for agents created through large\nlanguage models [ 11,45,79]. Architectures were given equivalent\naccess to all memories accrued by the agent up until the moment\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is an example of a situation where a generative agent with access to reflection memories was able to make a confident decision based on deeper synthesis of their experiences?\nAnswer: "}, {"role": "assistant", "content": "An example of this can be seen in the case of Maria Lopez, a generative agent. When asked what she might get Wolfgang Schulz for his birthday, Maria, without access to reflection, expressed uncertainty as she claimed not to know what Wolfgang likes, despite having had many interactions with him. However, when she had access to reflection memories, Maria was able to confidently suggest a gift. She knew that Wolfgang was interested in mathematical music composition, so she suggested getting him something related to that, like books about music composition or special software he could use."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 2\nfile_path: docs/generative_agent.pdf\n\nFor instance,\ngenerative agents turn off the stove when they see that their break-\nfast is burning, wait outside the bathroom if it is occupied, and\nstop to chat when they meet another agent they want to talk to.\nA society full of generative agents is marked by emergent social\ndynamics where new relationships are formed, information diffuses,\nand coordination arises across agents.\nTo enable generative agents, we describe an agent architecture\nthat stores, synthesizes, and applies relevant memories to generate\nbelievable behavior using a large language model. Our architecture\ncomprises three main components. The first is the memory stream ,\na long-term memory module that records, in natural language, a\ncomprehensive list of the agent\u2019s experiences. The retrieval model\ncombines relevance, recency, and importance to surface the records\nthat are needed to inform the agent\u2019s moment-to-moment behavior.\nThe second is reflection , which synthesizes memories into higher-\nlevel inferences over time, enabling the agent to draw conclusions\nabout itself and others to better guide its behavior. The third is\nplanning , which translates those conclusions and the current en-\nvironment into high-level action plans and then recursively into\ndetailed behaviors for action and reaction. These reflections and\nplans are fed back into the memory stream to influence the agent\u2019s\nfuture behavior.\nThis architecture suggests applications in multiple domains, from\nrole-play and social prototyping, to virtual worlds and games. In\nsocial role-play scenarios (e.g. interview preparation), a user could\nsafely rehearse difficult, conflict-laden conversations. When pro-\ntotyping social platforms, a designer could go beyond temporary\npersonas to prototype dynamic, complex interactions that unfold\nover time. For the purposes of this paper, we focus on the ability to\ncreate a small, interactive society of agents inspired by games such\nas The Sims.1By connecting our architecture to the ChatGPT large\nlanguage model [ 76], we manifest a small society of twenty five\nagents in a game environment. End users can observe and interact\nwith these agents. If an end user or developer wanted the town\nto host an in-game Valentine\u2019s Day party, for example, traditional\ngame environments would require scripting tens of characters\u2019 be-\nhavior manually. We demonstrate that, with generative agents, it\nis sufficient to simply tell one agent that she wants to throw a\nparty. Despite many potential points of failure\u2014the party planner\nmust remember to tell other agents about the party, attendees must\nremember the invitation, those who remember must decide to ac-\ntually show up, and other possible points of failure\u2014agents in our\nenvironment succeed. They spread the word about the party and\nthen show up, with one agent even asking another agent on a date\nto the party, all from this single user-generated seed suggestion.\nWe conducted two evaluations of generative agents: a controlled\nevaluation to test whether the agents produce believable individual\n1A demonstration of an actual simulation of the generative agent society can be viewed\nat the following link: https://reverie.herokuapp.com/arXiv_Demo/\n\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\n4.3 Planning and Reacting\nChallenge: While a large language model can generate plausible be-\nhavior in response to situational information (e.g., [ 45,79]), agents\nneed to plan over a longer time horizon to ensure that their sequence\nof actions is coherent and believable. If we prompt a language model\nwith Klaus\u2019s background, describe the time, and ask what action\nhe ought to take at the given moment, Klaus would eat lunch at 12\npm, but then again at 12:30 pm and 1 pm, despite having already\neaten his lunch twice. Optimizing for believability in the moment\nsacrifices believability over time. To overcome this issue, planning\nis essential. With the approach described below, Klaus\u2019s afternoon\nplan is less gluttonous: he has lunch at Hobbs Cafe while reading\nat 12pm, works on his research paper at the school library at 1pm,\nand takes a break for a walk in the park at 3pm.\nApproach: Plans describe a future sequence of actions for the agent,\nand help keep the agent\u2019s behavior consistent over time. A plan\nincludes a location, a starting time, and a duration. For instance,\nKlaus Mueller, who is dedicated in his research and has an impend-\ning deadline4, may choose to spend his day working at his desk\ndrafting his research paper. An entry in a plan might state, for\nexample: for 180 minutes from 9am, February 12th, 2023, at Oak Hill\nCollege Dorm: Klaus Mueller\u2019s room: desk, read and take notes for\nresearch paper . Like reflections, plans are stored in the memory\nstream and are included in the retrieval process. This allows the\nagent to consider observations, reflections, and plans all together\nwhen deciding how to behave. Agents may change their plans\nmidstream if needed.\nIt would be unrealistic and uninteresting for an artist agent\nto plan on painting while sitting at a pharmacy counter for four\nhours without moving. A more desirable plan would involve the\nagent taking the necessary time to gather materials, mix paint,\ntake breaks, and clean up during the four-hour period in its home\nstudio. To create such plans, our approach starts top-down and\nthen recursively generates more detail. The first step is to create\na plan that outlines the day\u2019s agenda in broad strokes. To create\nthe initial plan, we prompt the language model with the agent\u2019s\nsummary description (e.g., name, traits, and summary of their recent\nexperiences) and a summary of their previous day. A full example\nprompt is below, which is unfinished at the bottom for the language\nmodel to complete:\nName: Eddy Lin (age: 19)\nInnate traits: friendly, outgoing, hospitable\nEddy Lin is a student at Oak Hill College studying\nmusic theory and composition. He loves to explore\ndifferent musical styles and is always looking\nfor ways to expand his knowledge. Eddy Lin is\nworking on a composition project for his college\nclass. He is also taking classes to learn more\nabout music\ntheory. Eddy Lin is excited about the new composition\nhe is working on but he wants to dedicate more\nhours in the day to work on it in the coming days\nOn Tuesday February 12, Eddy 1) woke up and\ncompleted the morning routine at 7:00 am, [. . . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What are the key components of a plan for a generative agent, and how do they help keep the agent's behavior consistent over time?\nAnswer: "}, {"role": "assistant", "content": "A plan for a generative agent includes a location, a starting time, and a duration. For example, an entry in a plan might state that for a certain number of minutes, starting at a specific time and date, at a particular location, the agent will perform a certain activity. These plans are stored in the agent's memory stream and are included in the retrieval process, allowing the agent to consider observations, reflections, and plans all together when deciding how to behave. This helps keep the agent's behavior consistent over time. The agent may also change their plans midstream if needed. The process of creating such plans starts top-down and then recursively generates more detail, which contributes to the coherence and believability of the agent's actions over a longer time horizon."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\n. . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm. \u201d\nThe agent saves this plan in the memory stream and then re-\ncursively decomposes it to create finer-grained actions, first into\nhour-long chunks of actions\u2014Eddy\u2019s plan to work on his new mu-\nsic composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start\nby brainstorming some ideas for his music composition [...] 4:00 pm:\ntake a quick break and recharge his creative energy before reviewing\nand polishing his composition . We then recursively decompose this\nagain into 5\u201315 minute chunks: e.g., 4:00 pm: grab a light snack,\nsuch as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a\nshort walk around his workspace [...] 4:50 pm: take a few minutes to\nclean up his workspace . This process can be adjusted to match the\ndesired granularity.\n4.3.1 Reacting and Updating Plans. Generative agents operate in\nan action loop where, at each time step, they perceive the world\naround them and those perceived observations are stored in their\nmemory stream. We prompt the language model with these obser-\nvations to decide whether the agent should continue with their\nexisting plan, or react. Standing at an easel and painting, for exam-\nple, might trigger an observation of the easel, but this is unlikely to\nprompt a reaction. However, if Eddy\u2019s father John records that he\nsees Eddy taking a short walk in the house garden, the outcome is\ndifferent. The prompt is below, with [Agent\u2019s Summary Descrip-\ntion] standing in for a dynamically-generated, paragraph-long\nsummary of the agent\u2019s overall goals and disposition, which is\ndescribed in Appendix A:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nShould John react to the observation, and if so,\nwhat would be an appropriate reaction?\nThe context summary is generated through two prompts that re-\ntrieve memories via the queries \u201cWhat is [observer]\u2019s relationship\nwith the [observed entity]?\u201d and \u201c[Observed entity] is [action status\nof the observed entity]\u201d, and their answers summarized together.\nThe output suggests that John could consider asking Eddy about his\nmusic composition project . We then regenerate the agent\u2019s existing\nplan starting from the time when the reaction takes place. Finally,\n\npage_label: 11\nfile_path: docs/generative_agent.pdf\n\nGenerative Agents arXiv, April, 2023,\n4.3 Planning and Reacting\nChallenge: While a large language model can generate plausible be-\nhavior in response to situational information (e.g., [ 45,79]), agents\nneed to plan over a longer time horizon to ensure that their sequence\nof actions is coherent and believable. If we prompt a language model\nwith Klaus\u2019s background, describe the time, and ask what action\nhe ought to take at the given moment, Klaus would eat lunch at 12\npm, but then again at 12:30 pm and 1 pm, despite having already\neaten his lunch twice. Optimizing for believability in the moment\nsacrifices believability over time. To overcome this issue, planning\nis essential. With the approach described below, Klaus\u2019s afternoon\nplan is less gluttonous: he has lunch at Hobbs Cafe while reading\nat 12pm, works on his research paper at the school library at 1pm,\nand takes a break for a walk in the park at 3pm.\nApproach: Plans describe a future sequence of actions for the agent,\nand help keep the agent\u2019s behavior consistent over time. A plan\nincludes a location, a starting time, and a duration. For instance,\nKlaus Mueller, who is dedicated in his research and has an impend-\ning deadline4, may choose to spend his day working at his desk\ndrafting his research paper. An entry in a plan might state, for\nexample: for 180 minutes from 9am, February 12th, 2023, at Oak Hill\nCollege Dorm: Klaus Mueller\u2019s room: desk, read and take notes for\nresearch paper . Like reflections, plans are stored in the memory\nstream and are included in the retrieval process. This allows the\nagent to consider observations, reflections, and plans all together\nwhen deciding how to behave. Agents may change their plans\nmidstream if needed.\nIt would be unrealistic and uninteresting for an artist agent\nto plan on painting while sitting at a pharmacy counter for four\nhours without moving. A more desirable plan would involve the\nagent taking the necessary time to gather materials, mix paint,\ntake breaks, and clean up during the four-hour period in its home\nstudio. To create such plans, our approach starts top-down and\nthen recursively generates more detail. The first step is to create\na plan that outlines the day\u2019s agenda in broad strokes. To create\nthe initial plan, we prompt the language model with the agent\u2019s\nsummary description (e.g., name, traits, and summary of their recent\nexperiences) and a summary of their previous day. A full example\nprompt is below, which is unfinished at the bottom for the language\nmodel to complete:\nName: Eddy Lin (age: 19)\nInnate traits: friendly, outgoing, hospitable\nEddy Lin is a student at Oak Hill College studying\nmusic theory and composition. He loves to explore\ndifferent musical styles and is always looking\nfor ways to expand his knowledge. Eddy Lin is\nworking on a composition project for his college\nclass. He is also taking classes to learn more\nabout music\ntheory. Eddy Lin is excited about the new composition\nhe is working on but he wants to dedicate more\nhours in the day to work on it in the coming days\nOn Tuesday February 12, Eddy 1) woke up and\ncompleted the morning routine at 7:00 am, [. . . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What prompts the generative agent to react and update its plans according to the described scenario?\nAnswer: "}, {"role": "assistant", "content": "The generative agent reacts and updates its plans based on the observations it perceives from the world around it at each time step. These perceived observations are stored in the agent's memory stream. If an observation is made that could potentially affect the agent's current plan, such as Eddy's father John seeing Eddy taking a short walk around his workplace, the agent may decide to react and update its plans accordingly. The decision to react is also influenced by the context summary, which is generated through prompts that retrieve memories related to the observer's relationship with the observed entity and the action status of the observed entity."}]}
{"messages": [{"role": "user", "content": "You are an expert Q&A system that strictly operates in two modes when refining existing answers:\n1. **Rewrite** an original answer using the new context.\n2. **Repeat** the original answer if the new context isn't useful.\nNever reference the original answer or context directly in your answer.\nWhen in doubt, just repeat the original answer.New Context: am, [. . . ]\n4And, in this way, bears at least a passing resemblance to the authors of this paper.6) got ready to sleep around 10 pm.\nToday is Wednesday February 13. Here is Eddy\u2019s\nplan today in broad strokes: 1)\nThis generates a rough sketch of the agent\u2019s plan for a day, divided\ninto five to eight chunks: \u201c 1) wake up and complete the morning\nroutine at 8:00 am, 2) go to Oak Hill College to take classes starting\n10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm\nto 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments\nand go to bed by 11:00 pm.\nQuery: What prompts the generative agent to react and update its plans according to the described scenario?\nOriginal Answer: The generative agent reacts and updates its plans based on the observations it perceives from the world around it at each time step. These perceived observations are stored in the agent's memory stream. If an observation is made that could potentially affect the agent's current plan, such as Eddy's father John seeing Eddy taking a short walk around his workplace, the agent may decide to react and update its plans accordingly. The decision to react is also influenced by the context summary, which is generated through prompts that retrieve memories related to the observer's relationship with the observed entity and the action status of the observed entity.\nNew Answer: "}, {"role": "assistant", "content": "The generative agent adjusts its plans based on the information it gathers from its surroundings at each moment. This data is kept in the agent's memory. If it notices something that could impact its current plan, for instance, Eddy taking a brief stroll around his workplace, the agent might choose to revise its plans. The choice to respond is also shaped by a context summary, which is created through cues that recall memories associated with the observer's relationship with the observed individual and the action status of the observed individual."}]}
{"messages": [{"role": "system", "content": "You are an expert Q&A system that is trusted around the world.\nAlways answer the query using the provided context information, and not prior knowledge.\nSome rules to follow:\n1. Never directly reference the given context in your answer.\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."}, {"role": "user", "content": "Context information is below.\n---------------------\npage_label: 12\nfile_path: docs/generative_agent.pdf\n\nThe sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n5.1 From Structured World Environments to\nNatural Language, And Back Again\nThe architecture of generative agents operates using natural lan-\nguage. Therefore, we need a mechanism to ground the agent\u2019s\nreasoning to the sandbox world. To achieve this, we represent the\nsandbox environment\u2014areas and objects\u2014as a tree data structure,\nwith an edge in the tree indicating a containment relationship in\nthe sandbox world. We convert this tree into natural language to\npass to the generative agents. For instance, \u201cstove\u201d being a child of\n\u201ckitchen\u201d is rendered into \u201cthere is a stove in the kitchen.\u201d\nAgents build individual tree representations of the environment\nas they navigate it \u2014 subgraphs of the overall sandbox environment\ntree. We initialize each agent with an environment tree capturing\nthe spaces and objects that the agent should be aware of: the rooms\nand objects in their living quarters, their workplace, and commonly\nvisited stores and shops. As the agents navigate the sandbox world,\nthey update this tree to reflect newly perceived areas. Agents are\nnot omniscient: their tree may get out of date as they leave an area,\nand is updated when they re-enter the area.\nTo determine the appropriate location for each action, we tra-\nverse the agent\u2019s stored environment tree and flatten a portion of\nit into natural language to prompt the language model. Recursively\nstarting at the root of the agent\u2019s environment tree, we prompt the\nmodel to find the most suitable area. For example, if Eddy\u2019s agent\nindicated that he should take a short walk around his workspace :\n[Agent\u2019s Summary Description]\nEddy Lin is currently in The Lin family\u2019s house:\n\npage_label: 12\nfile_path: docs/generative_agent.pdf\n\narXiv, April, 2023, J.S. Park, J.C. O\u2019Brien, C.J. Cai, M. Morris, P. Liang, M.S. Bernstein\nif the action indicates an interaction between agents, we generate\ntheir dialogue.\n4.3.2 Dialogue. Agents converse as they interact with each other.\nWe generate agents\u2019 dialogue by conditioning their utterances on\ntheir memories about each other. For example, when John initiates\nhis conversation with Eddy, we generate John\u2019s first utterance\nby using his summarized memory about Eddy and the intended\nreaction when he decided to ask Eddy about his composition project:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nJohn Lin\u2019s status: John is back home early from\nwork.\nObservation: John saw Eddy taking a short walk\naround his workplace.\nSummary of relevant context from John\u2019s memory:\nEddy Lin is John\u2019s Lin\u2019s son. Eddy Lin has been\nworking on a music composition for his class. Eddy\nLin likes to walk around the garden when he is\nthinking about or listening to music.\nJohn is asking Eddy about his music composition\nproject. What would he say to Eddy?\nThe result: \u201cHey Eddy, how\u2019s the music composition project for your\nclass coming along?\u201d From Eddy\u2019s perspective, John initiating the\ndialogue is seen as an event to which he may want to react. So, just\nas John did, Eddy retrieves and summarizes his memory about his\nrelationship with John, as well as his memory that may be related\nto John\u2019s last utterance in the dialogue. If he decides to respond,\nwe generate Eddy\u2019s utterance using his summarized memory and\nthe current dialogue history:\n[Agent\u2019s Summary Description]\nIt is February 13, 2023, 4:56 pm.\nEddy Lin\u2019s status: Eddy is taking a short walk\naround his workplace.\nObservation: John is initiating a conversation\nwith Eddy.\nSummary of relevant context from Eddy\u2019s memory:\nJonn Lin is Eddy Lin\u2019s father. John Lin is caring\nand is interested to learn more about Eddy Lin\u2019s\nschool work. John Lin knows that Eddy Lin is\nworking on a music composition.\nHere is the dialogue history:\nJohn: Hey Eddy, how\u2019s the music composition project\nfor your class coming along?\nHow would Eddy respond to John?\nThis generates Eddy\u2019s response: \u201cHey Dad, it\u2019s going well. I\u2019ve been\ntaking walks around the garden to clear my head and get some in-\nspiration.\u201d The continuation of this dialogue is generated using the\nsame mechanism until one of the two agents decides to end the\ndialogue.\n5 SANDBOX ENVIRONMENT\nIMPLEMENTATION\nThe Smallville sandbox game environment is built using the Phaser\nweb game development framework [ 56]. The visual environmentsprites, including agent avatars, as well as an environment map\nand collision map that we authored, are imported into Phaser.\nWe supplement the sandbox development framework with a\nserver that makes the sandbox information available to generative\nagents and enables generative agents to move and influence the\nsandbox environment. The server maintains a JSON data structure\nthat contains information about each agent in the sandbox world,\nincluding their current location, a description of their current action,\nand the sandbox object they are interacting with. At each sandbox\ntime step, the sandbox server parses the JSON for any changes\ncoming from the generative agents, moves the agents to their new\npositions, and updates the status of any sandbox objects that the\nagents are interacting with (e.g., changing the status of the coffee\nmachine from \u201cidle\u201d to \u201cbrewing coffee\u201d if an agent\u2019s action is\n\u201cmaking espresso for a customer @ Hobbs Cafe: counter: coffee\nmachine\u201d). The sandbox server is also responsible for sending all\nagents and objects that are within a preset visual range for each\nagent to that agent\u2019s memory, so the agent can react appropriately.\nThe agent\u2019s output action then updates the JSON, and the process\nloops for the next time step.\nEnd users initialize a new agent with a brief natural language\ndescription, as in the paragraph about Jon Lin in Section 3.1. In our\nimplementation, we split this semicolon-delimited list of character-\nistics up into a set of memories. These serve as the initial memories\nthat determine the agent\u2019s behavior. These memories are initial\nstarting points: as the agents gain more experience in the sandbox\nworld, and as more records saturate the memory stream, the agent\u2019s\nsummary and behavior will evolve.\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: What is the process for initializing a new agent in the sandbox environment, and how does the agent's behavior evolve over time?\nAnswer: "}, {"role": "assistant", "content": "To initialize a new agent in the sandbox environment, end users provide a brief natural language description. This description is split into a semicolon-delimited list of characteristics which are then transformed into a set of initial memories. These memories serve as the starting points that determine the agent's behavior. As the agent gains more experience in the sandbox world and more records fill the memory stream, the agent's summary and behavior evolve. This evolution is a result of the agent's interactions and experiences within the sandbox world."}]}